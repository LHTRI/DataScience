{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triển khai mô hình hồi quy với mạng Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import subprocess\n",
    "from glob import glob\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import scipy.stats as ss\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import keras\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.initializers import RandomNormal, Constant\n",
    "from tensorflow.keras.metrics import Metric\n",
    "import tensorflow.keras.optimizers as Optimizer\n",
    "import tensorflow_addons as tfa \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "import os.path as path\n",
    "lib_path =  path.abspath(path.join('' ,\"../../api/common\"))\n",
    "sys.path.insert(1, lib_path)\n",
    "from transform_split_data import transform_split_data\n",
    "from predict import predict, evaluate\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext autotime\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Khởi tạo phương thức giải phóng bộ nhớ gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "- Load dữ liệu đã được chọn feature tự động"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>id$Year</th>\n",
       "      <th>Kyori</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_1</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_1</th>\n",
       "      <th>id$RaceNum</th>\n",
       "      <th>TrackCD_52</th>\n",
       "      <th>...</th>\n",
       "      <th>CH_Syotai_岩手</th>\n",
       "      <th>KS_Syotai_アメリカ</th>\n",
       "      <th>CH_Syotai_荒尾</th>\n",
       "      <th>KS_Syotai_フランス</th>\n",
       "      <th>KS_Syotai_川崎</th>\n",
       "      <th>KS_Syotai_笠松</th>\n",
       "      <th>id$JyoCD_1</th>\n",
       "      <th>KS_ChokyosiCode_365.0</th>\n",
       "      <th>CH_Syotai_川崎</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25609</td>\n",
       "      <td>2010104751</td>\n",
       "      <td>2015</td>\n",
       "      <td>-0.560469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.372077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.260655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7878</td>\n",
       "      <td>2007101609</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.381357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.017728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13521</td>\n",
       "      <td>2008110046</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.893973</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.372077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.825509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2220</td>\n",
       "      <td>2006105746</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.449541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29966</td>\n",
       "      <td>2010103540</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.787599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475191</th>\n",
       "      <td>13877</td>\n",
       "      <td>2006106478</td>\n",
       "      <td>2012</td>\n",
       "      <td>1.863602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.787599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.033771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475192</th>\n",
       "      <td>12705</td>\n",
       "      <td>2007101829</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.372077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.357076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475193</th>\n",
       "      <td>12294</td>\n",
       "      <td>2007100217</td>\n",
       "      <td>2011</td>\n",
       "      <td>-1.530097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.089118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475194</th>\n",
       "      <td>16594</td>\n",
       "      <td>2009103509</td>\n",
       "      <td>2012</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.495360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.056733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475195</th>\n",
       "      <td>15765</td>\n",
       "      <td>2008104265</td>\n",
       "      <td>2012</td>\n",
       "      <td>-0.560469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.203121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.016949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>475196 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        race_id    KettoNum  id$Year     Kyori  TenkoBaba$DirtBabaCD_0  \\\n",
       "0         25609  2010104751     2015 -0.560469                     1.0   \n",
       "1          7878  2007101609     2010  0.893973                     1.0   \n",
       "2         13521  2008110046     2011  0.893973                     1.0   \n",
       "3          2220  2006105746     2008  0.409159                     1.0   \n",
       "4         29966  2010103540     2016  0.166752                     0.0   \n",
       "...         ...         ...      ...       ...                     ...   \n",
       "475191    13877  2006106478     2012  1.863602                     0.0   \n",
       "475192    12705  2007101829     2011  0.166752                     0.0   \n",
       "475193    12294  2007100217     2011 -1.530097                     0.0   \n",
       "475194    16594  2009103509     2012 -1.045283                     0.0   \n",
       "475195    15765  2008104265     2012 -0.560469                     1.0   \n",
       "\n",
       "        TenkoBaba$SibaBabaCD_0  TenkoBaba$SibaBabaCD_1  \\\n",
       "0                          0.0                     1.0   \n",
       "1                          0.0                     0.0   \n",
       "2                          0.0                     1.0   \n",
       "3                          0.0                     1.0   \n",
       "4                          1.0                     0.0   \n",
       "...                        ...                     ...   \n",
       "475191                     1.0                     0.0   \n",
       "475192                     1.0                     0.0   \n",
       "475193                     1.0                     0.0   \n",
       "475194                     1.0                     0.0   \n",
       "475195                     0.0                     0.0   \n",
       "\n",
       "        TenkoBaba$DirtBabaCD_1  id$RaceNum  TrackCD_52  ...  \\\n",
       "0                          0.0    1.372077         0.0  ...   \n",
       "1                          0.0   -0.381357         0.0  ...   \n",
       "2                          0.0    1.372077         0.0  ...   \n",
       "3                          0.0   -1.550314         0.0  ...   \n",
       "4                          1.0    0.787599         0.0  ...   \n",
       "...                        ...         ...         ...  ...   \n",
       "475191                     1.0    0.787599         0.0  ...   \n",
       "475192                     1.0    1.372077         0.0  ...   \n",
       "475193                     1.0   -0.089118         0.0  ...   \n",
       "475194                     1.0    0.495360         0.0  ...   \n",
       "475195                     0.0    0.203121         0.0  ...   \n",
       "\n",
       "        CH_Syotai_岩手　　　　　　　　  KS_Syotai_アメリカ　　　　　　  CH_Syotai_荒尾　　　　　　　　  \\\n",
       "0                        0.0                   0.0                   0.0   \n",
       "1                        0.0                   0.0                   0.0   \n",
       "2                        0.0                   0.0                   0.0   \n",
       "3                        0.0                   0.0                   0.0   \n",
       "4                        0.0                   0.0                   0.0   \n",
       "...                      ...                   ...                   ...   \n",
       "475191                   0.0                   0.0                   0.0   \n",
       "475192                   0.0                   0.0                   0.0   \n",
       "475193                   0.0                   0.0                   0.0   \n",
       "475194                   0.0                   0.0                   0.0   \n",
       "475195                   0.0                   0.0                   0.0   \n",
       "\n",
       "        KS_Syotai_フランス　　　　　　  KS_Syotai_川崎　　　　　　　　  KS_Syotai_笠松　　　　　　　　  \\\n",
       "0                        0.0                   0.0                   0.0   \n",
       "1                        0.0                   0.0                   0.0   \n",
       "2                        0.0                   0.0                   0.0   \n",
       "3                        0.0                   0.0                   0.0   \n",
       "4                        0.0                   0.0                   0.0   \n",
       "...                      ...                   ...                   ...   \n",
       "475191                   0.0                   0.0                   0.0   \n",
       "475192                   0.0                   0.0                   0.0   \n",
       "475193                   0.0                   0.0                   0.0   \n",
       "475194                   0.0                   0.0                   0.0   \n",
       "475195                   0.0                   0.0                   0.0   \n",
       "\n",
       "        id$JyoCD_1  KS_ChokyosiCode_365.0  CH_Syotai_川崎　　　　　　　　      speed  \n",
       "0              0.0                    0.0                   0.0  62.260655  \n",
       "1              0.0                    0.0                   0.0  58.017728  \n",
       "2              0.0                    0.0                   0.0  59.825509  \n",
       "3              0.0                    0.0                   0.0  59.449541  \n",
       "4              1.0                    0.0                   0.0  56.666667  \n",
       "...            ...                    ...                   ...        ...  \n",
       "475191         0.0                    0.0                   0.0  54.033771  \n",
       "475192         1.0                    0.0                   0.0  57.357076  \n",
       "475193         0.0                    0.0                   0.0  59.210526  \n",
       "475194         0.0                    0.0                   0.0  59.056733  \n",
       "475195         0.0                    0.0                   0.0  61.016949  \n",
       "\n",
       "[475196 rows x 204 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.67 s\n"
     ]
    }
   ],
   "source": [
    "# Load dữ liệu\n",
    "train_data = pd.read_csv('..\\\\feature_selection\\\\train_data.csv')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>id$Year</th>\n",
       "      <th>Kyori</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_1</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_1</th>\n",
       "      <th>id$RaceNum</th>\n",
       "      <th>TrackCD_52</th>\n",
       "      <th>...</th>\n",
       "      <th>CH_Syotai_岩手</th>\n",
       "      <th>KS_Syotai_アメリカ</th>\n",
       "      <th>CH_Syotai_荒尾</th>\n",
       "      <th>KS_Syotai_フランス</th>\n",
       "      <th>KS_Syotai_川崎</th>\n",
       "      <th>KS_Syotai_笠松</th>\n",
       "      <th>id$JyoCD_1</th>\n",
       "      <th>KS_ChokyosiCode_365.0</th>\n",
       "      <th>CH_Syotai_川崎</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015101022</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015103483</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.908847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015106010</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.178082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015102342</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.775510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015102323</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19140</th>\n",
       "      <td>35925</td>\n",
       "      <td>2014105425</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.378378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19141</th>\n",
       "      <td>35925</td>\n",
       "      <td>2014105543</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19142</th>\n",
       "      <td>35925</td>\n",
       "      <td>2011106130</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.754011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19143</th>\n",
       "      <td>35925</td>\n",
       "      <td>2012102418</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.497782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19144</th>\n",
       "      <td>35925</td>\n",
       "      <td>2013104045</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.243816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19145 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       race_id    KettoNum  id$Year     Kyori  TenkoBaba$DirtBabaCD_0  \\\n",
       "0        34535  2015101022     2018 -1.045283                     0.0   \n",
       "1        34535  2015103483     2018 -1.045283                     0.0   \n",
       "2        34535  2015106010     2018 -1.045283                     0.0   \n",
       "3        34535  2015102342     2018 -1.045283                     0.0   \n",
       "4        34535  2015102323     2018 -1.045283                     0.0   \n",
       "...        ...         ...      ...       ...                     ...   \n",
       "19140    35925  2014105425     2018  0.409159                     0.0   \n",
       "19141    35925  2014105543     2018  0.409159                     0.0   \n",
       "19142    35925  2011106130     2018  0.409159                     0.0   \n",
       "19143    35925  2012102418     2018  0.409159                     0.0   \n",
       "19144    35925  2013104045     2018  0.409159                     0.0   \n",
       "\n",
       "       TenkoBaba$SibaBabaCD_0  TenkoBaba$SibaBabaCD_1  TenkoBaba$DirtBabaCD_1  \\\n",
       "0                         1.0                     0.0                     1.0   \n",
       "1                         1.0                     0.0                     1.0   \n",
       "2                         1.0                     0.0                     1.0   \n",
       "3                         1.0                     0.0                     1.0   \n",
       "4                         1.0                     0.0                     1.0   \n",
       "...                       ...                     ...                     ...   \n",
       "19140                     1.0                     0.0                     1.0   \n",
       "19141                     1.0                     0.0                     1.0   \n",
       "19142                     1.0                     0.0                     1.0   \n",
       "19143                     1.0                     0.0                     1.0   \n",
       "19144                     1.0                     0.0                     1.0   \n",
       "\n",
       "       id$RaceNum  TrackCD_52  ...  CH_Syotai_岩手　　　　　　　　  \\\n",
       "0       -1.550314         0.0  ...                   0.0   \n",
       "1       -1.550314         0.0  ...                   0.0   \n",
       "2       -1.550314         0.0  ...                   0.0   \n",
       "3       -1.550314         0.0  ...                   0.0   \n",
       "4       -1.550314         0.0  ...                   0.0   \n",
       "...           ...         ...  ...                   ...   \n",
       "19140    1.664316         0.0  ...                   0.0   \n",
       "19141    1.664316         0.0  ...                   0.0   \n",
       "19142    1.664316         0.0  ...                   0.0   \n",
       "19143    1.664316         0.0  ...                   0.0   \n",
       "19144    1.664316         0.0  ...                   0.0   \n",
       "\n",
       "       KS_Syotai_アメリカ　　　　　　  CH_Syotai_荒尾　　　　　　　　  KS_Syotai_フランス　　　　　　  \\\n",
       "0                       0.0                   0.0                   0.0   \n",
       "1                       0.0                   0.0                   0.0   \n",
       "2                       0.0                   0.0                   0.0   \n",
       "3                       0.0                   0.0                   0.0   \n",
       "4                       0.0                   0.0                   0.0   \n",
       "...                     ...                   ...                   ...   \n",
       "19140                   0.0                   0.0                   0.0   \n",
       "19141                   0.0                   0.0                   0.0   \n",
       "19142                   0.0                   0.0                   0.0   \n",
       "19143                   0.0                   0.0                   0.0   \n",
       "19144                   0.0                   0.0                   0.0   \n",
       "\n",
       "       KS_Syotai_川崎　　　　　　　　  KS_Syotai_笠松　　　　　　　　  id$JyoCD_1  \\\n",
       "0                       0.0                   0.0         0.0   \n",
       "1                       0.0                   0.0         0.0   \n",
       "2                       0.0                   0.0         0.0   \n",
       "3                       0.0                   0.0         0.0   \n",
       "4                       0.0                   0.0         0.0   \n",
       "...                     ...                   ...         ...   \n",
       "19140                   0.0                   0.0         0.0   \n",
       "19141                   0.0                   0.0         0.0   \n",
       "19142                   0.0                   0.0         0.0   \n",
       "19143                   0.0                   0.0         0.0   \n",
       "19144                   0.0                   0.0         0.0   \n",
       "\n",
       "       KS_ChokyosiCode_365.0  CH_Syotai_川崎　　　　　　　　      speed  \n",
       "0                        0.0                   0.0  58.064516  \n",
       "1                        0.0                   0.0  57.908847  \n",
       "2                        0.0                   0.0  59.178082  \n",
       "3                        0.0                   0.0  58.775510  \n",
       "4                        0.0                   0.0  57.142857  \n",
       "...                      ...                   ...        ...  \n",
       "19140                    0.0                   0.0  58.378378  \n",
       "19141                    0.0                   0.0  57.857143  \n",
       "19142                    0.0                   0.0  57.754011  \n",
       "19143                    0.0                   0.0  57.497782  \n",
       "19144                    0.0                   0.0  57.243816  \n",
       "\n",
       "[19145 rows x 204 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 532 ms\n"
     ]
    }
   ],
   "source": [
    "# Load dữ liệu\n",
    "test_data = pd.read_csv('..\\\\feature_selection\\\\test_data.csv')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id$Year</th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>speed</th>\n",
       "      <th>Time</th>\n",
       "      <th>KakuteiJyuni</th>\n",
       "      <th>top3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>25609</td>\n",
       "      <td>2010104751</td>\n",
       "      <td>62.260655</td>\n",
       "      <td>80.95</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>7878</td>\n",
       "      <td>2007101609</td>\n",
       "      <td>58.017728</td>\n",
       "      <td>124.10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011</td>\n",
       "      <td>13521</td>\n",
       "      <td>2008110046</td>\n",
       "      <td>59.825509</td>\n",
       "      <td>120.35</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008</td>\n",
       "      <td>2220</td>\n",
       "      <td>2006105746</td>\n",
       "      <td>59.449541</td>\n",
       "      <td>109.00</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>29966</td>\n",
       "      <td>2010103540</td>\n",
       "      <td>56.666667</td>\n",
       "      <td>108.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475191</th>\n",
       "      <td>2012</td>\n",
       "      <td>13877</td>\n",
       "      <td>2006106478</td>\n",
       "      <td>54.033771</td>\n",
       "      <td>159.90</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475192</th>\n",
       "      <td>2011</td>\n",
       "      <td>12705</td>\n",
       "      <td>2007101829</td>\n",
       "      <td>57.357076</td>\n",
       "      <td>106.70</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475193</th>\n",
       "      <td>2011</td>\n",
       "      <td>12294</td>\n",
       "      <td>2007100217</td>\n",
       "      <td>59.210526</td>\n",
       "      <td>60.80</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475194</th>\n",
       "      <td>2012</td>\n",
       "      <td>16594</td>\n",
       "      <td>2009103509</td>\n",
       "      <td>59.056733</td>\n",
       "      <td>73.15</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475195</th>\n",
       "      <td>2012</td>\n",
       "      <td>15765</td>\n",
       "      <td>2008104265</td>\n",
       "      <td>61.016949</td>\n",
       "      <td>82.60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>475196 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id$Year  race_id    KettoNum      speed    Time  KakuteiJyuni  top3\n",
       "0          2015    25609  2010104751  62.260655   80.95             7     0\n",
       "1          2010     7878  2007101609  58.017728  124.10             4     0\n",
       "2          2011    13521  2008110046  59.825509  120.35             4     0\n",
       "3          2008     2220  2006105746  59.449541  109.00             5     0\n",
       "4          2016    29966  2010103540  56.666667  108.00             4     0\n",
       "...         ...      ...         ...        ...     ...           ...   ...\n",
       "475191     2012    13877  2006106478  54.033771  159.90            12     0\n",
       "475192     2011    12705  2007101829  57.357076  106.70             4     0\n",
       "475193     2011    12294  2007100217  59.210526   60.80             7     0\n",
       "475194     2012    16594  2009103509  59.056733   73.15             9     0\n",
       "475195     2012    15765  2008104265  61.016949   82.60             1     1\n",
       "\n",
       "[475196 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 359 ms\n"
     ]
    }
   ],
   "source": [
    "y_train_df = pd.read_csv('..\\\\feature_selection\\\\y_train_df.csv')\n",
    "y_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id$Year</th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>speed</th>\n",
       "      <th>Time</th>\n",
       "      <th>KakuteiJyuni</th>\n",
       "      <th>top3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015101022</td>\n",
       "      <td>58.064516</td>\n",
       "      <td>74.4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015103483</td>\n",
       "      <td>57.908847</td>\n",
       "      <td>74.6</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015106010</td>\n",
       "      <td>59.178082</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015102342</td>\n",
       "      <td>58.775510</td>\n",
       "      <td>73.5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015102323</td>\n",
       "      <td>57.142857</td>\n",
       "      <td>75.6</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19140</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2014105425</td>\n",
       "      <td>58.378378</td>\n",
       "      <td>111.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19141</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2014105543</td>\n",
       "      <td>57.857143</td>\n",
       "      <td>112.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19142</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2011106130</td>\n",
       "      <td>57.754011</td>\n",
       "      <td>112.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19143</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2012102418</td>\n",
       "      <td>57.497782</td>\n",
       "      <td>112.7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19144</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2013104045</td>\n",
       "      <td>57.243816</td>\n",
       "      <td>113.2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19145 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id$Year  race_id    KettoNum      speed   Time  KakuteiJyuni  top3\n",
       "0         2018    34535  2015101022  58.064516   74.4            10     0\n",
       "1         2018    34535  2015103483  57.908847   74.6            11     0\n",
       "2         2018    34535  2015106010  59.178082   73.0             2     1\n",
       "3         2018    34535  2015102342  58.775510   73.5             6     0\n",
       "4         2018    34535  2015102323  57.142857   75.6            16     0\n",
       "...        ...      ...         ...        ...    ...           ...   ...\n",
       "19140     2018    35925  2014105425  58.378378  111.0             1     1\n",
       "19141     2018    35925  2014105543  57.857143  112.0             5     0\n",
       "19142     2018    35925  2011106130  57.754011  112.2             6     0\n",
       "19143     2018    35925  2012102418  57.497782  112.7             8     0\n",
       "19144     2018    35925  2013104045  57.243816  113.2            12     0\n",
       "\n",
       "[19145 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 78 ms\n"
     ]
    }
   ],
   "source": [
    "y_test_df = pd.read_csv('..\\\\feature_selection\\\\y_test_df.csv')\n",
    "y_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create X, y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kyori</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_1</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_1</th>\n",
       "      <th>id$RaceNum</th>\n",
       "      <th>TrackCD_52</th>\n",
       "      <th>TrackCD_17</th>\n",
       "      <th>JyokenInfo$SyubetuCD_18</th>\n",
       "      <th>GradeCD_</th>\n",
       "      <th>...</th>\n",
       "      <th>KS_ChokyosiCode_386.0</th>\n",
       "      <th>CH_Syotai_岩手</th>\n",
       "      <th>KS_Syotai_アメリカ</th>\n",
       "      <th>CH_Syotai_荒尾</th>\n",
       "      <th>KS_Syotai_フランス</th>\n",
       "      <th>KS_Syotai_川崎</th>\n",
       "      <th>KS_Syotai_笠松</th>\n",
       "      <th>id$JyoCD_1</th>\n",
       "      <th>KS_ChokyosiCode_365.0</th>\n",
       "      <th>CH_Syotai_川崎</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.560469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.372077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.893973</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.381357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.893973</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.372077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.787599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475191</th>\n",
       "      <td>1.863602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.787599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475192</th>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.372077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475193</th>\n",
       "      <td>-1.530097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.089118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475194</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.495360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475195</th>\n",
       "      <td>-0.560469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.203121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>475196 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Kyori  TenkoBaba$DirtBabaCD_0  TenkoBaba$SibaBabaCD_0  \\\n",
       "0      -0.560469                     1.0                     0.0   \n",
       "1       0.893973                     1.0                     0.0   \n",
       "2       0.893973                     1.0                     0.0   \n",
       "3       0.409159                     1.0                     0.0   \n",
       "4       0.166752                     0.0                     1.0   \n",
       "...          ...                     ...                     ...   \n",
       "475191  1.863602                     0.0                     1.0   \n",
       "475192  0.166752                     0.0                     1.0   \n",
       "475193 -1.530097                     0.0                     1.0   \n",
       "475194 -1.045283                     0.0                     1.0   \n",
       "475195 -0.560469                     1.0                     0.0   \n",
       "\n",
       "        TenkoBaba$SibaBabaCD_1  TenkoBaba$DirtBabaCD_1  id$RaceNum  \\\n",
       "0                          1.0                     0.0    1.372077   \n",
       "1                          0.0                     0.0   -0.381357   \n",
       "2                          1.0                     0.0    1.372077   \n",
       "3                          1.0                     0.0   -1.550314   \n",
       "4                          0.0                     1.0    0.787599   \n",
       "...                        ...                     ...         ...   \n",
       "475191                     0.0                     1.0    0.787599   \n",
       "475192                     0.0                     1.0    1.372077   \n",
       "475193                     0.0                     1.0   -0.089118   \n",
       "475194                     0.0                     1.0    0.495360   \n",
       "475195                     0.0                     0.0    0.203121   \n",
       "\n",
       "        TrackCD_52  TrackCD_17  JyokenInfo$SyubetuCD_18  GradeCD_   ...  \\\n",
       "0              0.0         0.0                      0.0        0.0  ...   \n",
       "1              0.0         1.0                      0.0        1.0  ...   \n",
       "2              0.0         0.0                      0.0        0.0  ...   \n",
       "3              0.0         0.0                      0.0        1.0  ...   \n",
       "4              0.0         0.0                      0.0        0.0  ...   \n",
       "...            ...         ...                      ...        ...  ...   \n",
       "475191         0.0         0.0                      0.0        0.0  ...   \n",
       "475192         0.0         0.0                      0.0        0.0  ...   \n",
       "475193         0.0         0.0                      0.0        1.0  ...   \n",
       "475194         0.0         0.0                      0.0        1.0  ...   \n",
       "475195         0.0         0.0                      0.0        1.0  ...   \n",
       "\n",
       "        KS_ChokyosiCode_386.0  CH_Syotai_岩手　　　　　　　　  KS_Syotai_アメリカ　　　　　　  \\\n",
       "0                         0.0                   0.0                   0.0   \n",
       "1                         0.0                   0.0                   0.0   \n",
       "2                         0.0                   0.0                   0.0   \n",
       "3                         0.0                   0.0                   0.0   \n",
       "4                         0.0                   0.0                   0.0   \n",
       "...                       ...                   ...                   ...   \n",
       "475191                    0.0                   0.0                   0.0   \n",
       "475192                    0.0                   0.0                   0.0   \n",
       "475193                    0.0                   0.0                   0.0   \n",
       "475194                    0.0                   0.0                   0.0   \n",
       "475195                    0.0                   0.0                   0.0   \n",
       "\n",
       "        CH_Syotai_荒尾　　　　　　　　  KS_Syotai_フランス　　　　　　  KS_Syotai_川崎　　　　　　　　  \\\n",
       "0                        0.0                   0.0                   0.0   \n",
       "1                        0.0                   0.0                   0.0   \n",
       "2                        0.0                   0.0                   0.0   \n",
       "3                        0.0                   0.0                   0.0   \n",
       "4                        0.0                   0.0                   0.0   \n",
       "...                      ...                   ...                   ...   \n",
       "475191                   0.0                   0.0                   0.0   \n",
       "475192                   0.0                   0.0                   0.0   \n",
       "475193                   0.0                   0.0                   0.0   \n",
       "475194                   0.0                   0.0                   0.0   \n",
       "475195                   0.0                   0.0                   0.0   \n",
       "\n",
       "        KS_Syotai_笠松　　　　　　　　  id$JyoCD_1  KS_ChokyosiCode_365.0  \\\n",
       "0                        0.0         0.0                    0.0   \n",
       "1                        0.0         0.0                    0.0   \n",
       "2                        0.0         0.0                    0.0   \n",
       "3                        0.0         0.0                    0.0   \n",
       "4                        0.0         1.0                    0.0   \n",
       "...                      ...         ...                    ...   \n",
       "475191                   0.0         0.0                    0.0   \n",
       "475192                   0.0         1.0                    0.0   \n",
       "475193                   0.0         0.0                    0.0   \n",
       "475194                   0.0         0.0                    0.0   \n",
       "475195                   0.0         0.0                    0.0   \n",
       "\n",
       "        CH_Syotai_川崎　　　　　　　　  \n",
       "0                        0.0  \n",
       "1                        0.0  \n",
       "2                        0.0  \n",
       "3                        0.0  \n",
       "4                        0.0  \n",
       "...                      ...  \n",
       "475191                   0.0  \n",
       "475192                   0.0  \n",
       "475193                   0.0  \n",
       "475194                   0.0  \n",
       "475195                   0.0  \n",
       "\n",
       "[475196 rows x 200 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 235 ms\n"
     ]
    }
   ],
   "source": [
    "drop_columns = ['race_id', 'KettoNum', 'id$Year', 'speed']\n",
    "X_train = train_data.drop(drop_columns, axis=1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    475196.000000\n",
       "mean         58.393319\n",
       "std           2.308153\n",
       "min          21.973550\n",
       "25%          56.942004\n",
       "50%          58.536585\n",
       "75%          59.916782\n",
       "max          66.666667\n",
       "Name: speed, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31 ms\n"
     ]
    }
   ],
   "source": [
    "y_train = train_data['speed']\n",
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEGCAYAAABPdROvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAd3UlEQVR4nO3df5BdZZ3n8fcHUIlo+E1XJslMxyWOQgJh6M1mhqmpO8Q1EZgJzkIZC02QbEWpsGJVtqYSZ2tHh00NzC5GsyXZjeIQUCekUJYUisoGbzluQWJAJATI0gW9pEmGbCRC2llSdPzuH+dpPLdzu/uG0+fevn0/r6pT99zvOc+5z3mA/vKc55zzKCIwMzN7u05qdQXMzKy9OZGYmVkhTiRmZlaIE4mZmRXiRGJmZoWc0uoKNNs555wT3d3d/PrXv+a0005rdXUmDLdHLbdHLbdHrU5sj8cff/xQRJxbb1vHJZLu7m527dpFtVqlUqm0ujoThtujltujltujVie2h6T/M9I2X9oyM7NCnEjMzKwQJxIzMyvEicTMzApxIjEzs0KcSMzMrBAnEjMzK8SJxMzMCnEiMTOzQkpPJJJOlvRzSQ+m72dJeljS8+nzzNy+ayX1StoraVEufqmk3WnbBklK8XdJujfFd0jqLvt8zKxzdK/53luLjawZPZKbgWdz39cA2yNiNrA9fUfSBcBS4EJgMXCHpJNTmY3ASmB2Whan+ArgcEScD6wHbiv3VMzMbLhSE4mkGcCVwNdz4SXA5rS+Gbg6F98SEUcj4kWgF5gvaRowNSIejWxe4LuHlRk61n3AwqHeipmZNUfZL238MvCXwHtzsa6IOAAQEQcknZfi04HHcvv1p9ibaX14fKjMvnSsQUmvAWcDh/KVkLSSrEdDV1cX1WqVgYEBqtVq0fObNNwetdwetTq1PVbPHXxrPX/+ndoeIyktkUi6CjgYEY9LqjRSpE4sRomPVqY2ELEJ2ATQ09MTlUqlI9/eORq3Ry23R61ObY/rc2MjfddV3lrv1PYYSZk9ksuAP5d0BXAqMFXSN4FXJE1LvZFpwMG0fz8wM1d+BrA/xWfUiefL9Es6BTgdeLWsEzIzs+OVNkYSEWsjYkZEdJMNoj8SEZ8AtgHL027LgQfS+jZgaboTaxbZoPrOdBnsiKQFafxj2bAyQ8e6Jv3GcT0SMzMrTysmtroV2CppBfAScC1AROyRtBV4BhgEVkXEsVTmRuAuYArwUFoA7gTukdRL1hNZ2qyTMDOzTFMSSURUgWpa/yWwcIT91gHr6sR3AXPqxN8gJSIzM2sNP9luZmaFOJGYmVkhrRgjMTObsPw6lBPnHomZmRXiHomZdTz3Qopxj8TMzApxIjEzs0KcSMzMrBAnEjMzK8SJxMzMCnEiMTOzQpxIzMysECcSMzMrxInEzMwKcSIxM7NC/IoUM+tIfi3K+CmtRyLpVEk7Jf1C0h5JX0zxL0h6WdKTabkiV2atpF5JeyUtysUvlbQ7bduQptwlTct7b4rvkNRd1vmYmVl9ZV7aOgpcHhEXA/OAxZIWpG3rI2JeWr4PIOkCsqlyLwQWA3dIOjntvxFYSTaP++y0HWAFcDgizgfWA7eVeD5mZlZHaYkkMgPp6zvSEqMUWQJsiYijEfEi0AvMlzQNmBoRj0ZEAHcDV+fKbE7r9wELh3orZmbWHKWOkaQexePA+cBXI2KHpI8AN0laBuwCVkfEYWA68FiueH+KvZnWh8dJn/sAImJQ0mvA2cChYfVYSdajoauri2q1ysDAANVqdTxPt625PWq5PWpNxvZYPXfwhPbPn/9kbI8iSk0kEXEMmCfpDOB+SXPILlPdQtY7uQW4HbgBqNeTiFHijLEtX49NwCaAnp6eqFQqVKtVKpXKCZ3PZOb2qOX2qDUZ2+P6Exxs77uu8tb6ZGyPIppy+29E/AqoAosj4pWIOBYRvwG+BsxPu/UDM3PFZgD7U3xGnXhNGUmnAKcDr5ZzFmZmVk+Zd22dm3oiSJoCfAh4Lo15DPko8HRa3wYsTXdizSIbVN8ZEQeAI5IWpPGPZcADuTLL0/o1wCNpHMXMbFx1r/neW4vVKvPS1jRgcxonOQnYGhEPSrpH0jyyS1B9wKcBImKPpK3AM8AgsCpdGgO4EbgLmAI8lBaAO4F7JPWS9USWlng+ZmZWR2mJJCKeAi6pE//kKGXWAevqxHcBc+rE3wCuLVZTMzMrwq9IMTOzQpxIzMysECcSMzMrxInEzMwKcSIxM7NCnEjMzKwQJxIzMyvEicTMzApxIjEzs0KcSMzMrBAnEjMzK8SJxMzMCil1Yiszs4nEr4Avh3skZmZWiBOJmZkV4kRiZmaFOJGYmVkhZc7ZfqqknZJ+IWmPpC+m+FmSHpb0fPo8M1dmraReSXslLcrFL5W0O23bkOZuJ83vfm+K75DUXdb5mJlZfWX2SI4Cl0fExcA8YLGkBcAaYHtEzAa2p+9IuoBszvULgcXAHWm+d4CNwEpgdloWp/gK4HBEnA+sB24r8XzMzKyO0hJJZAbS13ekJYAlwOYU3wxcndaXAFsi4mhEvAj0AvMlTQOmRsSjERHA3cPKDB3rPmDhUG/FzMyao9TnSFKP4nHgfOCrEbFDUldEHACIiAOSzku7TwceyxXvT7E30/rw+FCZfelYg5JeA84GDg2rx0qyHg1dXV1Uq1UGBgaoVqvjdq7tzu1Ry+1Ra7K0x+q5g+NynMnSHuOl1EQSEceAeZLOAO6XNGeU3ev1JGKU+GhlhtdjE7AJoKenJyqVCtVqlUqlMkp1Oovbo5bbo9ZkaY/rx+mBxLsWnzYp2mO8NOWurYj4FVAlG9t4JV2uIn0eTLv1AzNzxWYA+1N8Rp14TRlJpwCnA6+WcQ5mZlZfmXdtnZt6IkiaAnwIeA7YBixPuy0HHkjr24Cl6U6sWWSD6jvTZbAjkhak8Y9lw8oMHesa4JE0jmJmZk1S5qWtacDmNE5yErA1Ih6U9CiwVdIK4CXgWoCI2CNpK/AMMAisSpfGAG4E7gKmAA+lBeBO4B5JvWQ9kaUlno+ZtSG/X6t8pSWSiHgKuKRO/JfAwhHKrAPW1YnvAo4bX4mIN0iJyMysWXa//Npb4y19t17Z4tq0np9sNzOzQpxIzMysECcSMzMrxInEzMwKcSIxM7NCnEjMzKwQJxIzMyvEicTMzApxIjEzs0KcSMzMrBAnEjMzK8SJxMzMCnEiMTOzQpxIzMyskFKn2jUzawXPQdJc7pGYmVkhYyYSSbskrZJ05okcWNJMST+W9KykPZJuTvEvSHpZ0pNpuSJXZq2kXkl7JS3KxS+VtDtt25Cm3CVNy3tviu+Q1H0idTQzs+Ia6ZEsBX4H+JmkLZIWDf0hH8MgsDoiPggsAFZJuiBtWx8R89LyfYC0bSlwIbAYuCNN0wuwEVhJNo/77LQdYAVwOCLOB9YDtzVQLzMzG0djJpKI6I2IvwLeD3wb+AbwkqQvSjprlHIHIuKJtH4EeBaYPspPLQG2RMTRiHgR6AXmS5oGTI2IRyMigLuBq3NlNqf1+4CFDSY5MzMbJw0Ntku6CPgUcAXwHeBbwB8DjwDzGijfTTZ/+w7gMuAmScuAXWS9lsNkSeaxXLH+FHszrQ+Pkz73AUTEoKTXgLOBQ8N+fyVZj4auri6q1SoDAwNUq9VGTr8juD1quT1qtVt7rJ47WOrxu6b89jfaqV3KMmYikfQ48CvgTmBNRBxNm3ZIuqyB8u8hSz6fi4jXJW0EbgEifd4O3ADU60nEKHHG2PbbQMQmYBNAT09PVCoVqtUqlUplrOp3DLdHLbdHrXZrj+tLvmtr9dxBbt+d/fnsu65S6m+1g0Z6JNdGxAv1NkTEX4xWUNI7SD2YiPhuKvNKbvvXgAfT135gZq74DGB/is+oE8+X6Zd0CnA68GoD52RmZuOkkcH2fyvpjKEvks6U9J/GKpTGKu4Eno2IL+Xi03K7fRR4Oq1vA5amO7FmkQ2q74yIA8ARSQvSMZcBD+TKLE/r1wCPpHEUMzNrkkZ6JB+JiM8PfYmIw+mW3f8wRrnLgE8CuyU9mWKfBz4uaR7ZJag+4NPpuHskbQWeIbvja1VEHEvlbgTuAqYAD6UFskR1j6Resp7I0gbOx8zMxlEjieRkSe8aGhuRNAV411iFIuKn1B/D+P4oZdYB6+rEdwFz6sTfAK4dqy5mZlaeRhLJN4Htkv6erBdxA7+95dbMrKPlX8fSd+uVLaxJ64yZSCLi7yTtBhaS9TBuiYgfll4zMzNrCw09RxIR+XEJMzOztzTyrq2/kPS8pNckvS7piKTXm1E5MzOb+Brpkfwd8GcR8WzZlTEzs/bTyHMkrziJmJnZSBrpkeySdC/wP4Ch16Mw9KS6mZl1tkYSyVTgn4EP52IBOJGYmVlDt/9+qhkVMTOz9tTIXVvvl7Rd0tPp+0WSxno9ipmZdYhGBtu/BqwlmxeEiHgKv9PKzMySRhLJuyNi57BYubPGmJlZ22gkkRyS9C9IE0ZJugY4UGqtzMysbTRy19YqstkFPyDpZeBF4BOl1srMzNpGI3dtvQB8SNJpwEkRcaT8apmZnZjukqfXtZE1Mmf7fxz2HYCI+JuS6mRmZm2kkUtbv86tnwpcBfiVKWZmBjQw2B4Rt+eWdUAFmD5WOUkzJf1Y0rOS9ki6OcXPkvRweqPww5LOzJVZK6lX0l5Ji3LxSyXtTts2pLnbSfO735viOyR1n3gTmJlZEY3ctTXcu4H3NbDfILA6Ij4ILABWSboAWANsj4jZwPb0nbRtKXAhsBi4Q9LJ6VgbgZXA7LQsTvEVwOGIOB9YD9z2Ns7HzMwKaOTJ9t2SnkrLHmAv8JWxykXEgYh4Iq0fIbscNh1Ywm+n6t0MXJ3WlwBbIuJoRLwI9ALzJU0DpkbEoxERwN3Dygwd6z5g4VBvxczMmqORMZKrcuuDZK+VP6EHEtMlp0uAHUBXRByALNlIOi/tNh14LFesP8XeTOvD40Nl9qVjDUp6DTgbODTs91eS9Wjo6uqiWq0yMDBAtVo9kdOY1NwetdwetdqhPVbPbd5z0l1T6v/eRG+jsjSSSIbf7js1/z/9EfHqaIUlvQf4DvC5iHh9lA5DvQ0xSny0MrWBiE1kz8LQ09MTlUqFarVKpVIZreodxe1Ry+1Rqx3a4/om3v67eu4gt+8+/s9n33WVptVhImkkkTwBzAQOk/3hPgN4KW0LRhkvkfQOsiTyrdz8Ja9ImpZ6I9OAgynen35nyAxgf4rPqBPPl+mXdApwOjBqYjMzs/HVyGD7D8im2j0nIs4mu9T13YiYFRGjJREBdwLPRsSXcpu2AcvT+nLggVx8aboTaxbZoPrOdBnsiKQF6ZjLhpUZOtY1wCNpHMXMzJqkkR7Jv4yIzwx9iYiHJN3SQLnLgE8CuyU9mWKfB24FtkpaQdazuTYdd4+krcAzZGMxqyLiWCp3I3AXMAV4KC2QJap7JPWS9UT8VmIzsyZrJJEcSvOPfJPsUtYngF+OVSgifkr9MQyAhSOUWQesqxPfBcypE3+DlIjMzKw1Grm09XHgXOD+tJybYmZmZg29tPFV4GZJ74mIgSbUyczM2kgjDyT+kaRnyMYukHSxpDtKr5mZmbWFRi5trQcWkcZFIuIXwJ+UWSkzM2sfDb1rKyL2DQsdq7ujmZl1nEbu2ton6Y+AkPRO4LP4NfJmZsfJT67Vd+uVLaxJczXSI/kM2XS708meJJ+XvpuZmY3eI0mvcf9yRFzXpPqYmVmbGbVHkp4sPzdd0jIzMztOI2MkfcD/krSN3LS7w96fZWZmHWrEHomke9Lqx4AH077vzS1mZmaj9kgulfR7ZC9W/K9Nqo+ZmbWZ0RLJfyN7hfwsYFcuLsaYh8TMzDrHiIkkIjYAGyRtjIgbm1gnM7OGdDdxVkQb2ZjPkTiJmJnZaBp6RYqZmdlISkskkr4h6aCkp3OxL0h6WdKTabkit22tpF5JeyUtysUvlbQ7bduQptslTcl7b4rvkNRd1rmYmdnIyuyR3AUsrhNfHxHz0vJ9AEkXkE2Te2Eqc0d6qh5gI7CSbA732bljrgAOR8T5ZG8ovq2sEzEzs5GVlkgi4idk86g3YgmwJSKORsSLQC8wX9I0YGpEPBoRAdwNXJ0rszmt3wcsHOqtmJlZ8zTyZPt4u0nSMrJbildHxGGyF0I+ltunP8XeTOvD46TPfQARMSjpNeBs4NDwH5S0kqxXQ1dXF9VqlYGBAarV6nieV1tze9Rye9SaqO2xeu5gS363a8rYvz0R26sszU4kG4FbyJ5DuQW4HbiB7NmU4WKUOGNsqw1GbAI2AfT09ESlUqFarVKpVE6o8pOZ26OW26PWRG2P61t0++/quYPcvnv0P59911WaU5kJoKl3bUXEKxFxLCJ+A3wNmJ829QMzc7vOAPan+Iw68Zoykk4BTqfxS2lmZjZOmppI0pjHkI8CQ3d0bQOWpjuxZpENqu+MiAPAEUkL0vjHMuCBXJnlaf0a4JE0jmJm1nLda7731jLZlXZpS9I/ABXgHEn9wF8DFUnzyC5B9QGfBoiIPZK2As8Ag8Cq9Ap7gBvJ7gCbAjyUFoA7gXsk9ZL1RJaWdS5mZjay0hJJRHy8TvjOUfZfB6yrE98FzKkTfwO4tkgdzcysOD/ZbmZmhTiRmJlZIU4kZmZWiBOJmZkV4kRiZmaFtOIVKWZmb1snPJfRbtwjMTOzQpxIzMysECcSMzMrxInEzMwKcSIxM7NCnEjMzKwQJxIzMyvEicTMzApxIjEzs0L8ZLuZWcnyT+P33XplC2tSDvdIzMyskNISiaRvSDoo6elc7CxJD0t6Pn2emdu2VlKvpL2SFuXil0ranbZtSHO3k+Z3vzfFd0jqLutczMxsZGX2SO4CFg+LrQG2R8RsYHv6jqQLyOZcvzCVuUPSyanMRmAlMDstQ8dcARyOiPOB9cBtpZ2JmZmNqLREEhE/AV4dFl4CbE7rm4Grc/EtEXE0Il4EeoH5kqYBUyPi0YgI4O5hZYaOdR+wcKi3YmZmzdPswfauiDgAEBEHJJ2X4tOBx3L79afYm2l9eHyozL50rEFJrwFnA4eG/6iklWS9Grq6uqhWqwwMDFCtVsfrvNqe26OW26PWRGqP1XMHW10Fuqa8/XpMlHYcTxPlrq16PYkYJT5ameODEZuATQA9PT1RqVSoVqtUKpW3UdXJye1Ry+1RayK1x/UTYD6S1XMHuX332/vz2XddZXwrMwE0+66tV9LlKtLnwRTvB2bm9psB7E/xGXXiNWUknQKczvGX0szMrGTNTiTbgOVpfTnwQC6+NN2JNYtsUH1nugx2RNKCNP6xbFiZoWNdAzySxlHMzKyJSru0JekfgApwjqR+4K+BW4GtklYALwHXAkTEHklbgWeAQWBVRBxLh7qR7A6wKcBDaQG4E7hHUi9ZT2RpWediZmYjKy2RRMTHR9i0cIT91wHr6sR3AXPqxN8gJSIzm7w8R/vE5yfbzcysECcSMzMrxInEzMwKcSIxM7NCJsoDiWZmHWEyvlLePRIzMyvEicTMzApxIjEzs0KcSMzMrBAnEjMzK8R3bZnZhOPXorQX90jMzKwQJxIzMyvEicTMzApxIjEzs0KcSMzMrJCWJBJJfZJ2S3pS0q4UO0vSw5KeT59n5vZfK6lX0l5Ji3LxS9NxeiVtSNPxmplZE7WyR/KnETEvInrS9zXA9oiYDWxP35F0Adk0uhcCi4E7JJ2cymwEVpLN8T47bTczawvda7731tLOJtKlrSXA5rS+Gbg6F98SEUcj4kWgF5gvaRowNSIejYgA7s6VMTOzJmnVA4kB/EhSAP89IjYBXRFxACAiDkg6L+07HXgsV7Y/xd5M68Pjx5G0kqznQldXF9VqlYGBAarV6jieUntze9Rye9RqdnusnjvYtN96O7qmjH8d2/nft1YlkssiYn9KFg9Lem6UfeuNe8Qo8eODWaLaBNDT0xOVSoVqtUqlUjnBak9ebo9abo9azWiP2ss7E/ulG6vnDnL77vGtY991lXE9XjO15NJWROxPnweB+4H5wCvpchXp82DavR+YmSs+A9if4jPqxM3MrImankgknSbpvUPrwIeBp4FtwPK023LggbS+DVgq6V2SZpENqu9Ml8GOSFqQ7tZalitjZmZN0or+Yxdwf7pT9xTg2xHxA0k/A7ZKWgG8BFwLEBF7JG0FngEGgVURcSwd60bgLmAK8FBazMysiZqeSCLiBeDiOvFfAgtHKLMOWFcnvguYM951NDOzxk3sES0zm9Ta/fkJy0yk50jMzKwNOZGYmVkhvrRlZk3ly1mTj3skZmZWiHskZmYTQL6n1nfrlS2syYlzj8TMzApxj8TMSudxkcnNPRIzMyvEicTMzArxpS0zK4UvZ3UO90jMzKwQJxIzMyvEl7bMzCaYdnumxInEzMaNx0U6ky9tmZlZIe6RmFkh7oWUqx0uc7V9IpG0GPgKcDLw9Yi4tcVVMpv0nDxaY6ImlbZOJJJOBr4K/GugH/iZpG0R8Uxra2Y2+Th5TCzD/3m0MrG0dSIB5gO9aR54JG0BlgBOJGZjOJHEsHruIO3/52JyG+mfZzMSTLv/mzEd2Jf73g/8q+E7SVoJrExfByTtBc4BDpVew/bh9qjl9sj5rNujRju1h24bt0P93kgb2j2RqE4sjgtEbAI21RSUdkVET1kVazduj1puj1puj1puj1rtfvtvPzAz930GsL9FdTEz60jtnkh+BsyWNEvSO4GlwLYW18nMrKO09aWtiBiUdBPwQ7Lbf78REXsaLL5p7F06itujltujltujltsjRxHHDSmYmZk1rN0vbZmZWYs5kZiZWSGTPpFIminpx5KelbRH0s0pfpakhyU9nz7PbHVdm0HSqZJ2SvpFao8vpnhHtscQSSdL+rmkB9P3Tm+PPkm7JT0paVeKdWybSDpD0n2Snkt/S/6wk9tjuEmfSIBBYHVEfBBYAKySdAGwBtgeEbOB7el7JzgKXB4RFwPzgMWSFtC57THkZuDZ3PdObw+AP42IebnnJTq5Tb4C/CAiPgBcTPbvSie3R62I6KgFeIDs3Vx7gWkpNg3Y2+q6taAt3g08QfY2gI5tD7Lnj7YDlwMPpljHtkc65z7gnGGxjmwTYCrwIunmpE5vj3pLJ/RI3iKpG7gE2AF0RcQBgPR5Xgur1lTpMs6TwEHg4Yjo6PYAvgz8JfCbXKyT2wOyN0T8SNLj6RVD0Llt8j7g/wJ/ny5/fl3SaXRuexynYxKJpPcA3wE+FxGvt7o+rRQRxyJiHtn/ic+XNKfFVWoZSVcBByPi8VbXZYK5LCL+APgI2eXgP2l1hVroFOAPgI0RcQnwazr5MlYdHZFIJL2DLIl8KyK+m8KvSJqWtk8j+7/zjhIRvwKqwGI6tz0uA/5cUh+wBbhc0jfp3PYAICL2p8+DwP1kb9ru1DbpB/pTzx3gPrLE0qntcZxJn0gkCbgTeDYivpTbtA1YntaXk42dTHqSzpV0RlqfAnwIeI4ObY+IWBsRMyKim+wVO49ExCfo0PYAkHSapPcOrQMfBp6mQ9skIv4J2Cfp91NoIdlUFR3ZHvVM+ifbJf0x8I/Abn57DfzzZOMkW4HfBV4Cro2IV1tSySaSdBGwmeyVMicBWyPibySdTQe2R56kCvDvI+KqTm4PSe8j64VAdlnn2xGxrsPbZB7wdeCdwAvAp0j//dCB7THcpE8kZmZWrkl/acvMzMrlRGJmZoU4kZiZWSFOJGZmVogTiZmZFeJEYtbGJFUl9Yy9p1l5nEjMzKwQJxKzcZaeDP9emvPlaUkfS/N73Jbmgtkp6fy077mSviPpZ2m5LHeMb6TYzyUtSfEpkrZIekrSvcCUFp6qGZA9tWpm42sxsD8irgSQdDpwG/B6RMyXtIzsjcNXkc1zsT4ifirpd4EfAh8E/orsdS03pFfa7JT0P4FPA/8cEReltxQ80eRzMzuOn2w3G2eS3k+WELaSzW/yj+mlkJdHxAvpJaL/FBFnSzoI7M8VPxf4APBj4FSyidkAzgIWAX8LbIiIR9JvPQGsjIhdTTg1s7rcIzEbZxHxvyVdClwB/K2kHw1tyu+WPk8C/jAi/l/+GOllo/8mIvYOiw8/jlnLeYzEbJxJ+h2yy0/fBP4L2SvHAT6W+3w0rf8IuClXdl5a/SHw71JCQdIlKf4T4LoUmwNcVM5ZmDXOPRKz8TcX+M+SfgO8CdxINofFuyTtIPsfuI+nfT8LfFXSU2T/Pf4E+AxwC9k4ylMpmfSRjalsJJup7yngSWBnc07JbGQeIzFrgjRG0hMRh1pdF7Px5ktbZmZWiHskZmZWiHskZmZWiBOJmZkV4kRiZmaFOJGYmVkhTiRmZlbI/wfzh5lZbgUE/AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 469 ms\n"
     ]
    }
   ],
   "source": [
    "y_train.hist(bins=100);\n",
    "plt.xlabel('speed');\n",
    "plt.ylabel('frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kyori</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_1</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_1</th>\n",
       "      <th>id$RaceNum</th>\n",
       "      <th>TrackCD_52</th>\n",
       "      <th>TrackCD_17</th>\n",
       "      <th>JyokenInfo$SyubetuCD_18</th>\n",
       "      <th>GradeCD_</th>\n",
       "      <th>...</th>\n",
       "      <th>KS_ChokyosiCode_386.0</th>\n",
       "      <th>CH_Syotai_岩手</th>\n",
       "      <th>KS_Syotai_アメリカ</th>\n",
       "      <th>CH_Syotai_荒尾</th>\n",
       "      <th>KS_Syotai_フランス</th>\n",
       "      <th>KS_Syotai_川崎</th>\n",
       "      <th>KS_Syotai_笠松</th>\n",
       "      <th>id$JyoCD_1</th>\n",
       "      <th>KS_ChokyosiCode_365.0</th>\n",
       "      <th>CH_Syotai_川崎</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19140</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19141</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19142</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19143</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19144</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19145 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Kyori  TenkoBaba$DirtBabaCD_0  TenkoBaba$SibaBabaCD_0  \\\n",
       "0     -1.045283                     0.0                     1.0   \n",
       "1     -1.045283                     0.0                     1.0   \n",
       "2     -1.045283                     0.0                     1.0   \n",
       "3     -1.045283                     0.0                     1.0   \n",
       "4     -1.045283                     0.0                     1.0   \n",
       "...         ...                     ...                     ...   \n",
       "19140  0.409159                     0.0                     1.0   \n",
       "19141  0.409159                     0.0                     1.0   \n",
       "19142  0.409159                     0.0                     1.0   \n",
       "19143  0.409159                     0.0                     1.0   \n",
       "19144  0.409159                     0.0                     1.0   \n",
       "\n",
       "       TenkoBaba$SibaBabaCD_1  TenkoBaba$DirtBabaCD_1  id$RaceNum  TrackCD_52  \\\n",
       "0                         0.0                     1.0   -1.550314         0.0   \n",
       "1                         0.0                     1.0   -1.550314         0.0   \n",
       "2                         0.0                     1.0   -1.550314         0.0   \n",
       "3                         0.0                     1.0   -1.550314         0.0   \n",
       "4                         0.0                     1.0   -1.550314         0.0   \n",
       "...                       ...                     ...         ...         ...   \n",
       "19140                     0.0                     1.0    1.664316         0.0   \n",
       "19141                     0.0                     1.0    1.664316         0.0   \n",
       "19142                     0.0                     1.0    1.664316         0.0   \n",
       "19143                     0.0                     1.0    1.664316         0.0   \n",
       "19144                     0.0                     1.0    1.664316         0.0   \n",
       "\n",
       "       TrackCD_17  JyokenInfo$SyubetuCD_18  GradeCD_   ...  \\\n",
       "0             0.0                      0.0        1.0  ...   \n",
       "1             0.0                      0.0        1.0  ...   \n",
       "2             0.0                      0.0        1.0  ...   \n",
       "3             0.0                      0.0        1.0  ...   \n",
       "4             0.0                      0.0        1.0  ...   \n",
       "...           ...                      ...        ...  ...   \n",
       "19140         0.0                      0.0        1.0  ...   \n",
       "19141         0.0                      0.0        1.0  ...   \n",
       "19142         0.0                      0.0        1.0  ...   \n",
       "19143         0.0                      0.0        1.0  ...   \n",
       "19144         0.0                      0.0        1.0  ...   \n",
       "\n",
       "       KS_ChokyosiCode_386.0  CH_Syotai_岩手　　　　　　　　  KS_Syotai_アメリカ　　　　　　  \\\n",
       "0                        0.0                   0.0                   0.0   \n",
       "1                        0.0                   0.0                   0.0   \n",
       "2                        0.0                   0.0                   0.0   \n",
       "3                        0.0                   0.0                   0.0   \n",
       "4                        0.0                   0.0                   0.0   \n",
       "...                      ...                   ...                   ...   \n",
       "19140                    0.0                   0.0                   0.0   \n",
       "19141                    0.0                   0.0                   0.0   \n",
       "19142                    0.0                   0.0                   0.0   \n",
       "19143                    0.0                   0.0                   0.0   \n",
       "19144                    0.0                   0.0                   0.0   \n",
       "\n",
       "       CH_Syotai_荒尾　　　　　　　　  KS_Syotai_フランス　　　　　　  KS_Syotai_川崎　　　　　　　　  \\\n",
       "0                       0.0                   0.0                   0.0   \n",
       "1                       0.0                   0.0                   0.0   \n",
       "2                       0.0                   0.0                   0.0   \n",
       "3                       0.0                   0.0                   0.0   \n",
       "4                       0.0                   0.0                   0.0   \n",
       "...                     ...                   ...                   ...   \n",
       "19140                   0.0                   0.0                   0.0   \n",
       "19141                   0.0                   0.0                   0.0   \n",
       "19142                   0.0                   0.0                   0.0   \n",
       "19143                   0.0                   0.0                   0.0   \n",
       "19144                   0.0                   0.0                   0.0   \n",
       "\n",
       "       KS_Syotai_笠松　　　　　　　　  id$JyoCD_1  KS_ChokyosiCode_365.0  \\\n",
       "0                       0.0         0.0                    0.0   \n",
       "1                       0.0         0.0                    0.0   \n",
       "2                       0.0         0.0                    0.0   \n",
       "3                       0.0         0.0                    0.0   \n",
       "4                       0.0         0.0                    0.0   \n",
       "...                     ...         ...                    ...   \n",
       "19140                   0.0         0.0                    0.0   \n",
       "19141                   0.0         0.0                    0.0   \n",
       "19142                   0.0         0.0                    0.0   \n",
       "19143                   0.0         0.0                    0.0   \n",
       "19144                   0.0         0.0                    0.0   \n",
       "\n",
       "       CH_Syotai_川崎　　　　　　　　  \n",
       "0                       0.0  \n",
       "1                       0.0  \n",
       "2                       0.0  \n",
       "3                       0.0  \n",
       "4                       0.0  \n",
       "...                     ...  \n",
       "19140                   0.0  \n",
       "19141                   0.0  \n",
       "19142                   0.0  \n",
       "19143                   0.0  \n",
       "19144                   0.0  \n",
       "\n",
       "[19145 rows x 200 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31 ms\n"
     ]
    }
   ],
   "source": [
    "X_test = test_data.drop(drop_columns, axis=1)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    19145.000000\n",
       "mean        58.013408\n",
       "std          2.305421\n",
       "min         38.876890\n",
       "25%         56.509695\n",
       "50%         58.142665\n",
       "75%         59.558824\n",
       "max         65.573770\n",
       "Name: speed, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "y_test = test_data['speed']\n",
    "y_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 250 ms\n"
     ]
    }
   ],
   "source": [
    "y_test.hist(bins=100);\n",
    "plt.xlabel('speed');\n",
    "plt.ylabel('frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train with ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create and compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7_eizWaqi_7",
    "outputId": "4361457c-e661-4e75-8c78-3c1a7789b705"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def my_r2_score(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true) ) ) \n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "def BatchNorm():\n",
    "    return BatchNormalization(\n",
    "                momentum=0.95, \n",
    "                epsilon=0.005,\n",
    "                beta_initializer=RandomNormal(mean=0.0, stddev=0.05), \n",
    "                gamma_initializer=Constant(value=0.9)\n",
    "                )\n",
    "\n",
    "#model.add(BatchNorm())\n",
    "#model.add(Dropout(0.07))\n",
    "#model.add(Dense(units=num_2, activation='relu'))\n",
    "#model.add(BatchNorm())     \n",
    "\n",
    "def build_and_compile_model(X_train, num_units=100, activation='sigmoid'):\n",
    "    input_shape = X_train.shape[1] \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=num_units, activation=activation, kernel_initializer='he_normal',\n",
    "                    input_shape=(input_shape,)))\n",
    "    model.add(Dense(1))\n",
    "      \n",
    "    model.compile(loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError(), my_r2_score], optimizer=Optimizer.Adam(0.01)) \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setup callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6x6dteutin0",
    "outputId": "0b4eab09-31e5-4ef0-d9cd-105656a99abe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "class My_checkoint(Callback):\n",
    "        \n",
    "    def __init__(self, model, X_test, y_test, checkpoint_name):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.mode = model\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        cpn = checkpoint_name + format(epoch, '02d') + '-.hdf5'\n",
    "        #cpn = os.path.join(checkpoint_dir, 'model'+format(epoch, '02d') + '-.hdf5')\n",
    "        val_loss = self.mode.evaluate(self.X_test, self.y_test)\n",
    "        print('my_val_loss', val_loss)\n",
    "        self.mode.save(cpn)\n",
    "        \n",
    "def callback_model(model, checkpoint_name, logdir, X_test, y_test):\n",
    "  \n",
    "    _logdir = os.path.join(logdir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_callback = TensorBoard(_logdir, histogram_freq=1)\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                  factor=0.3,\n",
    "                                  patience=1,\n",
    "                                  mode='min',\n",
    "                                  verbose=1)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=7,\n",
    "                                   monitor='val_loss',\n",
    "                                   mode='min',\n",
    "                                   verbose=1)\n",
    "    \n",
    "    csv_logger = CSVLogger('log.log', separator=',', append=False)\n",
    "    \n",
    "    callbacks_list = [tensorboard_callback, reduce_lr, early_stopping, csv_logger, My_checkoint(model, X_test, y_test, checkpoint_name)]\n",
    "    \n",
    "    return callbacks_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xttwiHh4u0ES",
    "outputId": "31e678b8-9b64-4fb6-9154-525972b48aca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, checkpoint_name, logdir, batch_size=32, epochs=10, re_train=True):\n",
    "\n",
    "    if re_train:\n",
    "        # Clear old folder\n",
    "        %rmdir /q/s {logdir}\n",
    "        # Clear old file\n",
    "        path = checkpoint_name + '**'\n",
    "        all_path_files = glob(path)\n",
    "        for file in all_path_files:\n",
    "            os.remove(file)\n",
    "        # fit model\n",
    "        callbacks_list = callback_model(model, checkpoint_name, logdir, X_test, y_test)\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        #del model\n",
    "        \n",
    "    # Get best file by reading log file\n",
    "    df = pd.read_csv('log.log')\n",
    "    best_epoch = df.loc[df['val_loss']==df['val_loss'].min(), 'epoch'].values[0]\n",
    "    best_file = 'model-' + format(best_epoch, '02d') + '-.hdf5'\n",
    "    #best_file = os.path.join(checkpoint_dir, 'model'+format(best_epoch, '02d') + '-.hdf5')\n",
    "    model = load_model(best_file, custom_objects={'my_r2_score': my_r2_score})\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "    1/14850 [..............................] - ETA: 0s - loss: 3469.2051 - root_mean_squared_error: 58.9000 - my_r2_score: -630.6990WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "    2/14850 [..............................] - ETA: 40:27 - loss: 3439.9146 - root_mean_squared_error: 58.6508 - my_r2_score: -867.6533WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.3230s). Check your callbacks.\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.9075 - root_mean_squared_error: 0.9526 - my_r2_score: 0.4624\n",
      "my_val_loss [0.9074549078941345, 0.9526042938232422, 0.46239617466926575]\n",
      "14850/14850 [==============================] - 40s 3ms/step - loss: 6.7347 - root_mean_squared_error: 2.5951 - my_r2_score: -0.4230 - val_loss: 0.9075 - val_root_mean_squared_error: 0.9526 - val_my_r2_score: 0.4624\n",
      "Epoch 2/15\n",
      "14848/14850 [============================>.] - ETA: 0s - loss: 0.7349 - root_mean_squared_error: 0.8573 - my_r2_score: 0.8534\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 1.1217 - root_mean_squared_error: 1.0591 - my_r2_score: 0.3125\n",
      "my_val_loss [1.121716856956482, 1.0591113567352295, 0.31248167157173157]\n",
      "14850/14850 [==============================] - 42s 3ms/step - loss: 0.7349 - root_mean_squared_error: 0.8573 - my_r2_score: 0.8534 - val_loss: 1.1217 - val_root_mean_squared_error: 1.0591 - val_my_r2_score: 0.3125\n",
      "Epoch 3/15\n",
      "  1/599 [..............................] - ETA: 0s - loss: 0.4333 - root_mean_squared_error: 0.6582 - my_r2_score: 0.22538677WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.8161 - root_mean_squared_error: 0.9034 - my_r2_score: 0.5179\n",
      "my_val_loss [0.8160586953163147, 0.9033596515655518, 0.5178607702255249]\n",
      "14850/14850 [==============================] - 41s 3ms/step - loss: 0.6686 - root_mean_squared_error: 0.8177 - my_r2_score: 0.8677 - val_loss: 0.8161 - val_root_mean_squared_error: 0.9034 - val_my_r2_score: 0.5179\n",
      "Epoch 4/15\n",
      "14848/14850 [============================>.] - ETA: 0s - loss: 0.6568 - root_mean_squared_error: 0.8104 - my_r2_score: 0.8698\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "  1/599 [..............................] - ETA: 1s - loss: 0.4385 - root_mean_squared_error: 0.6622 - my_r2_score: 0.2160WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.8360 - root_mean_squared_error: 0.9144 - my_r2_score: 0.5060\n",
      "my_val_loss [0.8360360264778137, 0.9143500328063965, 0.5060019493103027]\n",
      "14850/14850 [==============================] - 40s 3ms/step - loss: 0.6568 - root_mean_squared_error: 0.8104 - my_r2_score: 0.8698 - val_loss: 0.8360 - val_root_mean_squared_error: 0.9144 - val_my_r2_score: 0.5060\n",
      "Epoch 5/15\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.7712 - root_mean_squared_error: 0.8782 - my_r2_score: 0.5459\n",
      "my_val_loss [0.7712075710296631, 0.8781842589378357, 0.5459275841712952]\n",
      "14850/14850 [==============================] - 40s 3ms/step - loss: 0.6294 - root_mean_squared_error: 0.7934 - my_r2_score: 0.8753 - val_loss: 0.7712 - val_root_mean_squared_error: 0.8782 - val_my_r2_score: 0.5459\n",
      "Epoch 6/15\n",
      "14842/14850 [============================>.] - ETA: 0s - loss: 0.6248 - root_mean_squared_error: 0.7904 - my_r2_score: 0.8760\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.7778 - root_mean_squared_error: 0.8819 - my_r2_score: 0.5407\n",
      "my_val_loss [0.7777546644210815, 0.8819040060043335, 0.5407440662384033]\n",
      "14850/14850 [==============================] - 41s 3ms/step - loss: 0.6248 - root_mean_squared_error: 0.7904 - my_r2_score: 0.8760 - val_loss: 0.7778 - val_root_mean_squared_error: 0.8819 - val_my_r2_score: 0.5407\n",
      "Epoch 7/15\n",
      "14848/14850 [============================>.] - ETA: 0s - loss: 0.6151 - root_mean_squared_error: 0.7843 - my_r2_score: 0.8782\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.7892 - root_mean_squared_error: 0.8883 - my_r2_score: 0.5336\n",
      "my_val_loss [0.7891647815704346, 0.8883494734764099, 0.5336031913757324]\n",
      "14850/14850 [==============================] - 41s 3ms/step - loss: 0.6151 - root_mean_squared_error: 0.7843 - my_r2_score: 0.8782 - val_loss: 0.7892 - val_root_mean_squared_error: 0.8883 - val_my_r2_score: 0.5336\n",
      "Epoch 8/15\n",
      "14837/14850 [============================>.] - ETA: 0s - loss: 0.6117 - root_mean_squared_error: 0.7821 - my_r2_score: 0.8790\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.7801 - root_mean_squared_error: 0.8832 - my_r2_score: 0.5386\n",
      "my_val_loss [0.7800612449645996, 0.8832107782363892, 0.5386125445365906]\n",
      "14850/14850 [==============================] - 41s 3ms/step - loss: 0.6117 - root_mean_squared_error: 0.7821 - my_r2_score: 0.8790 - val_loss: 0.7801 - val_root_mean_squared_error: 0.8832 - val_my_r2_score: 0.5386\n",
      "Epoch 9/15\n",
      "14844/14850 [============================>.] - ETA: 0s - loss: 0.6106 - root_mean_squared_error: 0.7814 - my_r2_score: 0.8792\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "  1/599 [..............................] - ETA: 0s - loss: 0.4724 - root_mean_squared_error: 0.6873 - my_r2_score: 0.1554WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.7791 - root_mean_squared_error: 0.8827 - my_r2_score: 0.5391\n",
      "my_val_loss [0.7790881991386414, 0.8826597332954407, 0.539069414138794]\n",
      "14850/14850 [==============================] - 41s 3ms/step - loss: 0.6106 - root_mean_squared_error: 0.7814 - my_r2_score: 0.8792 - val_loss: 0.7791 - val_root_mean_squared_error: 0.8827 - val_my_r2_score: 0.5391\n",
      "Epoch 10/15\n",
      "14840/14850 [============================>.] - ETA: 0s - loss: 0.6102 - root_mean_squared_error: 0.7812 - my_r2_score: 0.8795\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.7799 - root_mean_squared_error: 0.8831 - my_r2_score: 0.5386\n",
      "my_val_loss [0.7799464464187622, 0.8831457495689392, 0.538644015789032]\n",
      "14850/14850 [==============================] - 41s 3ms/step - loss: 0.6102 - root_mean_squared_error: 0.7812 - my_r2_score: 0.8795 - val_loss: 0.7799 - val_root_mean_squared_error: 0.8831 - val_my_r2_score: 0.5386\n",
      "Epoch 11/15\n",
      "14847/14850 [============================>.] - ETA: 0s - loss: 0.6101 - root_mean_squared_error: 0.7811 - my_r2_score: 0.8792\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.560999509019894e-07.\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.7840 - root_mean_squared_error: 0.8854 - my_r2_score: 0.5363\n",
      "my_val_loss [0.7840000987052917, 0.8854377865791321, 0.5363180041313171]\n",
      "14850/14850 [==============================] - 41s 3ms/step - loss: 0.6101 - root_mean_squared_error: 0.7811 - my_r2_score: 0.8792 - val_loss: 0.7840 - val_root_mean_squared_error: 0.8854 - val_my_r2_score: 0.5363\n",
      "Epoch 12/15\n",
      "14844/14850 [============================>.] - ETA: 0s - loss: 0.6101 - root_mean_squared_error: 0.7811 - my_r2_score: 0.8797\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.9682997844938655e-07.\n",
      "  1/599 [..............................] - ETA: 0s - loss: 0.4672 - root_mean_squared_error: 0.6835 - my_r2_score: 0.1647WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.7828 - root_mean_squared_error: 0.8848 - my_r2_score: 0.5370\n",
      "my_val_loss [0.7828477025032043, 0.8847867846488953, 0.537002444267273]\n",
      "14850/14850 [==============================] - 41s 3ms/step - loss: 0.6101 - root_mean_squared_error: 0.7811 - my_r2_score: 0.8797 - val_loss: 0.7828 - val_root_mean_squared_error: 0.8848 - val_my_r2_score: 0.5370\n",
      "Epoch 00012: early stopping\n",
      "time: 8min 10s\n"
     ]
    }
   ],
   "source": [
    "checkpoint_name = 'model-'\n",
    "baseDir = os.path.abspath(os.getcwd())\n",
    "logs_name = 'training_logs'\n",
    "logdir = os.path.join(baseDir, logs_name)\n",
    "#checkpoint_dir = os.path.join(baseDir, checkpoint_name)\n",
    "model = build_and_compile_model(X_train, num_units=100, activation='sigmoid')\n",
    "\n",
    "model = train_model(model, X_train, y_train, X_test, y_test, checkpoint_name, logdir,\n",
    "                    batch_size=32, epochs=15, re_train=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sai số rmse:                    0.792\n",
      "Hệ số xác định r2-score: 0.882\n",
      "Tỉ lệ True positive:           0.327\n",
      "time: 26.2 s\n"
     ]
    }
   ],
   "source": [
    "# On train\n",
    "train_result_df = evaluate(model, X_train, y_train_df)\n",
    "#train_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sai số rmse:                    0.878\n",
      "Hệ số xác định r2-score: 0.855\n",
      "Tỉ lệ True positive:           0.317\n",
      "time: 406 ms\n"
     ]
    }
   ],
   "source": [
    "# On test\n",
    "test_result_df = evaluate(model, X_test, y_test_df)\n",
    "#test_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">Nhận xét:\n",
    "    Tỉ lệ True Positive cần phải cải thiện nhiều"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>lr</th>\n",
       "      <th>my_r2_score</th>\n",
       "      <th>root_mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_my_r2_score</th>\n",
       "      <th>val_root_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6.523834</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>-0.298268</td>\n",
       "      <td>2.554180</td>\n",
       "      <td>0.827129</td>\n",
       "      <td>0.509247</td>\n",
       "      <td>0.909467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.736009</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.853567</td>\n",
       "      <td>0.857910</td>\n",
       "      <td>0.802817</td>\n",
       "      <td>0.523510</td>\n",
       "      <td>0.896001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.720937</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.856508</td>\n",
       "      <td>0.849080</td>\n",
       "      <td>0.822729</td>\n",
       "      <td>0.515926</td>\n",
       "      <td>0.907044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.662295</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.868737</td>\n",
       "      <td>0.813815</td>\n",
       "      <td>0.774598</td>\n",
       "      <td>0.545524</td>\n",
       "      <td>0.880112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.651887</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.871103</td>\n",
       "      <td>0.807395</td>\n",
       "      <td>0.783494</td>\n",
       "      <td>0.538678</td>\n",
       "      <td>0.885152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.629045</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.875474</td>\n",
       "      <td>0.793124</td>\n",
       "      <td>0.775176</td>\n",
       "      <td>0.543746</td>\n",
       "      <td>0.880441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.620113</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.877099</td>\n",
       "      <td>0.787473</td>\n",
       "      <td>0.818301</td>\n",
       "      <td>0.517743</td>\n",
       "      <td>0.904600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.617075</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.877736</td>\n",
       "      <td>0.785541</td>\n",
       "      <td>0.787645</td>\n",
       "      <td>0.536392</td>\n",
       "      <td>0.887493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.616007</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.878033</td>\n",
       "      <td>0.784861</td>\n",
       "      <td>0.808334</td>\n",
       "      <td>0.523768</td>\n",
       "      <td>0.899074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.615736</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.878078</td>\n",
       "      <td>0.784689</td>\n",
       "      <td>0.803857</td>\n",
       "      <td>0.526528</td>\n",
       "      <td>0.896581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.615633</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.878216</td>\n",
       "      <td>0.784623</td>\n",
       "      <td>0.800409</td>\n",
       "      <td>0.528633</td>\n",
       "      <td>0.894656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch      loss        lr  my_r2_score  root_mean_squared_error  val_loss  \\\n",
       "0       1  6.523834  0.010000    -0.298268                 2.554180  0.827129   \n",
       "1       2  0.736009  0.010000     0.853567                 0.857910  0.802817   \n",
       "2       3  0.720937  0.010000     0.856508                 0.849080  0.822729   \n",
       "3       4  0.662295  0.003000     0.868737                 0.813815  0.774598   \n",
       "4       5  0.651887  0.003000     0.871103                 0.807395  0.783494   \n",
       "5       6  0.629045  0.000900     0.875474                 0.793124  0.775176   \n",
       "6       7  0.620113  0.000270     0.877099                 0.787473  0.818301   \n",
       "7       8  0.617075  0.000081     0.877736                 0.785541  0.787645   \n",
       "8       9  0.616007  0.000024     0.878033                 0.784861  0.808334   \n",
       "9      10  0.615736  0.000007     0.878078                 0.784689  0.803857   \n",
       "10     11  0.615633  0.000002     0.878216                 0.784623  0.800409   \n",
       "\n",
       "    val_my_r2_score  val_root_mean_squared_error  \n",
       "0          0.509247                     0.909467  \n",
       "1          0.523510                     0.896001  \n",
       "2          0.515926                     0.907044  \n",
       "3          0.545524                     0.880112  \n",
       "4          0.538678                     0.885152  \n",
       "5          0.543746                     0.880441  \n",
       "6          0.517743                     0.904600  \n",
       "7          0.536392                     0.887493  \n",
       "8          0.523768                     0.899074  \n",
       "9          0.526528                     0.896581  \n",
       "10         0.528633                     0.894656  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "log_data = pd.read_csv('log.log')\n",
    "log_data['epoch'] = log_data['epoch'] + 1\n",
    "log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 360 ms\n"
     ]
    }
   ],
   "source": [
    "def plot_loss(log_data):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    #fig.add_subplot(1, 2, 1)\n",
    "    sns.lineplot(x='epoch', y='root_mean_squared_error', data=log_data, marker='s', label='train_loss');\n",
    "    sns.lineplot(x='epoch', y='val_root_mean_squared_error', data=log_data, marker='s', label='val_loss');\n",
    "    plt.xlabel('Epoch', size=13)\n",
    "    plt.xticks(np.arange(1, log_data['epoch'].max()+1, step=1))\n",
    "    plt.ylabel('Error [speed]', size=13)\n",
    "    plt.legend()\n",
    "    plt.title('Learning-curve', size=15, y=1.02)\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_loss(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 297 ms\n"
     ]
    }
   ],
   "source": [
    "def plot_r2(log_data):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    #fig.add_subplot(1, 2, 1)\n",
    "    sns.lineplot(x='epoch', y='my_r2_score', data=log_data, marker='s', label='train_score');\n",
    "    sns.lineplot(x='epoch', y='val_my_r2_score', data=log_data, marker='s', label='val_score');\n",
    "    plt.xlabel('Epoch', size=13)\n",
    "    plt.xticks(np.arange(1, log_data['epoch'].max()+1, step=1))\n",
    "    plt.ylabel('rate', size=13)\n",
    "    plt.legend()\n",
    "    plt.title('Score-curve', size=15, y=1.02)\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_r2(log_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tune parameter\n",
    "- Điều chỉnh các siêu tham số bằng phương pháp BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 140 ms\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners.bayesian import BayesianOptimization\n",
    "from kerastuner import HyperModel\n",
    "\n",
    "class RegressionHyperModel(HyperModel):\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            Dense(\n",
    "                units=hp.Int('units', 100, 300, 5, default=100),\n",
    "                activation=hp.Choice(\n",
    "                    'activation',\n",
    "                    values=['relu', 'sigmoid', 'tanh'],\n",
    "                    default='sigmoid'),\n",
    "                kernel_initializer='he_normal',\n",
    "                input_shape=self.input_shape\n",
    "            )\n",
    "        )\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "                                           optimizer=Optimizer.Adam(0.01)) \n",
    "        return model\n",
    "    \n",
    "class MyTuner(BayesianOptimization):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 16, 128, step=16)\n",
    "        #kwargs['epochs'] = trial.hyperparameters.Int('epochs', 10, 20, 5)\n",
    "        super(MyTuner, self).run_trial(trial, *args, **kwargs)    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">Nhận xét:\n",
    "    - Tổ hợp các trường hợp cần tìm kiếm ta có tất cả 40(no_units)*3(no_activation)*8(no_batch_size) = 960 lượt cần tìm kiếm\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project .\\basic_search\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from .\\basic_search\\tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "time: 375 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_shape = (X_train.shape[1], )\n",
    "hypermodel = RegressionHyperModel(input_shape)\n",
    "project_name = 'basic_search'\n",
    "\n",
    "tuner = MyTuner(\n",
    "            hypermodel,\n",
    "            objective='val_loss',\n",
    "            max_trials=10,\n",
    "            executions_per_trial=2,\n",
    "            seed=42,\n",
    "            project_name=project_name,\n",
    "            overwrite=False\n",
    "            )\n",
    "tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=15, callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=3)], verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các bộ tham số tốt nhất\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>activation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>275</td>\n",
       "      <td>128</td>\n",
       "      <td>sigmoid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>96</td>\n",
       "      <td>sigmoid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  batch_size activation\n",
       "0    275         128    sigmoid\n",
       "1    300         128       tanh\n",
       "2    300          96    sigmoid"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "list_best_hp = tuner.get_best_hyperparameters(num_trials=3)\n",
    "list_best_units = []\n",
    "list_best_batch_size = []\n",
    "list_best_activation = []\n",
    "\n",
    "for best_hp in list_best_hp:\n",
    "    best_units = best_hp.get('units')\n",
    "    list_best_units.append(best_units)\n",
    "    best_batch_size = best_hp.get('batch_size')\n",
    "    list_best_batch_size.append(best_batch_size)\n",
    "    best_activation = best_hp.get('activation')\n",
    "    list_best_activation.append(best_activation)\n",
    "\n",
    "hp_result = pd.DataFrame({'units':list_best_units,\n",
    "                            'batch_size': list_best_batch_size,\n",
    "                            'activation': list_best_activation\n",
    "                            })\n",
    "print('Các bộ tham số tốt nhất')\n",
    "hp_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">Phương pháp:\n",
    "    - Ta train lại 3 mô hình tốt nhất để đánh giá\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "   2/3713 [..............................] - ETA: 20:45 - loss: 3418.5300 - root_mean_squared_error: 58.4682 - my_r2_score: -584.3889WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.6660s). Check your callbacks.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7981 - root_mean_squared_error: 0.8934 - my_r2_score: 0.7946\n",
      "my_val_loss [0.7981205582618713, 0.8933759331703186, 0.7945979237556458]\n",
      "3713/3713 [==============================] - 12s 3ms/step - loss: 10.4131 - root_mean_squared_error: 3.2269 - my_r2_score: -0.9964 - val_loss: 0.7981 - val_root_mean_squared_error: 0.8934 - val_my_r2_score: 0.7946\n",
      "Epoch 2/15\n",
      "3700/3713 [============================>.] - ETA: 0s - loss: 0.6968 - root_mean_squared_error: 0.8347 - my_r2_score: 0.8684\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.8564 - root_mean_squared_error: 0.9254 - my_r2_score: 0.7751\n",
      "my_val_loss [0.8563973903656006, 0.925417423248291, 0.7750604748725891]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.6966 - root_mean_squared_error: 0.8346 - my_r2_score: 0.8685 - val_loss: 0.8564 - val_root_mean_squared_error: 0.9254 - val_my_r2_score: 0.7751\n",
      "Epoch 3/15\n",
      "  1/150 [..............................] - ETA: 0s - loss: 1.2748 - root_mean_squared_error: 1.1291 - my_r2_score: 0.683686WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7349 - root_mean_squared_error: 0.8572 - my_r2_score: 0.8098\n",
      "my_val_loss [0.7348592281341553, 0.857239305973053, 0.8097913861274719]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.6436 - root_mean_squared_error: 0.8022 - my_r2_score: 0.8785 - val_loss: 0.7349 - val_root_mean_squared_error: 0.8572 - val_my_r2_score: 0.8098\n",
      "Epoch 4/15\n",
      "3704/3713 [============================>.] - ETA: 0s - loss: 0.6345 - root_mean_squared_error: 0.7965 - my_r2_score: 0.8804\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7756 - root_mean_squared_error: 0.8807 - my_r2_score: 0.7990\n",
      "my_val_loss [0.7755750417709351, 0.8806673884391785, 0.7990131378173828]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.6344 - root_mean_squared_error: 0.7965 - my_r2_score: 0.8804 - val_loss: 0.7756 - val_root_mean_squared_error: 0.8807 - val_my_r2_score: 0.7990\n",
      "Epoch 5/15\n",
      "3709/3713 [============================>.] - ETA: 0s - loss: 0.6090 - root_mean_squared_error: 0.7804 - my_r2_score: 0.8851\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7849 - root_mean_squared_error: 0.8859 - my_r2_score: 0.7974\n",
      "my_val_loss [0.7848687767982483, 0.885928213596344, 0.797433078289032]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.6089 - root_mean_squared_error: 0.7803 - my_r2_score: 0.8852 - val_loss: 0.7849 - val_root_mean_squared_error: 0.8859 - val_my_r2_score: 0.7974\n",
      "Epoch 6/15\n",
      "3711/3713 [============================>.] - ETA: 0s - loss: 0.5996 - root_mean_squared_error: 0.7743 - my_r2_score: 0.8870\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.8060 - root_mean_squared_error: 0.8978 - my_r2_score: 0.7907\n",
      "my_val_loss [0.8060434460639954, 0.8977992534637451, 0.7907118797302246]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.5996 - root_mean_squared_error: 0.7743 - my_r2_score: 0.8870 - val_loss: 0.8060 - val_root_mean_squared_error: 0.8978 - val_my_r2_score: 0.7907\n",
      "Epoch 7/15\n",
      "3711/3713 [============================>.] - ETA: 0s - loss: 0.5962 - root_mean_squared_error: 0.7721 - my_r2_score: 0.8877\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "  1/150 [..............................] - ETA: 0s - loss: 1.1937 - root_mean_squared_error: 1.0925 - my_r2_score: 0.7037WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7591 - root_mean_squared_error: 0.8713 - my_r2_score: 0.8038\n",
      "my_val_loss [0.7591094970703125, 0.8712689280509949, 0.8038065433502197]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.5962 - root_mean_squared_error: 0.7721 - my_r2_score: 0.8877 - val_loss: 0.7591 - val_root_mean_squared_error: 0.8713 - val_my_r2_score: 0.8038\n",
      "Epoch 8/15\n",
      "3703/3713 [============================>.] - ETA: 0s - loss: 0.5950 - root_mean_squared_error: 0.7714 - my_r2_score: 0.8878\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7589 - root_mean_squared_error: 0.8712 - my_r2_score: 0.8039\n",
      "my_val_loss [0.7589455246925354, 0.8711748123168945, 0.8038511276245117]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.5949 - root_mean_squared_error: 0.7713 - my_r2_score: 0.8878 - val_loss: 0.7589 - val_root_mean_squared_error: 0.8712 - val_my_r2_score: 0.8039\n",
      "Epoch 9/15\n",
      "3706/3713 [============================>.] - ETA: 0s - loss: 0.5943 - root_mean_squared_error: 0.7709 - my_r2_score: 0.8881\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7649 - root_mean_squared_error: 0.8746 - my_r2_score: 0.8022\n",
      "my_val_loss [0.7648914456367493, 0.8745807409286499, 0.8022005558013916]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.5946 - root_mean_squared_error: 0.7711 - my_r2_score: 0.8880 - val_loss: 0.7649 - val_root_mean_squared_error: 0.8746 - val_my_r2_score: 0.8022\n",
      "Epoch 10/15\n",
      "3704/3713 [============================>.] - ETA: 0s - loss: 0.5943 - root_mean_squared_error: 0.7709 - my_r2_score: 0.8879\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.560999509019894e-07.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7635 - root_mean_squared_error: 0.8738 - my_r2_score: 0.8026\n",
      "my_val_loss [0.7634521126747131, 0.8737574815750122, 0.8026096820831299]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.5945 - root_mean_squared_error: 0.7710 - my_r2_score: 0.8879 - val_loss: 0.7635 - val_root_mean_squared_error: 0.8738 - val_my_r2_score: 0.8026\n",
      "Epoch 00010: early stopping\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.7349 - root_mean_squared_error: 0.8572 - my_r2_score: 0.5705\n",
      "Epoch 1/15\n",
      "   2/3713 [..............................] - ETA: 24:25 - loss: 3125.2473 - root_mean_squared_error: 55.9039 - my_r2_score: -585.4337WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.7867s). Check your callbacks.\n",
      "  1/150 [..............................] - ETA: 0s - loss: 1.2604 - root_mean_squared_error: 1.1227 - my_r2_score: 0.6872551WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7651 - root_mean_squared_error: 0.8747 - my_r2_score: 0.7999\n",
      "my_val_loss [0.7650836110115051, 0.874690592288971, 0.7998557090759277]\n",
      "3713/3713 [==============================] - 12s 3ms/step - loss: 6.0147 - root_mean_squared_error: 2.4525 - my_r2_score: -0.1513 - val_loss: 0.7651 - val_root_mean_squared_error: 0.8747 - val_my_r2_score: 0.7999\n",
      "Epoch 2/15\n",
      "3708/3713 [============================>.] - ETA: 0s - loss: 0.7495 - root_mean_squared_error: 0.8657 - my_r2_score: 0.8583\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.8610 - root_mean_squared_error: 0.9279 - my_r2_score: 0.7759\n",
      "my_val_loss [0.8609556555747986, 0.9278769493103027, 0.7759034037590027]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.7494 - root_mean_squared_error: 0.8657 - my_r2_score: 0.8583 - val_loss: 0.8610 - val_root_mean_squared_error: 0.9279 - val_my_r2_score: 0.7759\n",
      "Epoch 3/15\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7584 - root_mean_squared_error: 0.8708 - my_r2_score: 0.8049\n",
      "my_val_loss [0.7583760023117065, 0.8708478808403015, 0.8048928380012512]\n",
      "3713/3713 [==============================] - 10s 3ms/step - loss: 0.6763 - root_mean_squared_error: 0.8224 - my_r2_score: 0.8724 - val_loss: 0.7584 - val_root_mean_squared_error: 0.8708 - val_my_r2_score: 0.8049\n",
      "Epoch 4/15\n",
      "3711/3713 [============================>.] - ETA: 0s - loss: 0.6630 - root_mean_squared_error: 0.8142 - my_r2_score: 0.8748\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7640 - root_mean_squared_error: 0.8741 - my_r2_score: 0.8018\n",
      "my_val_loss [0.7640231847763062, 0.8740841746330261, 0.8017900586128235]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.6630 - root_mean_squared_error: 0.8142 - my_r2_score: 0.8748 - val_loss: 0.7640 - val_root_mean_squared_error: 0.8741 - val_my_r2_score: 0.8018\n",
      "Epoch 5/15\n",
      "3713/3713 [==============================] - ETA: 0s - loss: 0.6241 - root_mean_squared_error: 0.7900 - my_r2_score: 0.8823\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.7919 - root_mean_squared_error: 0.8899 - my_r2_score: 0.7937\n",
      "my_val_loss [0.7919064164161682, 0.8898912668228149, 0.7937155365943909]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.6241 - root_mean_squared_error: 0.7900 - my_r2_score: 0.8823 - val_loss: 0.7919 - val_root_mean_squared_error: 0.8899 - val_my_r2_score: 0.7937\n",
      "Epoch 6/15\n",
      "3709/3713 [============================>.] - ETA: 0s - loss: 0.6059 - root_mean_squared_error: 0.7784 - my_r2_score: 0.8858\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7616 - root_mean_squared_error: 0.8727 - my_r2_score: 0.8028\n",
      "my_val_loss [0.7615768313407898, 0.8726837038993835, 0.8028011322021484]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.6060 - root_mean_squared_error: 0.7784 - my_r2_score: 0.8858 - val_loss: 0.7616 - val_root_mean_squared_error: 0.8727 - val_my_r2_score: 0.8028\n",
      "Epoch 7/15\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7526 - root_mean_squared_error: 0.8675 - my_r2_score: 0.8054\n",
      "my_val_loss [0.7526059746742249, 0.8675286769866943, 0.8053680658340454]\n",
      "3713/3713 [==============================] - 10s 3ms/step - loss: 0.5993 - root_mean_squared_error: 0.7741 - my_r2_score: 0.8871 - val_loss: 0.7526 - val_root_mean_squared_error: 0.8675 - val_my_r2_score: 0.8054\n",
      "Epoch 8/15\n",
      "3712/3713 [============================>.] - ETA: 0s - loss: 0.5980 - root_mean_squared_error: 0.7733 - my_r2_score: 0.8873\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7536 - root_mean_squared_error: 0.8681 - my_r2_score: 0.8052\n",
      "my_val_loss [0.753633439540863, 0.8681206107139587, 0.8052124381065369]\n",
      "3713/3713 [==============================] - 10s 3ms/step - loss: 0.5980 - root_mean_squared_error: 0.7733 - my_r2_score: 0.8873 - val_loss: 0.7536 - val_root_mean_squared_error: 0.8681 - val_my_r2_score: 0.8052\n",
      "Epoch 9/15\n",
      "3711/3713 [============================>.] - ETA: 0s - loss: 0.5960 - root_mean_squared_error: 0.7720 - my_r2_score: 0.8875\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7564 - root_mean_squared_error: 0.8697 - my_r2_score: 0.8044\n",
      "my_val_loss [0.7563824653625488, 0.8697025179862976, 0.804398775100708]\n",
      "3713/3713 [==============================] - 10s 3ms/step - loss: 0.5959 - root_mean_squared_error: 0.7720 - my_r2_score: 0.8875 - val_loss: 0.7564 - val_root_mean_squared_error: 0.8697 - val_my_r2_score: 0.8044\n",
      "Epoch 10/15\n",
      "3706/3713 [============================>.] - ETA: 0s - loss: 0.5953 - root_mean_squared_error: 0.7716 - my_r2_score: 0.8877\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7561 - root_mean_squared_error: 0.8695 - my_r2_score: 0.8045\n",
      "my_val_loss [0.7560731172561646, 0.8695246577262878, 0.8045148849487305]\n",
      "3713/3713 [==============================] - 10s 3ms/step - loss: 0.5953 - root_mean_squared_error: 0.7715 - my_r2_score: 0.8877 - val_loss: 0.7561 - val_root_mean_squared_error: 0.8695 - val_my_r2_score: 0.8045\n",
      "Epoch 11/15\n",
      "3707/3713 [============================>.] - ETA: 0s - loss: 0.5950 - root_mean_squared_error: 0.7713 - my_r2_score: 0.8877\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.560999509019894e-07.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7532 - root_mean_squared_error: 0.8679 - my_r2_score: 0.8053\n",
      "my_val_loss [0.7532153129577637, 0.8678798079490662, 0.8052836656570435]\n",
      "3713/3713 [==============================] - 10s 3ms/step - loss: 0.5950 - root_mean_squared_error: 0.7714 - my_r2_score: 0.8877 - val_loss: 0.7532 - val_root_mean_squared_error: 0.8679 - val_my_r2_score: 0.8053\n",
      "Epoch 12/15\n",
      "3711/3713 [============================>.] - ETA: 0s - loss: 0.5949 - root_mean_squared_error: 0.7713 - my_r2_score: 0.8878\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.9682997844938655e-07.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7533 - root_mean_squared_error: 0.8679 - my_r2_score: 0.8053\n",
      "my_val_loss [0.7533164620399475, 0.8679380416870117, 0.8052546381950378]\n",
      "3713/3713 [==============================] - 10s 3ms/step - loss: 0.5950 - root_mean_squared_error: 0.7713 - my_r2_score: 0.8878 - val_loss: 0.7533 - val_root_mean_squared_error: 0.8679 - val_my_r2_score: 0.8053\n",
      "Epoch 13/15\n",
      "3708/3713 [============================>.] - ETA: 0s - loss: 0.5948 - root_mean_squared_error: 0.7712 - my_r2_score: 0.8878\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 5.9048991829513396e-08.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7528 - root_mean_squared_error: 0.8677 - my_r2_score: 0.8054\n",
      "my_val_loss [0.7528181076049805, 0.8676509261131287, 0.8053885102272034]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.5949 - root_mean_squared_error: 0.7713 - my_r2_score: 0.8878 - val_loss: 0.7528 - val_root_mean_squared_error: 0.8677 - val_my_r2_score: 0.8054\n",
      "Epoch 14/15\n",
      "3708/3713 [============================>.] - ETA: 0s - loss: 0.5950 - root_mean_squared_error: 0.7714 - my_r2_score: 0.8877\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.771469797517966e-08.\n",
      "  1/150 [..............................] - ETA: 0s - loss: 1.2009 - root_mean_squared_error: 1.0958 - my_r2_score: 0.7019WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7527 - root_mean_squared_error: 0.8676 - my_r2_score: 0.8054\n",
      "my_val_loss [0.7527303099632263, 0.8676003217697144, 0.8054121732711792]\n",
      "3713/3713 [==============================] - 10s 3ms/step - loss: 0.5949 - root_mean_squared_error: 0.7713 - my_r2_score: 0.8878 - val_loss: 0.7527 - val_root_mean_squared_error: 0.8676 - val_my_r2_score: 0.8054\n",
      "Epoch 00014: early stopping\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.7526 - root_mean_squared_error: 0.8675 - my_r2_score: 0.5528\n",
      "Epoch 1/15\n",
      "   2/4950 [..............................] - ETA: 32:44 - loss: 3204.1211 - root_mean_squared_error: 56.6050 - my_r2_score: -440.6477WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.7892s). Check your callbacks.\n",
      "  1/200 [..............................] - ETA: 0s - loss: 1.4417 - root_mean_squared_error: 1.2007 - my_r2_score: 0.6302749WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8173 - root_mean_squared_error: 0.9040 - my_r2_score: 0.7579\n",
      "my_val_loss [0.8173025250434875, 0.9040478467941284, 0.7579123973846436]\n",
      "4950/4950 [==============================] - 14s 3ms/step - loss: 6.5412 - root_mean_squared_error: 2.5576 - my_r2_score: -0.2717 - val_loss: 0.8173 - val_root_mean_squared_error: 0.9040 - val_my_r2_score: 0.7579\n",
      "Epoch 2/15\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7514 - root_mean_squared_error: 0.8668 - my_r2_score: 0.7767\n",
      "my_val_loss [0.7514128684997559, 0.8668407201766968, 0.7767360806465149]\n",
      "4950/4950 [==============================] - 13s 3ms/step - loss: 0.7046 - root_mean_squared_error: 0.8394 - my_r2_score: 0.8663 - val_loss: 0.7514 - val_root_mean_squared_error: 0.8668 - val_my_r2_score: 0.7767\n",
      "Epoch 3/15\n",
      "4933/4950 [============================>.] - ETA: 0s - loss: 0.6837 - root_mean_squared_error: 0.8269 - my_r2_score: 0.8702\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0188 - root_mean_squared_error: 1.0093 - my_r2_score: 0.6901\n",
      "my_val_loss [1.0187634229660034, 1.009338140487671, 0.6900670528411865]\n",
      "4950/4950 [==============================] - 14s 3ms/step - loss: 0.6849 - root_mean_squared_error: 0.8276 - my_r2_score: 0.8701 - val_loss: 1.0188 - val_root_mean_squared_error: 1.0093 - val_my_r2_score: 0.6901\n",
      "Epoch 4/15\n",
      "4949/4950 [============================>.] - ETA: 0s - loss: 0.6188 - root_mean_squared_error: 0.7866 - my_r2_score: 0.8828\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9019 - root_mean_squared_error: 0.9497 - my_r2_score: 0.7246\n",
      "my_val_loss [0.9019243717193604, 0.9496970176696777, 0.7245970964431763]\n",
      "4950/4950 [==============================] - 14s 3ms/step - loss: 0.6188 - root_mean_squared_error: 0.7866 - my_r2_score: 0.8828 - val_loss: 0.9019 - val_root_mean_squared_error: 0.9497 - val_my_r2_score: 0.7246\n",
      "Epoch 5/15\n",
      "4943/4950 [============================>.] - ETA: 0s - loss: 0.5923 - root_mean_squared_error: 0.7696 - my_r2_score: 0.8879\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8860 - root_mean_squared_error: 0.9413 - my_r2_score: 0.7312\n",
      "my_val_loss [0.88604336977005, 0.9412987232208252, 0.7311693429946899]\n",
      "4950/4950 [==============================] - 14s 3ms/step - loss: 0.5922 - root_mean_squared_error: 0.7696 - my_r2_score: 0.8879 - val_loss: 0.8860 - val_root_mean_squared_error: 0.9413 - val_my_r2_score: 0.7312\n",
      "Epoch 6/15\n",
      "4935/4950 [============================>.] - ETA: 0s - loss: 0.5819 - root_mean_squared_error: 0.7628 - my_r2_score: 0.8898\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8333 - root_mean_squared_error: 0.9129 - my_r2_score: 0.7480\n",
      "my_val_loss [0.8333203792572021, 0.9128638505935669, 0.747963547706604]\n",
      "4950/4950 [==============================] - 15s 3ms/step - loss: 0.5818 - root_mean_squared_error: 0.7628 - my_r2_score: 0.8898 - val_loss: 0.8333 - val_root_mean_squared_error: 0.9129 - val_my_r2_score: 0.7480\n",
      "Epoch 7/15\n",
      "4944/4950 [============================>.] - ETA: 0s - loss: 0.5780 - root_mean_squared_error: 0.7603 - my_r2_score: 0.8907\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8103 - root_mean_squared_error: 0.9002 - my_r2_score: 0.7555\n",
      "my_val_loss [0.8103107810020447, 0.9001726508140564, 0.7554996609687805]\n",
      "4950/4950 [==============================] - 14s 3ms/step - loss: 0.5780 - root_mean_squared_error: 0.7603 - my_r2_score: 0.8907 - val_loss: 0.8103 - val_root_mean_squared_error: 0.9002 - val_my_r2_score: 0.7555\n",
      "Epoch 8/15\n",
      "4936/4950 [============================>.] - ETA: 0s - loss: 0.5768 - root_mean_squared_error: 0.7595 - my_r2_score: 0.8910\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8066 - root_mean_squared_error: 0.8981 - my_r2_score: 0.7567\n",
      "my_val_loss [0.8065534234046936, 0.8980832099914551, 0.7566816806793213]\n",
      "4950/4950 [==============================] - 14s 3ms/step - loss: 0.5768 - root_mean_squared_error: 0.7595 - my_r2_score: 0.8910 - val_loss: 0.8066 - val_root_mean_squared_error: 0.8981 - val_my_r2_score: 0.7567\n",
      "Epoch 9/15\n",
      "4943/4950 [============================>.] - ETA: 0s - loss: 0.5765 - root_mean_squared_error: 0.7593 - my_r2_score: 0.8911\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8233 - root_mean_squared_error: 0.9074 - my_r2_score: 0.7512\n",
      "my_val_loss [0.82329261302948, 0.9073547124862671, 0.751183271408081]\n",
      "4950/4950 [==============================] - 14s 3ms/step - loss: 0.5764 - root_mean_squared_error: 0.7592 - my_r2_score: 0.8911 - val_loss: 0.8233 - val_root_mean_squared_error: 0.9074 - val_my_r2_score: 0.7512\n",
      "Epoch 00009: early stopping\n",
      "  1/599 [..............................] - ETA: 0s - loss: 0.5634 - root_mean_squared_error: 0.7506 - my_r2_score: -0.0073WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.7514 - root_mean_squared_error: 0.8668 - my_r2_score: 0.5533\n",
      "time: 6min 32s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_val_loss = []\n",
    "for num_units, activation, batch_size in zip(list_best_units, list_best_activation, list_best_batch_size):\n",
    "    model = build_and_compile_model(X_train, num_units=num_units, activation=activation)\n",
    "    model = train_model(model, X_train, y_train, X_test, y_test, checkpoint_name, logdir,\n",
    "                        batch_size=batch_size, epochs=15, re_train=True)\n",
    "    val_loss, _, _ = model.evaluate(X_test, y_test)\n",
    "    list_val_loss.append(val_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>activation</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>275</td>\n",
       "      <td>128</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.734859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.752606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>96</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.751413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  batch_size activation  val_loss\n",
       "0    275         128    sigmoid  0.734859\n",
       "1    300         128       tanh  0.752606\n",
       "2    300          96    sigmoid  0.751413"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "basic_result = pd.DataFrame({'units':list_best_units,\n",
    "                            'batch_size': list_best_batch_size,\n",
    "                            'activation': list_best_activation,\n",
    "                            'val_loss': list_val_loss})\n",
    "basic_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 265 ms\n"
     ]
    }
   ],
   "source": [
    "# Save improve_result\n",
    "basic_result.to_csv('basic_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">Ta chọn bộ tham số tốt nhất:\n",
    "    - units: 275\n",
    "    - batch_size: 128\n",
    "    - activation: sigmoid\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "   2/3713 [..............................] - ETA: 28:46 - loss: 3229.2866 - root_mean_squared_error: 56.8268 - my_r2_score: -740.8129WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.9274s). Check your callbacks.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.8407 - root_mean_squared_error: 0.9169 - my_r2_score: 0.7808\n",
      "my_val_loss [0.8406933546066284, 0.9168933033943176, 0.7807762622833252]\n",
      "3713/3713 [==============================] - 12s 3ms/step - loss: 9.1848 - root_mean_squared_error: 3.0306 - my_r2_score: -0.8311 - val_loss: 0.8407 - val_root_mean_squared_error: 0.9169 - val_my_r2_score: 0.7808\n",
      "Epoch 2/15\n",
      "3695/3713 [============================>.] - ETA: 0s - loss: 0.7013 - root_mean_squared_error: 0.8374 - my_r2_score: 0.8676\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 1.3432 - root_mean_squared_error: 1.1589 - my_r2_score: 0.6435\n",
      "my_val_loss [1.3431644439697266, 1.1589497327804565, 0.6435362696647644]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.7014 - root_mean_squared_error: 0.8375 - my_r2_score: 0.8676 - val_loss: 1.3432 - val_root_mean_squared_error: 1.1589 - val_my_r2_score: 0.6435\n",
      "Epoch 3/15\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7450 - root_mean_squared_error: 0.8631 - my_r2_score: 0.8076\n",
      "my_val_loss [0.7449622750282288, 0.8631119728088379, 0.8076227903366089]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.6457 - root_mean_squared_error: 0.8036 - my_r2_score: 0.8782 - val_loss: 0.7450 - val_root_mean_squared_error: 0.8631 - val_my_r2_score: 0.8076\n",
      "Epoch 4/15\n",
      "3706/3713 [============================>.] - ETA: 0s - loss: 0.6348 - root_mean_squared_error: 0.7967 - my_r2_score: 0.8803\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.8023 - root_mean_squared_error: 0.8957 - my_r2_score: 0.7932\n",
      "my_val_loss [0.8023124933242798, 0.8957189917564392, 0.7932282090187073]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.6346 - root_mean_squared_error: 0.7966 - my_r2_score: 0.8803 - val_loss: 0.8023 - val_root_mean_squared_error: 0.8957 - val_my_r2_score: 0.7932\n",
      "Epoch 5/15\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7402 - root_mean_squared_error: 0.8603 - my_r2_score: 0.8084\n",
      "my_val_loss [0.7401949763298035, 0.8603458404541016, 0.8084349632263184]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.6091 - root_mean_squared_error: 0.7805 - my_r2_score: 0.8852 - val_loss: 0.7402 - val_root_mean_squared_error: 0.8603 - val_my_r2_score: 0.8084\n",
      "Epoch 6/15\n",
      "3710/3713 [============================>.] - ETA: 0s - loss: 0.6047 - root_mean_squared_error: 0.7776 - my_r2_score: 0.8859\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "  1/150 [..............................] - ETA: 0s - loss: 1.2304 - root_mean_squared_error: 1.1092 - my_r2_score: 0.6946WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7519 - root_mean_squared_error: 0.8671 - my_r2_score: 0.8047A: 0s - loss: 0.7738 - root_mean_squared_error: 0.8796 - my_r2_score: 0.\n",
      "my_val_loss [0.7519310712814331, 0.8671395778656006, 0.804747998714447]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.6047 - root_mean_squared_error: 0.7776 - my_r2_score: 0.8860 - val_loss: 0.7519 - val_root_mean_squared_error: 0.8671 - val_my_r2_score: 0.8047\n",
      "Epoch 7/15\n",
      "3712/3713 [============================>.] - ETA: 0s - loss: 0.5944 - root_mean_squared_error: 0.7709 - my_r2_score: 0.8879\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7781 - root_mean_squared_error: 0.8821 - my_r2_score: 0.7974\n",
      "my_val_loss [0.7780686020851135, 0.8820819854736328, 0.7973582744598389]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.5943 - root_mean_squared_error: 0.7709 - my_r2_score: 0.8880 - val_loss: 0.7781 - val_root_mean_squared_error: 0.8821 - val_my_r2_score: 0.7974\n",
      "Epoch 8/15\n",
      "3706/3713 [============================>.] - ETA: 0s - loss: 0.5906 - root_mean_squared_error: 0.7685 - my_r2_score: 0.8887\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7579 - root_mean_squared_error: 0.8706 - my_r2_score: 0.8028\n",
      "my_val_loss [0.7579134106636047, 0.8705822229385376, 0.8028005957603455]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.5906 - root_mean_squared_error: 0.7685 - my_r2_score: 0.8887 - val_loss: 0.7579 - val_root_mean_squared_error: 0.8706 - val_my_r2_score: 0.8028\n",
      "Epoch 9/15\n",
      "3694/3713 [============================>.] - ETA: 0s - loss: 0.5882 - root_mean_squared_error: 0.7670 - my_r2_score: 0.8891\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7737 - root_mean_squared_error: 0.8796 - my_r2_score: 0.7984\n",
      "my_val_loss [0.7737380862236023, 0.8796238303184509, 0.798396646976471]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.5894 - root_mean_squared_error: 0.7677 - my_r2_score: 0.8889 - val_loss: 0.7737 - val_root_mean_squared_error: 0.8796 - val_my_r2_score: 0.7984\n",
      "Epoch 10/15\n",
      "3712/3713 [============================>.] - ETA: 0s - loss: 0.5891 - root_mean_squared_error: 0.7675 - my_r2_score: 0.8890\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7652 - root_mean_squared_error: 0.8747 - my_r2_score: 0.8008\n",
      "my_val_loss [0.7651556134223938, 0.874731719493866, 0.8007676005363464]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.5890 - root_mean_squared_error: 0.7675 - my_r2_score: 0.8890 - val_loss: 0.7652 - val_root_mean_squared_error: 0.8747 - val_my_r2_score: 0.8008\n",
      "Epoch 11/15\n",
      "3701/3713 [============================>.] - ETA: 0s - loss: 0.5888 - root_mean_squared_error: 0.7673 - my_r2_score: 0.8889\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.560999509019894e-07.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7695 - root_mean_squared_error: 0.8772 - my_r2_score: 0.7996\n",
      "my_val_loss [0.7694546580314636, 0.8771856427192688, 0.7995555996894836]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.5889 - root_mean_squared_error: 0.7674 - my_r2_score: 0.8889 - val_loss: 0.7695 - val_root_mean_squared_error: 0.8772 - val_my_r2_score: 0.7996\n",
      "Epoch 12/15\n",
      "3700/3713 [============================>.] - ETA: 0s - loss: 0.5890 - root_mean_squared_error: 0.7675 - my_r2_score: 0.8891\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.9682997844938655e-07.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7678 - root_mean_squared_error: 0.8762 - my_r2_score: 0.8000\n",
      "my_val_loss [0.7677560448646545, 0.8762168884277344, 0.8000296950340271]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.5889 - root_mean_squared_error: 0.7674 - my_r2_score: 0.8891 - val_loss: 0.7678 - val_root_mean_squared_error: 0.8762 - val_my_r2_score: 0.8000\n",
      "Epoch 00012: early stopping\n",
      "time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "model = build_and_compile_model(X_train, num_units=275, activation='sigmoid')\n",
    "model = train_model(model, X_train, y_train, X_test, y_test, checkpoint_name, logdir,\n",
    "                        batch_size=128, epochs=15, re_train=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sai số rmse:                    0.780\n",
      "Hệ số xác định r2-score: 0.886\n",
      "Tỉ lệ True positive:           0.338\n",
      "time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "# On train\n",
    "train_result_df = evaluate(model, X_train, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sai số rmse:                    0.860\n",
      "Hệ số xác định r2-score: 0.861\n",
      "Tỉ lệ True positive:           0.335\n",
      "time: 469 ms\n"
     ]
    }
   ],
   "source": [
    "# On test\n",
    "test_result_df = evaluate(model, X_test, y_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">Nhận xét:\n",
    "    - Độ chính xác mô hình được cải thiện với tham số tốt nhất\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
