{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thử nghiệm hàm kích hoạt mới: Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import subprocess\n",
    "from glob import glob\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import scipy.stats as ss\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import keras\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.initializers import RandomNormal, Constant\n",
    "from tensorflow.keras.metrics import Metric\n",
    "import tensorflow.keras.optimizers as Optimizer\n",
    "import tensorflow_addons as tfa \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "import os.path as path\n",
    "lib_path =  path.abspath(path.join('' ,\"../../api/common\"))\n",
    "sys.path.insert(1, lib_path)\n",
    "from transform_split_data import transform_split_data\n",
    "from predict import predict, evaluate\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext autotime\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Khởi tạo phương thức giải phóng bộ nhớ gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "time: 1 s\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "- Load dữ liệu theo các option "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>id$Year</th>\n",
       "      <th>ChokyosiCode</th>\n",
       "      <th>BanusiCode</th>\n",
       "      <th>UM_BreederCode</th>\n",
       "      <th>Odds</th>\n",
       "      <th>Kyori</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_0</th>\n",
       "      <th>...</th>\n",
       "      <th>KS_Syotai_川崎</th>\n",
       "      <th>KS_Syotai_笠松</th>\n",
       "      <th>id$JyoCD_1</th>\n",
       "      <th>KS_ChokyosiCode_365.0</th>\n",
       "      <th>CH_Syotai_川崎</th>\n",
       "      <th>top2_ChokyosiCode</th>\n",
       "      <th>top2_BanusiCode</th>\n",
       "      <th>top2_UM_BreederCode</th>\n",
       "      <th>before_Odds</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12135</td>\n",
       "      <td>2009100729</td>\n",
       "      <td>2011</td>\n",
       "      <td>1010</td>\n",
       "      <td>949030</td>\n",
       "      <td>600016</td>\n",
       "      <td>222</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.352506</td>\n",
       "      <td>-0.921203</td>\n",
       "      <td>0.090456</td>\n",
       "      <td>0.243086</td>\n",
       "      <td>59.833795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26439</td>\n",
       "      <td>2011103176</td>\n",
       "      <td>2015</td>\n",
       "      <td>1095</td>\n",
       "      <td>477030</td>\n",
       "      <td>100046</td>\n",
       "      <td>35</td>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.254953</td>\n",
       "      <td>-0.661138</td>\n",
       "      <td>-0.651838</td>\n",
       "      <td>-0.672843</td>\n",
       "      <td>58.230257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33661</td>\n",
       "      <td>2013100779</td>\n",
       "      <td>2017</td>\n",
       "      <td>1123</td>\n",
       "      <td>78006</td>\n",
       "      <td>703397</td>\n",
       "      <td>148</td>\n",
       "      <td>2.348416</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.206765</td>\n",
       "      <td>-0.442809</td>\n",
       "      <td>-1.241063</td>\n",
       "      <td>-0.613398</td>\n",
       "      <td>57.920792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28482</td>\n",
       "      <td>2011105992</td>\n",
       "      <td>2016</td>\n",
       "      <td>1129</td>\n",
       "      <td>547800</td>\n",
       "      <td>610012</td>\n",
       "      <td>52</td>\n",
       "      <td>1.378788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.453573</td>\n",
       "      <td>-0.510523</td>\n",
       "      <td>-0.974169</td>\n",
       "      <td>-0.578217</td>\n",
       "      <td>60.876249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>291</td>\n",
       "      <td>2004104307</td>\n",
       "      <td>2008</td>\n",
       "      <td>331</td>\n",
       "      <td>142006</td>\n",
       "      <td>701079</td>\n",
       "      <td>933</td>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.477666</td>\n",
       "      <td>-1.938882</td>\n",
       "      <td>-0.988132</td>\n",
       "      <td>0.072652</td>\n",
       "      <td>57.627119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475191</th>\n",
       "      <td>18201</td>\n",
       "      <td>2005106482</td>\n",
       "      <td>2013</td>\n",
       "      <td>1013</td>\n",
       "      <td>789006</td>\n",
       "      <td>400018</td>\n",
       "      <td>911</td>\n",
       "      <td>3.027155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232084</td>\n",
       "      <td>-0.467961</td>\n",
       "      <td>0.392597</td>\n",
       "      <td>3.096416</td>\n",
       "      <td>49.966265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475192</th>\n",
       "      <td>7309</td>\n",
       "      <td>2006102916</td>\n",
       "      <td>2010</td>\n",
       "      <td>1076</td>\n",
       "      <td>310007</td>\n",
       "      <td>393126</td>\n",
       "      <td>369</td>\n",
       "      <td>-1.530097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.380029</td>\n",
       "      <td>0.211269</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.553954</td>\n",
       "      <td>60.301508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475193</th>\n",
       "      <td>16882</td>\n",
       "      <td>2007104657</td>\n",
       "      <td>2012</td>\n",
       "      <td>1102</td>\n",
       "      <td>103006</td>\n",
       "      <td>330314</td>\n",
       "      <td>63</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.541780</td>\n",
       "      <td>-1.376219</td>\n",
       "      <td>-0.335163</td>\n",
       "      <td>-0.299192</td>\n",
       "      <td>62.113587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475194</th>\n",
       "      <td>8963</td>\n",
       "      <td>2007104503</td>\n",
       "      <td>2010</td>\n",
       "      <td>221</td>\n",
       "      <td>100006</td>\n",
       "      <td>310390</td>\n",
       "      <td>109</td>\n",
       "      <td>-1.530097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.098465</td>\n",
       "      <td>-0.945524</td>\n",
       "      <td>-1.037796</td>\n",
       "      <td>-0.678908</td>\n",
       "      <td>60.050042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475195</th>\n",
       "      <td>32570</td>\n",
       "      <td>2014103689</td>\n",
       "      <td>2017</td>\n",
       "      <td>436</td>\n",
       "      <td>129008</td>\n",
       "      <td>510045</td>\n",
       "      <td>717</td>\n",
       "      <td>-0.802876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.745284</td>\n",
       "      <td>0.159340</td>\n",
       "      <td>-0.418325</td>\n",
       "      <td>-0.477525</td>\n",
       "      <td>57.849197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>475196 rows × 212 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        race_id    KettoNum  id$Year  ChokyosiCode  BanusiCode  \\\n",
       "0         12135  2009100729     2011          1010      949030   \n",
       "1         26439  2011103176     2015          1095      477030   \n",
       "2         33661  2013100779     2017          1123       78006   \n",
       "3         28482  2011105992     2016          1129      547800   \n",
       "4           291  2004104307     2008           331      142006   \n",
       "...         ...         ...      ...           ...         ...   \n",
       "475191    18201  2005106482     2013          1013      789006   \n",
       "475192     7309  2006102916     2010          1076      310007   \n",
       "475193    16882  2007104657     2012          1102      103006   \n",
       "475194     8963  2007104503     2010           221      100006   \n",
       "475195    32570  2014103689     2017           436      129008   \n",
       "\n",
       "        UM_BreederCode  Odds     Kyori  TenkoBaba$DirtBabaCD_0  \\\n",
       "0               600016   222 -1.045283                     1.0   \n",
       "1               100046    35  0.166752                     0.0   \n",
       "2               703397   148  2.348416                     1.0   \n",
       "3               610012    52  1.378788                     1.0   \n",
       "4               701079   933  0.166752                     0.0   \n",
       "...                ...   ...       ...                     ...   \n",
       "475191          400018   911  3.027155                     0.0   \n",
       "475192          393126   369 -1.530097                     0.0   \n",
       "475193          330314    63 -1.045283                     1.0   \n",
       "475194          310390   109 -1.530097                     0.0   \n",
       "475195          510045   717 -0.802876                     0.0   \n",
       "\n",
       "        TenkoBaba$SibaBabaCD_0  ...  KS_Syotai_川崎　　　　　　　　  \\\n",
       "0                          0.0  ...                   0.0   \n",
       "1                          1.0  ...                   0.0   \n",
       "2                          0.0  ...                   0.0   \n",
       "3                          0.0  ...                   0.0   \n",
       "4                          1.0  ...                   0.0   \n",
       "...                        ...  ...                   ...   \n",
       "475191                     0.0  ...                   0.0   \n",
       "475192                     1.0  ...                   0.0   \n",
       "475193                     0.0  ...                   0.0   \n",
       "475194                     1.0  ...                   0.0   \n",
       "475195                     1.0  ...                   0.0   \n",
       "\n",
       "        KS_Syotai_笠松　　　　　　　　  id$JyoCD_1  KS_ChokyosiCode_365.0  \\\n",
       "0                        0.0         0.0                    0.0   \n",
       "1                        0.0         0.0                    0.0   \n",
       "2                        0.0         0.0                    0.0   \n",
       "3                        0.0         0.0                    0.0   \n",
       "4                        0.0         0.0                    0.0   \n",
       "...                      ...         ...                    ...   \n",
       "475191                   0.0         0.0                    0.0   \n",
       "475192                   0.0         0.0                    0.0   \n",
       "475193                   0.0         0.0                    0.0   \n",
       "475194                   0.0         0.0                    0.0   \n",
       "475195                   0.0         0.0                    0.0   \n",
       "\n",
       "        CH_Syotai_川崎　　　　　　　　  top2_ChokyosiCode  top2_BanusiCode  \\\n",
       "0                        0.0          -1.352506        -0.921203   \n",
       "1                        0.0           0.254953        -0.661138   \n",
       "2                        0.0          -0.206765        -0.442809   \n",
       "3                        0.0           0.453573        -0.510523   \n",
       "4                        0.0          -1.477666        -1.938882   \n",
       "...                      ...                ...              ...   \n",
       "475191                   0.0           0.232084        -0.467961   \n",
       "475192                   0.0          -0.380029         0.211269   \n",
       "475193                   0.0           0.541780        -1.376219   \n",
       "475194                   0.0          -1.098465        -0.945524   \n",
       "475195                   0.0          -0.745284         0.159340   \n",
       "\n",
       "        top2_UM_BreederCode  before_Odds      speed  \n",
       "0                  0.090456     0.243086  59.833795  \n",
       "1                 -0.651838    -0.672843  58.230257  \n",
       "2                 -1.241063    -0.613398  57.920792  \n",
       "3                 -0.974169    -0.578217  60.876249  \n",
       "4                 -0.988132     0.072652  57.627119  \n",
       "...                     ...          ...        ...  \n",
       "475191             0.392597     3.096416  49.966265  \n",
       "475192             1.335489    -0.553954  60.301508  \n",
       "475193            -0.335163    -0.299192  62.113587  \n",
       "475194            -1.037796    -0.678908  60.050042  \n",
       "475195            -0.418325    -0.477525  57.849197  \n",
       "\n",
       "[475196 rows x 212 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "# Load dữ liệu\n",
    "train_data = pd.read_csv('train_data_all.csv')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>id$Year</th>\n",
       "      <th>Kyori</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_1</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_1</th>\n",
       "      <th>id$RaceNum</th>\n",
       "      <th>TrackCD_52</th>\n",
       "      <th>...</th>\n",
       "      <th>KS_Syotai_川崎</th>\n",
       "      <th>KS_Syotai_笠松</th>\n",
       "      <th>id$JyoCD_1</th>\n",
       "      <th>KS_ChokyosiCode_365.0</th>\n",
       "      <th>CH_Syotai_川崎</th>\n",
       "      <th>top2_ChokyosiCode</th>\n",
       "      <th>top2_BanusiCode</th>\n",
       "      <th>top2_UM_BreederCode</th>\n",
       "      <th>before_Odds</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015101022</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.398731</td>\n",
       "      <td>-0.216783</td>\n",
       "      <td>0.174543</td>\n",
       "      <td>-0.638874</td>\n",
       "      <td>58.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015103483</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.415822</td>\n",
       "      <td>-0.318228</td>\n",
       "      <td>-1.717806</td>\n",
       "      <td>4.068149</td>\n",
       "      <td>57.908847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015106010</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.351113</td>\n",
       "      <td>-2.011561</td>\n",
       "      <td>-0.232294</td>\n",
       "      <td>-0.547888</td>\n",
       "      <td>59.178082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015102342</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.812923</td>\n",
       "      <td>-0.653485</td>\n",
       "      <td>-1.837928</td>\n",
       "      <td>0.188494</td>\n",
       "      <td>58.775510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015102323</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.469803</td>\n",
       "      <td>-0.723257</td>\n",
       "      <td>-0.230315</td>\n",
       "      <td>-0.456902</td>\n",
       "      <td>57.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19140</th>\n",
       "      <td>35925</td>\n",
       "      <td>2014105425</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.252109</td>\n",
       "      <td>1.811460</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.671629</td>\n",
       "      <td>58.378378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19141</th>\n",
       "      <td>35925</td>\n",
       "      <td>2014105543</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.897933</td>\n",
       "      <td>2.023608</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.698319</td>\n",
       "      <td>57.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19142</th>\n",
       "      <td>35925</td>\n",
       "      <td>2011106130</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.608380</td>\n",
       "      <td>0.873101</td>\n",
       "      <td>-1.450211</td>\n",
       "      <td>1.291248</td>\n",
       "      <td>57.754011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19143</th>\n",
       "      <td>35925</td>\n",
       "      <td>2012102418</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.897933</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.435675</td>\n",
       "      <td>-0.701958</td>\n",
       "      <td>57.497782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19144</th>\n",
       "      <td>35925</td>\n",
       "      <td>2013104045</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.291467</td>\n",
       "      <td>0.204303</td>\n",
       "      <td>-0.451692</td>\n",
       "      <td>-0.032299</td>\n",
       "      <td>57.243816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19145 rows × 208 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       race_id    KettoNum  id$Year     Kyori  TenkoBaba$DirtBabaCD_0  \\\n",
       "0        34535  2015101022     2018 -1.045283                     0.0   \n",
       "1        34535  2015103483     2018 -1.045283                     0.0   \n",
       "2        34535  2015106010     2018 -1.045283                     0.0   \n",
       "3        34535  2015102342     2018 -1.045283                     0.0   \n",
       "4        34535  2015102323     2018 -1.045283                     0.0   \n",
       "...        ...         ...      ...       ...                     ...   \n",
       "19140    35925  2014105425     2018  0.409159                     0.0   \n",
       "19141    35925  2014105543     2018  0.409159                     0.0   \n",
       "19142    35925  2011106130     2018  0.409159                     0.0   \n",
       "19143    35925  2012102418     2018  0.409159                     0.0   \n",
       "19144    35925  2013104045     2018  0.409159                     0.0   \n",
       "\n",
       "       TenkoBaba$SibaBabaCD_0  TenkoBaba$SibaBabaCD_1  TenkoBaba$DirtBabaCD_1  \\\n",
       "0                         1.0                     0.0                     1.0   \n",
       "1                         1.0                     0.0                     1.0   \n",
       "2                         1.0                     0.0                     1.0   \n",
       "3                         1.0                     0.0                     1.0   \n",
       "4                         1.0                     0.0                     1.0   \n",
       "...                       ...                     ...                     ...   \n",
       "19140                     1.0                     0.0                     1.0   \n",
       "19141                     1.0                     0.0                     1.0   \n",
       "19142                     1.0                     0.0                     1.0   \n",
       "19143                     1.0                     0.0                     1.0   \n",
       "19144                     1.0                     0.0                     1.0   \n",
       "\n",
       "       id$RaceNum  TrackCD_52  ...  KS_Syotai_川崎　　　　　　　　  \\\n",
       "0       -1.550314         0.0  ...                   0.0   \n",
       "1       -1.550314         0.0  ...                   0.0   \n",
       "2       -1.550314         0.0  ...                   0.0   \n",
       "3       -1.550314         0.0  ...                   0.0   \n",
       "4       -1.550314         0.0  ...                   0.0   \n",
       "...           ...         ...  ...                   ...   \n",
       "19140    1.664316         0.0  ...                   0.0   \n",
       "19141    1.664316         0.0  ...                   0.0   \n",
       "19142    1.664316         0.0  ...                   0.0   \n",
       "19143    1.664316         0.0  ...                   0.0   \n",
       "19144    1.664316         0.0  ...                   0.0   \n",
       "\n",
       "       KS_Syotai_笠松　　　　　　　　  id$JyoCD_1  KS_ChokyosiCode_365.0  \\\n",
       "0                       0.0         0.0                    0.0   \n",
       "1                       0.0         0.0                    0.0   \n",
       "2                       0.0         0.0                    0.0   \n",
       "3                       0.0         0.0                    0.0   \n",
       "4                       0.0         0.0                    0.0   \n",
       "...                     ...         ...                    ...   \n",
       "19140                   0.0         0.0                    0.0   \n",
       "19141                   0.0         0.0                    0.0   \n",
       "19142                   0.0         0.0                    0.0   \n",
       "19143                   0.0         0.0                    0.0   \n",
       "19144                   0.0         0.0                    0.0   \n",
       "\n",
       "       CH_Syotai_川崎　　　　　　　　  top2_ChokyosiCode  top2_BanusiCode  \\\n",
       "0                       0.0           0.398731        -0.216783   \n",
       "1                       0.0          -1.415822        -0.318228   \n",
       "2                       0.0          -1.351113        -2.011561   \n",
       "3                       0.0          -0.812923        -0.653485   \n",
       "4                       0.0          -1.469803        -0.723257   \n",
       "...                     ...                ...              ...   \n",
       "19140                   0.0           0.252109         1.811460   \n",
       "19141                   0.0           1.897933         2.023608   \n",
       "19142                   0.0          -0.608380         0.873101   \n",
       "19143                   0.0           1.897933         0.945055   \n",
       "19144                   0.0           0.291467         0.204303   \n",
       "\n",
       "       top2_UM_BreederCode  before_Odds      speed  \n",
       "0                 0.174543    -0.638874  58.064516  \n",
       "1                -1.717806     4.068149  57.908847  \n",
       "2                -0.232294    -0.547888  59.178082  \n",
       "3                -1.837928     0.188494  58.775510  \n",
       "4                -0.230315    -0.456902  57.142857  \n",
       "...                    ...          ...        ...  \n",
       "19140             1.335489    -0.671629  58.378378  \n",
       "19141             1.335489    -0.698319  57.857143  \n",
       "19142            -1.450211     1.291248  57.754011  \n",
       "19143             0.435675    -0.701958  57.497782  \n",
       "19144            -0.451692    -0.032299  57.243816  \n",
       "\n",
       "[19145 rows x 208 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 547 ms\n"
     ]
    }
   ],
   "source": [
    "# Load dữ liệu\n",
    "test_data = pd.read_csv('test_data_all.csv')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id$Year</th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>speed</th>\n",
       "      <th>Time</th>\n",
       "      <th>KakuteiJyuni</th>\n",
       "      <th>top3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011</td>\n",
       "      <td>12135</td>\n",
       "      <td>2009100729</td>\n",
       "      <td>59.833795</td>\n",
       "      <td>72.20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>26439</td>\n",
       "      <td>2011103176</td>\n",
       "      <td>58.230257</td>\n",
       "      <td>105.10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>33661</td>\n",
       "      <td>2013100779</td>\n",
       "      <td>57.920792</td>\n",
       "      <td>161.60</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>28482</td>\n",
       "      <td>2011105992</td>\n",
       "      <td>60.876249</td>\n",
       "      <td>130.10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008</td>\n",
       "      <td>291</td>\n",
       "      <td>2004104307</td>\n",
       "      <td>57.627119</td>\n",
       "      <td>106.20</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475191</th>\n",
       "      <td>2013</td>\n",
       "      <td>18201</td>\n",
       "      <td>2005106482</td>\n",
       "      <td>49.966265</td>\n",
       "      <td>207.50</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475192</th>\n",
       "      <td>2010</td>\n",
       "      <td>7309</td>\n",
       "      <td>2006102916</td>\n",
       "      <td>60.301508</td>\n",
       "      <td>59.70</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475193</th>\n",
       "      <td>2012</td>\n",
       "      <td>16882</td>\n",
       "      <td>2007104657</td>\n",
       "      <td>62.113587</td>\n",
       "      <td>69.55</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475194</th>\n",
       "      <td>2010</td>\n",
       "      <td>8963</td>\n",
       "      <td>2007104503</td>\n",
       "      <td>60.050042</td>\n",
       "      <td>59.95</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475195</th>\n",
       "      <td>2017</td>\n",
       "      <td>32570</td>\n",
       "      <td>2014103689</td>\n",
       "      <td>57.849197</td>\n",
       "      <td>80.90</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>475196 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id$Year  race_id    KettoNum      speed    Time  KakuteiJyuni  top3\n",
       "0          2011    12135  2009100729  59.833795   72.20             1     1\n",
       "1          2015    26439  2011103176  58.230257  105.10             1     1\n",
       "2          2017    33661  2013100779  57.920792  161.60            11     0\n",
       "3          2016    28482  2011105992  60.876249  130.10             2     1\n",
       "4          2008      291  2004104307  57.627119  106.20             5     0\n",
       "...         ...      ...         ...        ...     ...           ...   ...\n",
       "475191     2013    18201  2005106482  49.966265  207.50            13     0\n",
       "475192     2010     7309  2006102916  60.301508   59.70             8     0\n",
       "475193     2012    16882  2007104657  62.113587   69.55             6     0\n",
       "475194     2010     8963  2007104503  60.050042   59.95             5     0\n",
       "475195     2017    32570  2014103689  57.849197   80.90             8     0\n",
       "\n",
       "[475196 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 359 ms\n"
     ]
    }
   ],
   "source": [
    "y_train_df = pd.read_csv('y_train_df_all.csv')\n",
    "y_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id$Year</th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>speed</th>\n",
       "      <th>Time</th>\n",
       "      <th>KakuteiJyuni</th>\n",
       "      <th>top3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015101022</td>\n",
       "      <td>58.064516</td>\n",
       "      <td>74.4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015103483</td>\n",
       "      <td>57.908847</td>\n",
       "      <td>74.6</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015106010</td>\n",
       "      <td>59.178082</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015102342</td>\n",
       "      <td>58.775510</td>\n",
       "      <td>73.5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015102323</td>\n",
       "      <td>57.142857</td>\n",
       "      <td>75.6</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19140</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2014105425</td>\n",
       "      <td>58.378378</td>\n",
       "      <td>111.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19141</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2014105543</td>\n",
       "      <td>57.857143</td>\n",
       "      <td>112.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19142</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2011106130</td>\n",
       "      <td>57.754011</td>\n",
       "      <td>112.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19143</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2012102418</td>\n",
       "      <td>57.497782</td>\n",
       "      <td>112.7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19144</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2013104045</td>\n",
       "      <td>57.243816</td>\n",
       "      <td>113.2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19145 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id$Year  race_id    KettoNum      speed   Time  KakuteiJyuni  top3\n",
       "0         2018    34535  2015101022  58.064516   74.4            10     0\n",
       "1         2018    34535  2015103483  57.908847   74.6            11     0\n",
       "2         2018    34535  2015106010  59.178082   73.0             2     1\n",
       "3         2018    34535  2015102342  58.775510   73.5             6     0\n",
       "4         2018    34535  2015102323  57.142857   75.6            16     0\n",
       "...        ...      ...         ...        ...    ...           ...   ...\n",
       "19140     2018    35925  2014105425  58.378378  111.0             1     1\n",
       "19141     2018    35925  2014105543  57.857143  112.0             5     0\n",
       "19142     2018    35925  2011106130  57.754011  112.2             6     0\n",
       "19143     2018    35925  2012102418  57.497782  112.7             8     0\n",
       "19144     2018    35925  2013104045  57.243816  113.2            12     0\n",
       "\n",
       "[19145 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 47 ms\n"
     ]
    }
   ],
   "source": [
    "y_test_df = pd.read_csv('y_test_df_all.csv')\n",
    "y_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create X, y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kyori</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_1</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_1</th>\n",
       "      <th>id$RaceNum</th>\n",
       "      <th>TrackCD_52</th>\n",
       "      <th>TrackCD_17</th>\n",
       "      <th>JyokenInfo$SyubetuCD_18</th>\n",
       "      <th>GradeCD_</th>\n",
       "      <th>...</th>\n",
       "      <th>KS_Syotai_フランス</th>\n",
       "      <th>KS_Syotai_川崎</th>\n",
       "      <th>KS_Syotai_笠松</th>\n",
       "      <th>id$JyoCD_1</th>\n",
       "      <th>KS_ChokyosiCode_365.0</th>\n",
       "      <th>CH_Syotai_川崎</th>\n",
       "      <th>top2_ChokyosiCode</th>\n",
       "      <th>top2_BanusiCode</th>\n",
       "      <th>top2_UM_BreederCode</th>\n",
       "      <th>before_Odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.352506</td>\n",
       "      <td>-0.921203</td>\n",
       "      <td>0.090456</td>\n",
       "      <td>0.243086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.254953</td>\n",
       "      <td>-0.661138</td>\n",
       "      <td>-0.651838</td>\n",
       "      <td>-0.672843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.348416</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.203121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.206765</td>\n",
       "      <td>-0.442809</td>\n",
       "      <td>-1.241063</td>\n",
       "      <td>-0.613398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.378788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.079838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.453573</td>\n",
       "      <td>-0.510523</td>\n",
       "      <td>-0.974169</td>\n",
       "      <td>-0.578217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.477666</td>\n",
       "      <td>-1.938882</td>\n",
       "      <td>-0.988132</td>\n",
       "      <td>0.072652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475191</th>\n",
       "      <td>3.027155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.673597</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232084</td>\n",
       "      <td>-0.467961</td>\n",
       "      <td>0.392597</td>\n",
       "      <td>3.096416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475192</th>\n",
       "      <td>-1.530097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.965836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.380029</td>\n",
       "      <td>0.211269</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.553954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475193</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.541780</td>\n",
       "      <td>-1.376219</td>\n",
       "      <td>-0.335163</td>\n",
       "      <td>-0.299192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475194</th>\n",
       "      <td>-1.530097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.258075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.098465</td>\n",
       "      <td>-0.945524</td>\n",
       "      <td>-1.037796</td>\n",
       "      <td>-0.678908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475195</th>\n",
       "      <td>-0.802876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.258075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.745284</td>\n",
       "      <td>0.159340</td>\n",
       "      <td>-0.418325</td>\n",
       "      <td>-0.477525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>475196 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Kyori  TenkoBaba$DirtBabaCD_0  TenkoBaba$SibaBabaCD_0  \\\n",
       "0      -1.045283                     1.0                     0.0   \n",
       "1       0.166752                     0.0                     1.0   \n",
       "2       2.348416                     1.0                     0.0   \n",
       "3       1.378788                     1.0                     0.0   \n",
       "4       0.166752                     0.0                     1.0   \n",
       "...          ...                     ...                     ...   \n",
       "475191  3.027155                     0.0                     0.0   \n",
       "475192 -1.530097                     0.0                     1.0   \n",
       "475193 -1.045283                     1.0                     0.0   \n",
       "475194 -1.530097                     0.0                     1.0   \n",
       "475195 -0.802876                     0.0                     1.0   \n",
       "\n",
       "        TenkoBaba$SibaBabaCD_1  TenkoBaba$DirtBabaCD_1  id$RaceNum  \\\n",
       "0                          0.0                     0.0   -1.550314   \n",
       "1                          0.0                     0.0    0.495360   \n",
       "2                          1.0                     0.0    0.203121   \n",
       "3                          1.0                     0.0    1.079838   \n",
       "4                          0.0                     0.0    0.495360   \n",
       "...                        ...                     ...         ...   \n",
       "475191                     0.0                     0.0   -0.673597   \n",
       "475192                     0.0                     0.0   -0.965836   \n",
       "475193                     1.0                     0.0    0.495360   \n",
       "475194                     0.0                     1.0   -1.258075   \n",
       "475195                     0.0                     1.0   -1.258075   \n",
       "\n",
       "        TrackCD_52  TrackCD_17  JyokenInfo$SyubetuCD_18  GradeCD_   ...  \\\n",
       "0              0.0         1.0                      0.0        1.0  ...   \n",
       "1              0.0         0.0                      0.0        1.0  ...   \n",
       "2              0.0         0.0                      0.0        1.0  ...   \n",
       "3              0.0         0.0                      0.0        0.0  ...   \n",
       "4              0.0         0.0                      0.0        1.0  ...   \n",
       "...            ...         ...                      ...        ...  ...   \n",
       "475191         1.0         0.0                      0.0        1.0  ...   \n",
       "475192         0.0         0.0                      0.0        1.0  ...   \n",
       "475193         0.0         1.0                      0.0        1.0  ...   \n",
       "475194         0.0         0.0                      0.0        1.0  ...   \n",
       "475195         0.0         0.0                      0.0        1.0  ...   \n",
       "\n",
       "        KS_Syotai_フランス　　　　　　  KS_Syotai_川崎　　　　　　　　  KS_Syotai_笠松　　　　　　　　  \\\n",
       "0                        0.0                   0.0                   0.0   \n",
       "1                        0.0                   0.0                   0.0   \n",
       "2                        0.0                   0.0                   0.0   \n",
       "3                        0.0                   0.0                   0.0   \n",
       "4                        0.0                   0.0                   0.0   \n",
       "...                      ...                   ...                   ...   \n",
       "475191                   0.0                   0.0                   0.0   \n",
       "475192                   0.0                   0.0                   0.0   \n",
       "475193                   0.0                   0.0                   0.0   \n",
       "475194                   0.0                   0.0                   0.0   \n",
       "475195                   0.0                   0.0                   0.0   \n",
       "\n",
       "        id$JyoCD_1  KS_ChokyosiCode_365.0  CH_Syotai_川崎　　　　　　　　  \\\n",
       "0              0.0                    0.0                   0.0   \n",
       "1              0.0                    0.0                   0.0   \n",
       "2              0.0                    0.0                   0.0   \n",
       "3              0.0                    0.0                   0.0   \n",
       "4              0.0                    0.0                   0.0   \n",
       "...            ...                    ...                   ...   \n",
       "475191         0.0                    0.0                   0.0   \n",
       "475192         0.0                    0.0                   0.0   \n",
       "475193         0.0                    0.0                   0.0   \n",
       "475194         0.0                    0.0                   0.0   \n",
       "475195         0.0                    0.0                   0.0   \n",
       "\n",
       "        top2_ChokyosiCode  top2_BanusiCode  top2_UM_BreederCode  before_Odds  \n",
       "0               -1.352506        -0.921203             0.090456     0.243086  \n",
       "1                0.254953        -0.661138            -0.651838    -0.672843  \n",
       "2               -0.206765        -0.442809            -1.241063    -0.613398  \n",
       "3                0.453573        -0.510523            -0.974169    -0.578217  \n",
       "4               -1.477666        -1.938882            -0.988132     0.072652  \n",
       "...                   ...              ...                  ...          ...  \n",
       "475191           0.232084        -0.467961             0.392597     3.096416  \n",
       "475192          -0.380029         0.211269             1.335489    -0.553954  \n",
       "475193           0.541780        -1.376219            -0.335163    -0.299192  \n",
       "475194          -1.098465        -0.945524            -1.037796    -0.678908  \n",
       "475195          -0.745284         0.159340            -0.418325    -0.477525  \n",
       "\n",
       "[475196 rows x 204 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 297 ms\n"
     ]
    }
   ],
   "source": [
    "ref_col = ['ChokyosiCode', 'BanusiCode', 'UM_BreederCode', 'Odds']\n",
    "drop_columns = ['race_id', 'KettoNum', 'id$Year', 'speed'] + ref_col\n",
    "X_train = train_data.drop(drop_columns, axis=1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    475196.000000\n",
       "mean         58.393319\n",
       "std           2.308153\n",
       "min          21.973550\n",
       "25%          56.942004\n",
       "50%          58.536585\n",
       "75%          59.916782\n",
       "max          66.666667\n",
       "Name: speed, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 47 ms\n"
     ]
    }
   ],
   "source": [
    "y_train = train_data['speed']\n",
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEGCAYAAABPdROvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAd3UlEQVR4nO3df5BdZZ3n8fcHUIlo+E1XJslMxyWOQgJh6M1mhqmpO8Q1EZgJzkIZC02QbEWpsGJVtqYSZ2tHh00NzC5GsyXZjeIQUCekUJYUisoGbzluQWJAJATI0gW9pEmGbCRC2llSdPzuH+dpPLdzu/uG0+fevn0/r6pT99zvOc+5z3mA/vKc55zzKCIwMzN7u05qdQXMzKy9OZGYmVkhTiRmZlaIE4mZmRXiRGJmZoWc0uoKNNs555wT3d3d/PrXv+a0005rdXUmDLdHLbdHLbdHrU5sj8cff/xQRJxbb1vHJZLu7m527dpFtVqlUqm0ujoThtujltujltujVie2h6T/M9I2X9oyM7NCnEjMzKwQJxIzMyvEicTMzApxIjEzs0KcSMzMrBAnEjMzK8SJxMzMCnEiMTOzQkpPJJJOlvRzSQ+m72dJeljS8+nzzNy+ayX1StoraVEufqmk3WnbBklK8XdJujfFd0jqLvt8zKxzdK/53luLjawZPZKbgWdz39cA2yNiNrA9fUfSBcBS4EJgMXCHpJNTmY3ASmB2Whan+ArgcEScD6wHbiv3VMzMbLhSE4mkGcCVwNdz4SXA5rS+Gbg6F98SEUcj4kWgF5gvaRowNSIejWxe4LuHlRk61n3AwqHeipmZNUfZL238MvCXwHtzsa6IOAAQEQcknZfi04HHcvv1p9ibaX14fKjMvnSsQUmvAWcDh/KVkLSSrEdDV1cX1WqVgYEBqtVq0fObNNwetdwetTq1PVbPHXxrPX/+ndoeIyktkUi6CjgYEY9LqjRSpE4sRomPVqY2ELEJ2ATQ09MTlUqlI9/eORq3Ry23R61ObY/rc2MjfddV3lrv1PYYSZk9ksuAP5d0BXAqMFXSN4FXJE1LvZFpwMG0fz8wM1d+BrA/xWfUiefL9Es6BTgdeLWsEzIzs+OVNkYSEWsjYkZEdJMNoj8SEZ8AtgHL027LgQfS+jZgaboTaxbZoPrOdBnsiKQFafxj2bAyQ8e6Jv3GcT0SMzMrTysmtroV2CppBfAScC1AROyRtBV4BhgEVkXEsVTmRuAuYArwUFoA7gTukdRL1hNZ2qyTMDOzTFMSSURUgWpa/yWwcIT91gHr6sR3AXPqxN8gJSIzM2sNP9luZmaFOJGYmVkhrRgjMTObsPw6lBPnHomZmRXiHomZdTz3Qopxj8TMzApxIjEzs0KcSMzMrBAnEjMzK8SJxMzMCnEiMTOzQpxIzMysECcSMzMrxInEzMwKcSIxM7NC/IoUM+tIfi3K+CmtRyLpVEk7Jf1C0h5JX0zxL0h6WdKTabkiV2atpF5JeyUtysUvlbQ7bduQptwlTct7b4rvkNRd1vmYmVl9ZV7aOgpcHhEXA/OAxZIWpG3rI2JeWr4PIOkCsqlyLwQWA3dIOjntvxFYSTaP++y0HWAFcDgizgfWA7eVeD5mZlZHaYkkMgPp6zvSEqMUWQJsiYijEfEi0AvMlzQNmBoRj0ZEAHcDV+fKbE7r9wELh3orZmbWHKWOkaQexePA+cBXI2KHpI8AN0laBuwCVkfEYWA68FiueH+KvZnWh8dJn/sAImJQ0mvA2cChYfVYSdajoauri2q1ysDAANVqdTxPt625PWq5PWpNxvZYPXfwhPbPn/9kbI8iSk0kEXEMmCfpDOB+SXPILlPdQtY7uQW4HbgBqNeTiFHijLEtX49NwCaAnp6eqFQqVKtVKpXKCZ3PZOb2qOX2qDUZ2+P6Exxs77uu8tb6ZGyPIppy+29E/AqoAosj4pWIOBYRvwG+BsxPu/UDM3PFZgD7U3xGnXhNGUmnAKcDr5ZzFmZmVk+Zd22dm3oiSJoCfAh4Lo15DPko8HRa3wYsTXdizSIbVN8ZEQeAI5IWpPGPZcADuTLL0/o1wCNpHMXMbFx1r/neW4vVKvPS1jRgcxonOQnYGhEPSrpH0jyyS1B9wKcBImKPpK3AM8AgsCpdGgO4EbgLmAI8lBaAO4F7JPWS9USWlng+ZmZWR2mJJCKeAi6pE//kKGXWAevqxHcBc+rE3wCuLVZTMzMrwq9IMTOzQpxIzMysECcSMzMrxInEzMwKcSIxM7NCnEjMzKwQJxIzMyvEicTMzApxIjEzs0KcSMzMrBAnEjMzK8SJxMzMCil1Yiszs4nEr4Avh3skZmZWiBOJmZkV4kRiZmaFOJGYmVkhZc7ZfqqknZJ+IWmPpC+m+FmSHpb0fPo8M1dmraReSXslLcrFL5W0O23bkOZuJ83vfm+K75DUXdb5mJlZfWX2SI4Cl0fExcA8YLGkBcAaYHtEzAa2p+9IuoBszvULgcXAHWm+d4CNwEpgdloWp/gK4HBEnA+sB24r8XzMzKyO0hJJZAbS13ekJYAlwOYU3wxcndaXAFsi4mhEvAj0AvMlTQOmRsSjERHA3cPKDB3rPmDhUG/FzMyao9TnSFKP4nHgfOCrEbFDUldEHACIiAOSzku7TwceyxXvT7E30/rw+FCZfelYg5JeA84GDg2rx0qyHg1dXV1Uq1UGBgaoVqvjdq7tzu1Ry+1Ra7K0x+q5g+NynMnSHuOl1EQSEceAeZLOAO6XNGeU3ev1JGKU+GhlhtdjE7AJoKenJyqVCtVqlUqlMkp1Oovbo5bbo9ZkaY/rx+mBxLsWnzYp2mO8NOWurYj4FVAlG9t4JV2uIn0eTLv1AzNzxWYA+1N8Rp14TRlJpwCnA6+WcQ5mZlZfmXdtnZt6IkiaAnwIeA7YBixPuy0HHkjr24Cl6U6sWWSD6jvTZbAjkhak8Y9lw8oMHesa4JE0jmJmZk1S5qWtacDmNE5yErA1Ih6U9CiwVdIK4CXgWoCI2CNpK/AMMAisSpfGAG4E7gKmAA+lBeBO4B5JvWQ9kaUlno+ZtSG/X6t8pSWSiHgKuKRO/JfAwhHKrAPW1YnvAo4bX4mIN0iJyMysWXa//Npb4y19t17Z4tq0np9sNzOzQpxIzMysECcSMzMrxInEzMwKcSIxM7NCnEjMzKwQJxIzMyvEicTMzApxIjEzs0KcSMzMrBAnEjMzK8SJxMzMCnEiMTOzQpxIzMyskFKn2jUzawXPQdJc7pGYmVkhYyYSSbskrZJ05okcWNJMST+W9KykPZJuTvEvSHpZ0pNpuSJXZq2kXkl7JS3KxS+VtDtt25Cm3CVNy3tviu+Q1H0idTQzs+Ia6ZEsBX4H+JmkLZIWDf0hH8MgsDoiPggsAFZJuiBtWx8R89LyfYC0bSlwIbAYuCNN0wuwEVhJNo/77LQdYAVwOCLOB9YDtzVQLzMzG0djJpKI6I2IvwLeD3wb+AbwkqQvSjprlHIHIuKJtH4EeBaYPspPLQG2RMTRiHgR6AXmS5oGTI2IRyMigLuBq3NlNqf1+4CFDSY5MzMbJw0Ntku6CPgUcAXwHeBbwB8DjwDzGijfTTZ/+w7gMuAmScuAXWS9lsNkSeaxXLH+FHszrQ+Pkz73AUTEoKTXgLOBQ8N+fyVZj4auri6q1SoDAwNUq9VGTr8juD1quT1qtVt7rJ47WOrxu6b89jfaqV3KMmYikfQ48CvgTmBNRBxNm3ZIuqyB8u8hSz6fi4jXJW0EbgEifd4O3ADU60nEKHHG2PbbQMQmYBNAT09PVCoVqtUqlUplrOp3DLdHLbdHrXZrj+tLvmtr9dxBbt+d/fnsu65S6m+1g0Z6JNdGxAv1NkTEX4xWUNI7SD2YiPhuKvNKbvvXgAfT135gZq74DGB/is+oE8+X6Zd0CnA68GoD52RmZuOkkcH2fyvpjKEvks6U9J/GKpTGKu4Eno2IL+Xi03K7fRR4Oq1vA5amO7FmkQ2q74yIA8ARSQvSMZcBD+TKLE/r1wCPpHEUMzNrkkZ6JB+JiM8PfYmIw+mW3f8wRrnLgE8CuyU9mWKfBz4uaR7ZJag+4NPpuHskbQWeIbvja1VEHEvlbgTuAqYAD6UFskR1j6Resp7I0gbOx8zMxlEjieRkSe8aGhuRNAV411iFIuKn1B/D+P4oZdYB6+rEdwFz6sTfAK4dqy5mZlaeRhLJN4Htkv6erBdxA7+95dbMrKPlX8fSd+uVLaxJ64yZSCLi7yTtBhaS9TBuiYgfll4zMzNrCw09RxIR+XEJMzOztzTyrq2/kPS8pNckvS7piKTXm1E5MzOb+Brpkfwd8GcR8WzZlTEzs/bTyHMkrziJmJnZSBrpkeySdC/wP4Ch16Mw9KS6mZl1tkYSyVTgn4EP52IBOJGYmVlDt/9+qhkVMTOz9tTIXVvvl7Rd0tPp+0WSxno9ipmZdYhGBtu/BqwlmxeEiHgKv9PKzMySRhLJuyNi57BYubPGmJlZ22gkkRyS9C9IE0ZJugY4UGqtzMysbTRy19YqstkFPyDpZeBF4BOl1srMzNpGI3dtvQB8SNJpwEkRcaT8apmZnZjukqfXtZE1Mmf7fxz2HYCI+JuS6mRmZm2kkUtbv86tnwpcBfiVKWZmBjQw2B4Rt+eWdUAFmD5WOUkzJf1Y0rOS9ki6OcXPkvRweqPww5LOzJVZK6lX0l5Ji3LxSyXtTts2pLnbSfO735viOyR1n3gTmJlZEY3ctTXcu4H3NbDfILA6Ij4ILABWSboAWANsj4jZwPb0nbRtKXAhsBi4Q9LJ6VgbgZXA7LQsTvEVwOGIOB9YD9z2Ns7HzMwKaOTJ9t2SnkrLHmAv8JWxykXEgYh4Iq0fIbscNh1Ywm+n6t0MXJ3WlwBbIuJoRLwI9ALzJU0DpkbEoxERwN3Dygwd6z5g4VBvxczMmqORMZKrcuuDZK+VP6EHEtMlp0uAHUBXRByALNlIOi/tNh14LFesP8XeTOvD40Nl9qVjDUp6DTgbODTs91eS9Wjo6uqiWq0yMDBAtVo9kdOY1NwetdwetdqhPVbPbd5z0l1T6v/eRG+jsjSSSIbf7js1/z/9EfHqaIUlvQf4DvC5iHh9lA5DvQ0xSny0MrWBiE1kz8LQ09MTlUqFarVKpVIZreodxe1Ry+1Rqx3a4/om3v67eu4gt+8+/s9n33WVptVhImkkkTwBzAQOk/3hPgN4KW0LRhkvkfQOsiTyrdz8Ja9ImpZ6I9OAgynen35nyAxgf4rPqBPPl+mXdApwOjBqYjMzs/HVyGD7D8im2j0nIs4mu9T13YiYFRGjJREBdwLPRsSXcpu2AcvT+nLggVx8aboTaxbZoPrOdBnsiKQF6ZjLhpUZOtY1wCNpHMXMzJqkkR7Jv4yIzwx9iYiHJN3SQLnLgE8CuyU9mWKfB24FtkpaQdazuTYdd4+krcAzZGMxqyLiWCp3I3AXMAV4KC2QJap7JPWS9UT8VmIzsyZrJJEcSvOPfJPsUtYngF+OVSgifkr9MQyAhSOUWQesqxPfBcypE3+DlIjMzKw1Grm09XHgXOD+tJybYmZmZg29tPFV4GZJ74mIgSbUyczM2kgjDyT+kaRnyMYukHSxpDtKr5mZmbWFRi5trQcWkcZFIuIXwJ+UWSkzM2sfDb1rKyL2DQsdq7ujmZl1nEbu2ton6Y+AkPRO4LP4NfJmZsfJT67Vd+uVLaxJczXSI/kM2XS708meJJ+XvpuZmY3eI0mvcf9yRFzXpPqYmVmbGbVHkp4sPzdd0jIzMztOI2MkfcD/krSN3LS7w96fZWZmHWrEHomke9Lqx4AH077vzS1mZmaj9kgulfR7ZC9W/K9Nqo+ZmbWZ0RLJfyN7hfwsYFcuLsaYh8TMzDrHiIkkIjYAGyRtjIgbm1gnM7OGdDdxVkQb2ZjPkTiJmJnZaBp6RYqZmdlISkskkr4h6aCkp3OxL0h6WdKTabkit22tpF5JeyUtysUvlbQ7bduQptslTcl7b4rvkNRd1rmYmdnIyuyR3AUsrhNfHxHz0vJ9AEkXkE2Te2Eqc0d6qh5gI7CSbA732bljrgAOR8T5ZG8ovq2sEzEzs5GVlkgi4idk86g3YgmwJSKORsSLQC8wX9I0YGpEPBoRAdwNXJ0rszmt3wcsHOqtmJlZ8zTyZPt4u0nSMrJbildHxGGyF0I+ltunP8XeTOvD46TPfQARMSjpNeBs4NDwH5S0kqxXQ1dXF9VqlYGBAarV6nieV1tze9Rye9SaqO2xeu5gS363a8rYvz0R26sszU4kG4FbyJ5DuQW4HbiB7NmU4WKUOGNsqw1GbAI2AfT09ESlUqFarVKpVE6o8pOZ26OW26PWRG2P61t0++/quYPcvnv0P59911WaU5kJoKl3bUXEKxFxLCJ+A3wNmJ829QMzc7vOAPan+Iw68Zoykk4BTqfxS2lmZjZOmppI0pjHkI8CQ3d0bQOWpjuxZpENqu+MiAPAEUkL0vjHMuCBXJnlaf0a4JE0jmJm1nLda7731jLZlXZpS9I/ABXgHEn9wF8DFUnzyC5B9QGfBoiIPZK2As8Ag8Cq9Ap7gBvJ7gCbAjyUFoA7gXsk9ZL1RJaWdS5mZjay0hJJRHy8TvjOUfZfB6yrE98FzKkTfwO4tkgdzcysOD/ZbmZmhTiRmJlZIU4kZmZWiBOJmZkV4kRiZmaFtOIVKWZmb1snPJfRbtwjMTOzQpxIzMysECcSMzMrxInEzMwKcSIxM7NCnEjMzKwQJxIzMyvEicTMzApxIjEzs0L8ZLuZWcnyT+P33XplC2tSDvdIzMyskNISiaRvSDoo6elc7CxJD0t6Pn2emdu2VlKvpL2SFuXil0ranbZtSHO3k+Z3vzfFd0jqLutczMxsZGX2SO4CFg+LrQG2R8RsYHv6jqQLyOZcvzCVuUPSyanMRmAlMDstQ8dcARyOiPOB9cBtpZ2JmZmNqLREEhE/AV4dFl4CbE7rm4Grc/EtEXE0Il4EeoH5kqYBUyPi0YgI4O5hZYaOdR+wcKi3YmZmzdPswfauiDgAEBEHJJ2X4tOBx3L79afYm2l9eHyozL50rEFJrwFnA4eG/6iklWS9Grq6uqhWqwwMDFCtVsfrvNqe26OW26PWRGqP1XMHW10Fuqa8/XpMlHYcTxPlrq16PYkYJT5ameODEZuATQA9PT1RqVSoVqtUKpW3UdXJye1Ry+1RayK1x/UTYD6S1XMHuX332/vz2XddZXwrMwE0+66tV9LlKtLnwRTvB2bm9psB7E/xGXXiNWUknQKczvGX0szMrGTNTiTbgOVpfTnwQC6+NN2JNYtsUH1nugx2RNKCNP6xbFiZoWNdAzySxlHMzKyJSru0JekfgApwjqR+4K+BW4GtklYALwHXAkTEHklbgWeAQWBVRBxLh7qR7A6wKcBDaQG4E7hHUi9ZT2RpWediZmYjKy2RRMTHR9i0cIT91wHr6sR3AXPqxN8gJSIzm7w8R/vE5yfbzcysECcSMzMrxInEzMwKcSIxM7NCJsoDiWZmHWEyvlLePRIzMyvEicTMzApxIjEzs0KcSMzMrBAnEjMzK8R3bZnZhOPXorQX90jMzKwQJxIzMyvEicTMzApxIjEzs0KcSMzMrJCWJBJJfZJ2S3pS0q4UO0vSw5KeT59n5vZfK6lX0l5Ji3LxS9NxeiVtSNPxmplZE7WyR/KnETEvInrS9zXA9oiYDWxP35F0Adk0uhcCi4E7JJ2cymwEVpLN8T47bTczawvda7731tLOJtKlrSXA5rS+Gbg6F98SEUcj4kWgF5gvaRowNSIejYgA7s6VMTOzJmnVA4kB/EhSAP89IjYBXRFxACAiDkg6L+07HXgsV7Y/xd5M68Pjx5G0kqznQldXF9VqlYGBAarV6jieUntze9Rye9RqdnusnjvYtN96O7qmjH8d2/nft1YlkssiYn9KFg9Lem6UfeuNe8Qo8eODWaLaBNDT0xOVSoVqtUqlUjnBak9ebo9abo9azWiP2ss7E/ulG6vnDnL77vGtY991lXE9XjO15NJWROxPnweB+4H5wCvpchXp82DavR+YmSs+A9if4jPqxM3MrImankgknSbpvUPrwIeBp4FtwPK023LggbS+DVgq6V2SZpENqu9Ml8GOSFqQ7tZalitjZmZN0or+Yxdwf7pT9xTg2xHxA0k/A7ZKWgG8BFwLEBF7JG0FngEGgVURcSwd60bgLmAK8FBazMysiZqeSCLiBeDiOvFfAgtHKLMOWFcnvguYM951NDOzxk3sES0zm9Ta/fkJy0yk50jMzKwNOZGYmVkhvrRlZk3ly1mTj3skZmZWiHskZmYTQL6n1nfrlS2syYlzj8TMzApxj8TMSudxkcnNPRIzMyvEicTMzArxpS0zK4UvZ3UO90jMzKwQJxIzMyvEl7bMzCaYdnumxInEzMaNx0U6ky9tmZlZIe6RmFkh7oWUqx0uc7V9IpG0GPgKcDLw9Yi4tcVVMpv0nDxaY6ImlbZOJJJOBr4K/GugH/iZpG0R8Uxra2Y2+Th5TCzD/3m0MrG0dSIB5gO9aR54JG0BlgBOJGZjOJHEsHruIO3/52JyG+mfZzMSTLv/mzEd2Jf73g/8q+E7SVoJrExfByTtBc4BDpVew/bh9qjl9sj5rNujRju1h24bt0P93kgb2j2RqE4sjgtEbAI21RSUdkVET1kVazduj1puj1puj1puj1rtfvtvPzAz930GsL9FdTEz60jtnkh+BsyWNEvSO4GlwLYW18nMrKO09aWtiBiUdBPwQ7Lbf78REXsaLL5p7F06itujltujltujltsjRxHHDSmYmZk1rN0vbZmZWYs5kZiZWSGTPpFIminpx5KelbRH0s0pfpakhyU9nz7PbHVdm0HSqZJ2SvpFao8vpnhHtscQSSdL+rmkB9P3Tm+PPkm7JT0paVeKdWybSDpD0n2Snkt/S/6wk9tjuEmfSIBBYHVEfBBYAKySdAGwBtgeEbOB7el7JzgKXB4RFwPzgMWSFtC57THkZuDZ3PdObw+AP42IebnnJTq5Tb4C/CAiPgBcTPbvSie3R62I6KgFeIDs3Vx7gWkpNg3Y2+q6taAt3g08QfY2gI5tD7Lnj7YDlwMPpljHtkc65z7gnGGxjmwTYCrwIunmpE5vj3pLJ/RI3iKpG7gE2AF0RcQBgPR5Xgur1lTpMs6TwEHg4Yjo6PYAvgz8JfCbXKyT2wOyN0T8SNLj6RVD0Llt8j7g/wJ/ny5/fl3SaXRuexynYxKJpPcA3wE+FxGvt7o+rRQRxyJiHtn/ic+XNKfFVWoZSVcBByPi8VbXZYK5LCL+APgI2eXgP2l1hVroFOAPgI0RcQnwazr5MlYdHZFIJL2DLIl8KyK+m8KvSJqWtk8j+7/zjhIRvwKqwGI6tz0uA/5cUh+wBbhc0jfp3PYAICL2p8+DwP1kb9ru1DbpB/pTzx3gPrLE0qntcZxJn0gkCbgTeDYivpTbtA1YntaXk42dTHqSzpV0RlqfAnwIeI4ObY+IWBsRMyKim+wVO49ExCfo0PYAkHSapPcOrQMfBp6mQ9skIv4J2Cfp91NoIdlUFR3ZHvVM+ifbJf0x8I/Abn57DfzzZOMkW4HfBV4Cro2IV1tSySaSdBGwmeyVMicBWyPibySdTQe2R56kCvDvI+KqTm4PSe8j64VAdlnn2xGxrsPbZB7wdeCdwAvAp0j//dCB7THcpE8kZmZWrkl/acvMzMrlRGJmZoU4kZiZWSFOJGZmVogTiZmZFeJEYtbGJFUl9Yy9p1l5nEjMzKwQJxKzcZaeDP9emvPlaUkfS/N73Jbmgtkp6fy077mSviPpZ2m5LHeMb6TYzyUtSfEpkrZIekrSvcCUFp6qGZA9tWpm42sxsD8irgSQdDpwG/B6RMyXtIzsjcNXkc1zsT4ifirpd4EfAh8E/orsdS03pFfa7JT0P4FPA/8cEReltxQ80eRzMzuOn2w3G2eS3k+WELaSzW/yj+mlkJdHxAvpJaL/FBFnSzoI7M8VPxf4APBj4FSyidkAzgIWAX8LbIiIR9JvPQGsjIhdTTg1s7rcIzEbZxHxvyVdClwB/K2kHw1tyu+WPk8C/jAi/l/+GOllo/8mIvYOiw8/jlnLeYzEbJxJ+h2yy0/fBP4L2SvHAT6W+3w0rf8IuClXdl5a/SHw71JCQdIlKf4T4LoUmwNcVM5ZmDXOPRKz8TcX+M+SfgO8CdxINofFuyTtIPsfuI+nfT8LfFXSU2T/Pf4E+AxwC9k4ylMpmfSRjalsJJup7yngSWBnc07JbGQeIzFrgjRG0hMRh1pdF7Px5ktbZmZWiHskZmZWiHskZmZWiBOJmZkV4kRiZmaFOJGYmVkhTiRmZlbI/wfzh5lZbgUE/AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 469 ms\n"
     ]
    }
   ],
   "source": [
    "y_train.hist(bins=100);\n",
    "plt.xlabel('speed');\n",
    "plt.ylabel('frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kyori</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_1</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_1</th>\n",
       "      <th>id$RaceNum</th>\n",
       "      <th>TrackCD_52</th>\n",
       "      <th>TrackCD_17</th>\n",
       "      <th>JyokenInfo$SyubetuCD_18</th>\n",
       "      <th>GradeCD_</th>\n",
       "      <th>...</th>\n",
       "      <th>KS_Syotai_フランス</th>\n",
       "      <th>KS_Syotai_川崎</th>\n",
       "      <th>KS_Syotai_笠松</th>\n",
       "      <th>id$JyoCD_1</th>\n",
       "      <th>KS_ChokyosiCode_365.0</th>\n",
       "      <th>CH_Syotai_川崎</th>\n",
       "      <th>top2_ChokyosiCode</th>\n",
       "      <th>top2_BanusiCode</th>\n",
       "      <th>top2_UM_BreederCode</th>\n",
       "      <th>before_Odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.398731</td>\n",
       "      <td>-0.216783</td>\n",
       "      <td>0.174543</td>\n",
       "      <td>-0.638874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.415822</td>\n",
       "      <td>-0.318228</td>\n",
       "      <td>-1.717806</td>\n",
       "      <td>4.068149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.351113</td>\n",
       "      <td>-2.011561</td>\n",
       "      <td>-0.232294</td>\n",
       "      <td>-0.547888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.812923</td>\n",
       "      <td>-0.653485</td>\n",
       "      <td>-1.837928</td>\n",
       "      <td>0.188494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.469803</td>\n",
       "      <td>-0.723257</td>\n",
       "      <td>-0.230315</td>\n",
       "      <td>-0.456902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19140</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.252109</td>\n",
       "      <td>1.811460</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.671629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19141</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.897933</td>\n",
       "      <td>2.023608</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.698319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19142</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.608380</td>\n",
       "      <td>0.873101</td>\n",
       "      <td>-1.450211</td>\n",
       "      <td>1.291248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19143</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.897933</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.435675</td>\n",
       "      <td>-0.701958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19144</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.291467</td>\n",
       "      <td>0.204303</td>\n",
       "      <td>-0.451692</td>\n",
       "      <td>-0.032299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19145 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Kyori  TenkoBaba$DirtBabaCD_0  TenkoBaba$SibaBabaCD_0  \\\n",
       "0     -1.045283                     0.0                     1.0   \n",
       "1     -1.045283                     0.0                     1.0   \n",
       "2     -1.045283                     0.0                     1.0   \n",
       "3     -1.045283                     0.0                     1.0   \n",
       "4     -1.045283                     0.0                     1.0   \n",
       "...         ...                     ...                     ...   \n",
       "19140  0.409159                     0.0                     1.0   \n",
       "19141  0.409159                     0.0                     1.0   \n",
       "19142  0.409159                     0.0                     1.0   \n",
       "19143  0.409159                     0.0                     1.0   \n",
       "19144  0.409159                     0.0                     1.0   \n",
       "\n",
       "       TenkoBaba$SibaBabaCD_1  TenkoBaba$DirtBabaCD_1  id$RaceNum  TrackCD_52  \\\n",
       "0                         0.0                     1.0   -1.550314         0.0   \n",
       "1                         0.0                     1.0   -1.550314         0.0   \n",
       "2                         0.0                     1.0   -1.550314         0.0   \n",
       "3                         0.0                     1.0   -1.550314         0.0   \n",
       "4                         0.0                     1.0   -1.550314         0.0   \n",
       "...                       ...                     ...         ...         ...   \n",
       "19140                     0.0                     1.0    1.664316         0.0   \n",
       "19141                     0.0                     1.0    1.664316         0.0   \n",
       "19142                     0.0                     1.0    1.664316         0.0   \n",
       "19143                     0.0                     1.0    1.664316         0.0   \n",
       "19144                     0.0                     1.0    1.664316         0.0   \n",
       "\n",
       "       TrackCD_17  JyokenInfo$SyubetuCD_18  GradeCD_   ...  \\\n",
       "0             0.0                      0.0        1.0  ...   \n",
       "1             0.0                      0.0        1.0  ...   \n",
       "2             0.0                      0.0        1.0  ...   \n",
       "3             0.0                      0.0        1.0  ...   \n",
       "4             0.0                      0.0        1.0  ...   \n",
       "...           ...                      ...        ...  ...   \n",
       "19140         0.0                      0.0        1.0  ...   \n",
       "19141         0.0                      0.0        1.0  ...   \n",
       "19142         0.0                      0.0        1.0  ...   \n",
       "19143         0.0                      0.0        1.0  ...   \n",
       "19144         0.0                      0.0        1.0  ...   \n",
       "\n",
       "       KS_Syotai_フランス　　　　　　  KS_Syotai_川崎　　　　　　　　  KS_Syotai_笠松　　　　　　　　  \\\n",
       "0                       0.0                   0.0                   0.0   \n",
       "1                       0.0                   0.0                   0.0   \n",
       "2                       0.0                   0.0                   0.0   \n",
       "3                       0.0                   0.0                   0.0   \n",
       "4                       0.0                   0.0                   0.0   \n",
       "...                     ...                   ...                   ...   \n",
       "19140                   0.0                   0.0                   0.0   \n",
       "19141                   0.0                   0.0                   0.0   \n",
       "19142                   0.0                   0.0                   0.0   \n",
       "19143                   0.0                   0.0                   0.0   \n",
       "19144                   0.0                   0.0                   0.0   \n",
       "\n",
       "       id$JyoCD_1  KS_ChokyosiCode_365.0  CH_Syotai_川崎　　　　　　　　  \\\n",
       "0             0.0                    0.0                   0.0   \n",
       "1             0.0                    0.0                   0.0   \n",
       "2             0.0                    0.0                   0.0   \n",
       "3             0.0                    0.0                   0.0   \n",
       "4             0.0                    0.0                   0.0   \n",
       "...           ...                    ...                   ...   \n",
       "19140         0.0                    0.0                   0.0   \n",
       "19141         0.0                    0.0                   0.0   \n",
       "19142         0.0                    0.0                   0.0   \n",
       "19143         0.0                    0.0                   0.0   \n",
       "19144         0.0                    0.0                   0.0   \n",
       "\n",
       "       top2_ChokyosiCode  top2_BanusiCode  top2_UM_BreederCode  before_Odds  \n",
       "0               0.398731        -0.216783             0.174543    -0.638874  \n",
       "1              -1.415822        -0.318228            -1.717806     4.068149  \n",
       "2              -1.351113        -2.011561            -0.232294    -0.547888  \n",
       "3              -0.812923        -0.653485            -1.837928     0.188494  \n",
       "4              -1.469803        -0.723257            -0.230315    -0.456902  \n",
       "...                  ...              ...                  ...          ...  \n",
       "19140           0.252109         1.811460             1.335489    -0.671629  \n",
       "19141           1.897933         2.023608             1.335489    -0.698319  \n",
       "19142          -0.608380         0.873101            -1.450211     1.291248  \n",
       "19143           1.897933         0.945055             0.435675    -0.701958  \n",
       "19144           0.291467         0.204303            -0.451692    -0.032299  \n",
       "\n",
       "[19145 rows x 204 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 32 ms\n"
     ]
    }
   ],
   "source": [
    "drop_columns = ['race_id', 'KettoNum', 'id$Year', 'speed']\n",
    "X_test = test_data.drop(drop_columns, axis=1)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    19145.000000\n",
       "mean        58.013408\n",
       "std          2.305421\n",
       "min         38.876890\n",
       "25%         56.509695\n",
       "50%         58.142665\n",
       "75%         59.558824\n",
       "max         65.573770\n",
       "Name: speed, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "y_test = test_data['speed']\n",
    "y_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 328 ms\n"
     ]
    }
   ],
   "source": [
    "y_test.hist(bins=100);\n",
    "plt.xlabel('speed');\n",
    "plt.ylabel('frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train with ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create and compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7_eizWaqi_7",
    "outputId": "4361457c-e661-4e75-8c78-3c1a7789b705"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def gaussian(x):\n",
    "    return K.exp(-K.pow(x, 2))\n",
    "\n",
    "def my_r2_score(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true) ) ) \n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "def BatchNorm():\n",
    "    return BatchNormalization(\n",
    "                momentum=0.95, \n",
    "                epsilon=0.005,\n",
    "                beta_initializer=RandomNormal(mean=0.0, stddev=0.05), \n",
    "                gamma_initializer=Constant(value=0.9)\n",
    "                )\n",
    "\n",
    "#model.add(BatchNorm())\n",
    "#model.add(Dropout(0.07))\n",
    "#model.add(Dense(units=num_2, activation='relu'))\n",
    "#model.add(BatchNorm())     \n",
    "\n",
    "def build_and_compile_model(X_train, num_units=100, activation='sigmoid'):\n",
    "    input_shape = X_train.shape[1] \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=num_units, activation=activation, kernel_initializer='he_normal',\n",
    "                    input_shape=(input_shape,)))\n",
    "    model.add(Dense(1))\n",
    "      \n",
    "    model.compile(loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError(), my_r2_score], optimizer=Optimizer.Adam(0.01)) #my_r2_score\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setup callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6x6dteutin0",
    "outputId": "0b4eab09-31e5-4ef0-d9cd-105656a99abe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "class My_checkoint(Callback):\n",
    "        \n",
    "    def __init__(self, model, X_test, y_test, checkpoint_name):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.mode = model\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        cpn = checkpoint_name + format(epoch, '02d') + '-.hdf5'\n",
    "        #cpn = os.path.join(checkpoint_dir, 'model'+format(epoch, '02d') + '-.hdf5')\n",
    "        val_loss = self.mode.evaluate(self.X_test, self.y_test)\n",
    "        print('my_val_loss', val_loss)\n",
    "        self.mode.save(cpn)\n",
    "        \n",
    "def callback_model(model, checkpoint_name, logdir, X_test, y_test):\n",
    "  \n",
    "    _logdir = os.path.join(logdir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_callback = TensorBoard(_logdir, histogram_freq=1)\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                  factor=0.3,\n",
    "                                  patience=1,\n",
    "                                  mode='min',\n",
    "                                  verbose=1)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=7,\n",
    "                                   monitor='val_loss',\n",
    "                                   mode='min',\n",
    "                                   verbose=1)\n",
    "    \n",
    "    csv_logger = CSVLogger('log.log', separator=',', append=False)\n",
    "    \n",
    "    callbacks_list = [tensorboard_callback, reduce_lr, early_stopping, csv_logger, My_checkoint(model, X_test, y_test, checkpoint_name)]\n",
    "    \n",
    "    return callbacks_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xttwiHh4u0ES",
    "outputId": "31e678b8-9b64-4fb6-9154-525972b48aca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, checkpoint_name, logdir, batch_size=32, epochs=10, re_train=True):\n",
    "\n",
    "    if re_train:\n",
    "        # Clear old folder\n",
    "        %rmdir /q/s {logdir}\n",
    "        # Clear old file\n",
    "        path = checkpoint_name + '**'\n",
    "        all_path_files = glob(path)\n",
    "        for file in all_path_files:\n",
    "            os.remove(file)\n",
    "        # fit model\n",
    "        callbacks_list = callback_model(model, checkpoint_name, logdir, X_test, y_test)\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        #del model\n",
    "        \n",
    "    # Get best file by reading log file\n",
    "    df = pd.read_csv('log.log')\n",
    "    best_epoch = df.loc[df['val_loss']==df['val_loss'].min(), 'epoch'].values[0]\n",
    "    best_file = 'model-' + format(best_epoch, '02d') + '-.hdf5'\n",
    "    #best_file = os.path.join(checkpoint_dir, 'model'+format(best_epoch, '02d') + '-.hdf5')\n",
    "    model = load_model(best_file, custom_objects={'my_r2_score': my_r2_score,\n",
    "                                                                                 'gaussian': gaussian})\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">Train với bộ thông số tốt nhất trước khi cải tiến:\n",
    "    - units: 275\n",
    "    - batch_size: 128\n",
    "    - activation: sigmoid\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "   2/7425 [..............................] - ETA: 53:00 - loss: 3159.2061 - root_mean_squared_error: 56.2068 - my_r2_score: -613.6632WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.8510s). Check your callbacks.\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 2.0627 - root_mean_squared_error: 1.4362 - my_r2_score: 0.1854\n",
      "my_val_loss [2.06274676322937, 1.4362266063690186, 0.18540234863758087]\n",
      "7425/7425 [==============================] - 22s 3ms/step - loss: 6.9052 - root_mean_squared_error: 2.6278 - my_r2_score: -0.4246 - val_loss: 2.0627 - val_root_mean_squared_error: 1.4362 - val_my_r2_score: 0.1854\n",
      "Epoch 2/15\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 1.6530 - root_mean_squared_error: 1.2857 - my_r2_score: 0.3655\n",
      "my_val_loss [1.6530492305755615, 1.2857096195220947, 0.3655342161655426]\n",
      "7425/7425 [==============================] - 21s 3ms/step - loss: 1.4767 - root_mean_squared_error: 1.2152 - my_r2_score: 0.7125 - val_loss: 1.6530 - val_root_mean_squared_error: 1.2857 - val_my_r2_score: 0.3655\n",
      "Epoch 3/15\n",
      "7411/7425 [============================>.] - ETA: 0s - loss: 1.3880 - root_mean_squared_error: 1.1781 - my_r2_score: 0.7299\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "300/300 [==============================] - 1s 2ms/step - loss: 1.6612 - root_mean_squared_error: 1.2889 - my_r2_score: 0.3536\n",
      "my_val_loss [1.6612212657928467, 1.2888836860656738, 0.3536272943019867]\n",
      "7425/7425 [==============================] - 21s 3ms/step - loss: 1.3877 - root_mean_squared_error: 1.1780 - my_r2_score: 0.7299 - val_loss: 1.6612 - val_root_mean_squared_error: 1.2889 - val_my_r2_score: 0.3536\n",
      "Epoch 4/15\n",
      "3733/7425 [==============>...............] - ETA: 9s - loss: 0.7523 - root_mean_squared_error: 0.8674 - my_r2_score: 0.8564"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-d9448ec5bf6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_and_compile_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgaussian\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m model = train_model(model, X_train, y_train, X_test, y_test, checkpoint_name, logdir,\n\u001b[0m\u001b[0;32m      9\u001b[0m                     batch_size=64, epochs=15, re_train=True)\n",
      "\u001b[1;32m<ipython-input-17-97940642393f>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, X_train, y_train, X_test, y_test, checkpoint_name, logdir, batch_size, epochs, re_train)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mcallbacks_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         model.fit(X_train, y_train,\n\u001b[0m\u001b[0;32m     14\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \"\"\"\n\u001b[0;32m    439\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized hook: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    343\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Only convert once.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m           \u001b[0mnumpy_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    531\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     \"\"\"\n\u001b[0;32m   1062\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1027\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "checkpoint_name = 'model-'\n",
    "baseDir = os.path.abspath(os.getcwd())\n",
    "logs_name = 'training_logs'\n",
    "logdir = os.path.join(baseDir, logs_name)\n",
    "#checkpoint_dir = os.path.join(baseDir, checkpoint_name)\n",
    "model = build_and_compile_model(X_train, num_units=200, activation=gaussian)\n",
    "\n",
    "model = train_model(model, X_train, y_train, X_test, y_test, checkpoint_name, logdir,\n",
    "                    batch_size=64, epochs=15, re_train=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sai số rmse:                    0.736\n",
      "Hệ số xác định r2-score: 0.898\n",
      "Tỉ lệ True positive:           0.412\n",
      "time: 35.6 s\n"
     ]
    }
   ],
   "source": [
    "# On train\n",
    "train_result_df = evaluate(model, X_train, y_train_df)\n",
    "#train_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sai số rmse:                    0.855\n",
      "Hệ số xác định r2-score: 0.863\n",
      "Tỉ lệ True positive:           0.382\n",
      "time: 515 ms\n"
     ]
    }
   ],
   "source": [
    "# On test\n",
    "test_result_df = evaluate(model, X_test, y_test_df)\n",
    "#test_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">Nhận xét:\n",
    "    Hàm kích hoạt mới kém hơn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>lr</th>\n",
       "      <th>my_r2_score</th>\n",
       "      <th>root_mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_my_r2_score</th>\n",
       "      <th>val_root_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8.666882</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>-0.585802</td>\n",
       "      <td>2.943957</td>\n",
       "      <td>1.132825</td>\n",
       "      <td>0.705530</td>\n",
       "      <td>1.064343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.048181</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.800541</td>\n",
       "      <td>1.023807</td>\n",
       "      <td>1.476352</td>\n",
       "      <td>0.614940</td>\n",
       "      <td>1.215052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.678018</td>\n",
       "      <td>3.000000e-03</td>\n",
       "      <td>0.871873</td>\n",
       "      <td>0.823418</td>\n",
       "      <td>0.805324</td>\n",
       "      <td>0.792658</td>\n",
       "      <td>0.897398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.659664</td>\n",
       "      <td>3.000000e-03</td>\n",
       "      <td>0.875399</td>\n",
       "      <td>0.812197</td>\n",
       "      <td>0.796801</td>\n",
       "      <td>0.795216</td>\n",
       "      <td>0.892637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.646553</td>\n",
       "      <td>3.000000e-03</td>\n",
       "      <td>0.877985</td>\n",
       "      <td>0.804085</td>\n",
       "      <td>0.799723</td>\n",
       "      <td>0.795418</td>\n",
       "      <td>0.894272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.574599</td>\n",
       "      <td>9.000000e-04</td>\n",
       "      <td>0.891769</td>\n",
       "      <td>0.758023</td>\n",
       "      <td>0.791060</td>\n",
       "      <td>0.796617</td>\n",
       "      <td>0.889415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.565654</td>\n",
       "      <td>9.000000e-04</td>\n",
       "      <td>0.893476</td>\n",
       "      <td>0.752100</td>\n",
       "      <td>0.738551</td>\n",
       "      <td>0.809950</td>\n",
       "      <td>0.859390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.561114</td>\n",
       "      <td>9.000000e-04</td>\n",
       "      <td>0.894334</td>\n",
       "      <td>0.749076</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>0.811802</td>\n",
       "      <td>0.854671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.556122</td>\n",
       "      <td>9.000000e-04</td>\n",
       "      <td>0.895359</td>\n",
       "      <td>0.745736</td>\n",
       "      <td>0.805891</td>\n",
       "      <td>0.789868</td>\n",
       "      <td>0.897714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.532463</td>\n",
       "      <td>2.700000e-04</td>\n",
       "      <td>0.899704</td>\n",
       "      <td>0.729701</td>\n",
       "      <td>0.753456</td>\n",
       "      <td>0.805362</td>\n",
       "      <td>0.868018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.522291</td>\n",
       "      <td>8.100000e-05</td>\n",
       "      <td>0.901668</td>\n",
       "      <td>0.722697</td>\n",
       "      <td>0.746185</td>\n",
       "      <td>0.807156</td>\n",
       "      <td>0.863820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.518473</td>\n",
       "      <td>2.430000e-05</td>\n",
       "      <td>0.902509</td>\n",
       "      <td>0.720050</td>\n",
       "      <td>0.748766</td>\n",
       "      <td>0.806646</td>\n",
       "      <td>0.865313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.517138</td>\n",
       "      <td>7.290000e-06</td>\n",
       "      <td>0.902737</td>\n",
       "      <td>0.719123</td>\n",
       "      <td>0.748887</td>\n",
       "      <td>0.806536</td>\n",
       "      <td>0.865382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.516720</td>\n",
       "      <td>2.187000e-06</td>\n",
       "      <td>0.902863</td>\n",
       "      <td>0.718833</td>\n",
       "      <td>0.749998</td>\n",
       "      <td>0.806258</td>\n",
       "      <td>0.866024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.516581</td>\n",
       "      <td>6.560999e-07</td>\n",
       "      <td>0.902784</td>\n",
       "      <td>0.718735</td>\n",
       "      <td>0.750305</td>\n",
       "      <td>0.806174</td>\n",
       "      <td>0.866201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch      loss            lr  my_r2_score  root_mean_squared_error  \\\n",
       "0       1  8.666882  1.000000e-02    -0.585802                 2.943957   \n",
       "1       2  1.048181  1.000000e-02     0.800541                 1.023807   \n",
       "2       3  0.678018  3.000000e-03     0.871873                 0.823418   \n",
       "3       4  0.659664  3.000000e-03     0.875399                 0.812197   \n",
       "4       5  0.646553  3.000000e-03     0.877985                 0.804085   \n",
       "5       6  0.574599  9.000000e-04     0.891769                 0.758023   \n",
       "6       7  0.565654  9.000000e-04     0.893476                 0.752100   \n",
       "7       8  0.561114  9.000000e-04     0.894334                 0.749076   \n",
       "8       9  0.556122  9.000000e-04     0.895359                 0.745736   \n",
       "9      10  0.532463  2.700000e-04     0.899704                 0.729701   \n",
       "10     11  0.522291  8.100000e-05     0.901668                 0.722697   \n",
       "11     12  0.518473  2.430000e-05     0.902509                 0.720050   \n",
       "12     13  0.517138  7.290000e-06     0.902737                 0.719123   \n",
       "13     14  0.516720  2.187000e-06     0.902863                 0.718833   \n",
       "14     15  0.516581  6.560999e-07     0.902784                 0.718735   \n",
       "\n",
       "    val_loss  val_my_r2_score  val_root_mean_squared_error  \n",
       "0   1.132825         0.705530                     1.064343  \n",
       "1   1.476352         0.614940                     1.215052  \n",
       "2   0.805324         0.792658                     0.897398  \n",
       "3   0.796801         0.795216                     0.892637  \n",
       "4   0.799723         0.795418                     0.894272  \n",
       "5   0.791060         0.796617                     0.889415  \n",
       "6   0.738551         0.809950                     0.859390  \n",
       "7   0.730463         0.811802                     0.854671  \n",
       "8   0.805891         0.789868                     0.897714  \n",
       "9   0.753456         0.805362                     0.868018  \n",
       "10  0.746185         0.807156                     0.863820  \n",
       "11  0.748766         0.806646                     0.865313  \n",
       "12  0.748887         0.806536                     0.865382  \n",
       "13  0.749998         0.806258                     0.866024  \n",
       "14  0.750305         0.806174                     0.866201  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "log_data = pd.read_csv('log.log')\n",
    "log_data['epoch'] = log_data['epoch'] + 1\n",
    "log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 375 ms\n"
     ]
    }
   ],
   "source": [
    "def plot_loss(log_data):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    #fig.add_subplot(1, 2, 1)\n",
    "    sns.lineplot(x='epoch', y='root_mean_squared_error', data=log_data, marker='s', label='train_loss');\n",
    "    sns.lineplot(x='epoch', y='val_root_mean_squared_error', data=log_data, marker='s', label='val_loss');\n",
    "    plt.xlabel('Epoch', size=13)\n",
    "    plt.xticks(np.arange(1, log_data['epoch'].max()+1, step=1))\n",
    "    plt.ylabel('Error [speed]', size=13)\n",
    "    plt.legend()\n",
    "    plt.title('Learning-curve', size=15, y=1.02)\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_loss(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 391 ms\n"
     ]
    }
   ],
   "source": [
    "def plot_r2(log_data):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    #fig.add_subplot(1, 2, 1)\n",
    "    sns.lineplot(x='epoch', y='my_r2_score', data=log_data, marker='s', label='train_score');\n",
    "    sns.lineplot(x='epoch', y='val_my_r2_score', data=log_data, marker='s', label='val_score');\n",
    "    plt.xlabel('Epoch', size=13)\n",
    "    plt.xticks(np.arange(1, log_data['epoch'].max()+1, step=1))\n",
    "    plt.ylabel('rate', size=13)\n",
    "    plt.legend()\n",
    "    plt.title('Score-curve', size=15, y=1.02)\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_r2(log_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tune parameter\n",
    "- Điều chỉnh các siêu tham số bằng phương pháp BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners.bayesian import BayesianOptimization\n",
    "from kerastuner import HyperModel\n",
    "\n",
    "class RegressionHyperModel(HyperModel):\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            Dense(\n",
    "                units=hp.Int('units', 100, 300, 5, default=100),\n",
    "                activation=hp.Choice(\n",
    "                    'activation',\n",
    "                    values=['gelu', 'sigmoid', 'tanh'],\n",
    "                    default='sigmoid'),\n",
    "                kernel_initializer='he_normal',\n",
    "                input_shape=self.input_shape\n",
    "            )\n",
    "        )\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "                                           optimizer=Optimizer.Adam(0.01)) \n",
    "        return model\n",
    "    \n",
    "class MyTuner(BayesianOptimization):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 16, 128, step=16)\n",
    "        #kwargs['epochs'] = trial.hyperparameters.Int('epochs', 10, 20, 5)\n",
    "        super(MyTuner, self).run_trial(trial, *args, **kwargs)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "units             |230               |?                 \n",
      "activation        |gelu              |?                 \n",
      "\n",
      "Invalid model 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 104, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-28-5ec84f32ba3d>\", line 12, in build\n",
      "    Dense(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\", line 1147, in __init__\n",
      "    self.activation = activations.get(activation)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\", line 529, in get\n",
      "    return deserialize(identifier)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\", line 488, in deserialize\n",
      "    return deserialize_keras_object(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 377, in deserialize_keras_object\n",
      "    raise ValueError(\n",
      "ValueError: Unknown activation function: gelu\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 104, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-28-5ec84f32ba3d>\", line 12, in build\n",
      "    Dense(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\", line 1147, in __init__\n",
      "    self.activation = activations.get(activation)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\", line 529, in get\n",
      "    return deserialize(identifier)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\", line 488, in deserialize\n",
      "    return deserialize_keras_object(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 377, in deserialize_keras_object\n",
      "    raise ValueError(\n",
      "ValueError: Unknown activation function: gelu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid model 1/5\n",
      "Invalid model 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 104, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-28-5ec84f32ba3d>\", line 12, in build\n",
      "    Dense(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\", line 1147, in __init__\n",
      "    self.activation = activations.get(activation)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\", line 529, in get\n",
      "    return deserialize(identifier)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\", line 488, in deserialize\n",
      "    return deserialize_keras_object(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 377, in deserialize_keras_object\n",
      "    raise ValueError(\n",
      "ValueError: Unknown activation function: gelu\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 104, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-28-5ec84f32ba3d>\", line 12, in build\n",
      "    Dense(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\", line 1147, in __init__\n",
      "    self.activation = activations.get(activation)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\", line 529, in get\n",
      "    return deserialize(identifier)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\", line 488, in deserialize\n",
      "    return deserialize_keras_object(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 377, in deserialize_keras_object\n",
      "    raise ValueError(\n",
      "ValueError: Unknown activation function: gelu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid model 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 104, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-28-5ec84f32ba3d>\", line 12, in build\n",
      "    Dense(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\", line 1147, in __init__\n",
      "    self.activation = activations.get(activation)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\", line 529, in get\n",
      "    return deserialize(identifier)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\", line 488, in deserialize\n",
      "    return deserialize_keras_object(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 377, in deserialize_keras_object\n",
      "    raise ValueError(\n",
      "ValueError: Unknown activation function: gelu\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 104, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-28-5ec84f32ba3d>\", line 12, in build\n",
      "    Dense(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\", line 1147, in __init__\n",
      "    self.activation = activations.get(activation)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\", line 529, in get\n",
      "    return deserialize(identifier)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\", line 488, in deserialize\n",
      "    return deserialize_keras_object(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 377, in deserialize_keras_object\n",
      "    raise ValueError(\n",
      "ValueError: Unknown activation function: gelu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid model 4/5\n",
      "Invalid model 5/5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Too many failed attempts to build model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mmaybe_distribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-5ec84f32ba3d>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m     11\u001b[0m         model.add(\n\u001b[1;32m---> 12\u001b[1;33m             Dense(\n\u001b[0m\u001b[0;32m     13\u001b[0m                 \u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'units'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1148\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[0midentifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(name, custom_objects)\u001b[0m\n\u001b[0;32m    487\u001b[0m   \"\"\"\n\u001b[1;32m--> 488\u001b[1;33m   return deserialize_keras_object(\n\u001b[0m\u001b[0;32m    489\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    376\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    378\u001b[0m             'Unknown ' + printable_module_name + ': ' + object_name)\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown activation function: gelu",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-869f427a1769>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             )\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\kerastuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-5ec84f32ba3d>\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m#kwargs['epochs'] = trial.hyperparameters.Int('epochs', 10, 20, 5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMyTuner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\kerastuner\\engine\\multi_execution_tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'callbacks'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'min'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\kerastuner\\engine\\tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[1;34m(self, trial, fit_args, fit_kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mfit\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \"\"\"\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\u001b[0m in \u001b[0;36m_build_wrapper\u001b[1;34m(self, hp, *args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;31m# to the search space.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mhp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_fail_streak\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m                     raise RuntimeError(\n\u001b[0m\u001b[0;32m    113\u001b[0m                         'Too many failed attempts to build model.')\n\u001b[0;32m    114\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Too many failed attempts to build model."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.36 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_shape = (X_train.shape[1], )\n",
    "hypermodel = RegressionHyperModel(input_shape)\n",
    "project_name = 'gaussian_search'\n",
    "\n",
    "tuner = MyTuner(\n",
    "            hypermodel,\n",
    "            objective='val_loss',\n",
    "            max_trials=10,\n",
    "            executions_per_trial=2,\n",
    "            seed=42,\n",
    "            project_name=project_name,\n",
    "            overwrite=True\n",
    "            )\n",
    "tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=15, callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=3)], verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các bộ tham số tốt nhất\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>activation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180</td>\n",
       "      <td>112</td>\n",
       "      <td>sigmoid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>145</td>\n",
       "      <td>96</td>\n",
       "      <td>sigmoid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>sigmoid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  batch_size activation\n",
       "0    180         112    sigmoid\n",
       "1    145          96    sigmoid\n",
       "2    190          80    sigmoid"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "list_best_hp = tuner.get_best_hyperparameters(num_trials=3)\n",
    "list_best_units = []\n",
    "list_best_batch_size = []\n",
    "list_best_activation = []\n",
    "\n",
    "for best_hp in list_best_hp:\n",
    "    best_units = best_hp.get('units')\n",
    "    list_best_units.append(best_units)\n",
    "    best_batch_size = best_hp.get('batch_size')\n",
    "    list_best_batch_size.append(best_batch_size)\n",
    "    best_activation = best_hp.get('activation')\n",
    "    list_best_activation.append(best_activation)\n",
    "\n",
    "hp_result = pd.DataFrame({'units':list_best_units,\n",
    "                            'batch_size': list_best_batch_size,\n",
    "                            'activation': list_best_activation\n",
    "                            })\n",
    "print('Các bộ tham số tốt nhất')\n",
    "hp_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">Phương pháp:\n",
    "    - Ta train lại 3 mô hình tốt nhất để đánh giá\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "   2/4243 [..............................] - ETA: 4:30:43 - loss: 3340.3420 - root_mean_squared_error: 57.7957 - my_r2_score: -615.6491WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 7.6564s). Check your callbacks.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7374 - root_mean_squared_error: 0.8587 - my_r2_score: 0.7954\n",
      "my_val_loss [0.7374114394187927, 0.8587266206741333, 0.7953792214393616]\n",
      "4243/4243 [==============================] - 20s 5ms/step - loss: 11.9850 - root_mean_squared_error: 3.4619 - my_r2_score: -1.2797 - val_loss: 0.7374 - val_root_mean_squared_error: 0.8587 - val_my_r2_score: 0.7954\n",
      "Epoch 2/15\n",
      "4234/4243 [============================>.] - ETA: 0s - loss: 0.6467 - root_mean_squared_error: 0.8042 - my_r2_score: 0.8778\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7409 - root_mean_squared_error: 0.8608 - my_r2_score: 0.7967\n",
      "my_val_loss [0.7409430742263794, 0.8607804775238037, 0.7966784238815308]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.6466 - root_mean_squared_error: 0.8041 - my_r2_score: 0.8778 - val_loss: 0.7409 - val_root_mean_squared_error: 0.8608 - val_my_r2_score: 0.7967\n",
      "Epoch 3/15\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7346 - root_mean_squared_error: 0.8571 - my_r2_score: 0.7957\n",
      "my_val_loss [0.7346359491348267, 0.8571090698242188, 0.7956712245941162]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5961 - root_mean_squared_error: 0.7721 - my_r2_score: 0.8874 - val_loss: 0.7346 - val_root_mean_squared_error: 0.8571 - val_my_r2_score: 0.7957\n",
      "Epoch 4/15\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.6880 - root_mean_squared_error: 0.8294 - my_r2_score: 0.8089\n",
      "my_val_loss [0.6879829168319702, 0.829447329044342, 0.8089497685432434]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5845 - root_mean_squared_error: 0.7645 - my_r2_score: 0.8897 - val_loss: 0.6880 - val_root_mean_squared_error: 0.8294 - val_my_r2_score: 0.8089\n",
      "Epoch 5/15\n",
      "4241/4243 [============================>.] - ETA: 0s - loss: 0.5761 - root_mean_squared_error: 0.7590 - my_r2_score: 0.8914\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7277 - root_mean_squared_error: 0.8530 - my_r2_score: 0.7957\n",
      "my_val_loss [0.7276540398597717, 0.8530263900756836, 0.7956914305686951]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5761 - root_mean_squared_error: 0.7590 - my_r2_score: 0.8914 - val_loss: 0.7277 - val_root_mean_squared_error: 0.8530 - val_my_r2_score: 0.7957\n",
      "Epoch 6/15\n",
      "4238/4243 [============================>.] - ETA: 0s - loss: 0.5523 - root_mean_squared_error: 0.7432 - my_r2_score: 0.8959\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7119 - root_mean_squared_error: 0.8437 - my_r2_score: 0.7998\n",
      "my_val_loss [0.7118668556213379, 0.8437220454216003, 0.7998181581497192]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5524 - root_mean_squared_error: 0.7432 - my_r2_score: 0.8959 - val_loss: 0.7119 - val_root_mean_squared_error: 0.8437 - val_my_r2_score: 0.7998\n",
      "Epoch 7/15\n",
      "4237/4243 [============================>.] - ETA: 0s - loss: 0.5428 - root_mean_squared_error: 0.7368 - my_r2_score: 0.8976\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7073 - root_mean_squared_error: 0.8410 - my_r2_score: 0.8017\n",
      "my_val_loss [0.7072784900665283, 0.8409985303878784, 0.8016982078552246]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5428 - root_mean_squared_error: 0.7367 - my_r2_score: 0.8976 - val_loss: 0.7073 - val_root_mean_squared_error: 0.8410 - val_my_r2_score: 0.8017\n",
      "Epoch 8/15\n",
      "4243/4243 [==============================] - ETA: 0s - loss: 0.5393 - root_mean_squared_error: 0.7344 - my_r2_score: 0.8984\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "  1/171 [..............................] - ETA: 0s - loss: 1.1603 - root_mean_squared_error: 1.0772 - my_r2_score: 0.7370WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7160 - root_mean_squared_error: 0.8461 - my_r2_score: 0.7991\n",
      "my_val_loss [0.7159593105316162, 0.8461437821388245, 0.7990587949752808]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5393 - root_mean_squared_error: 0.7344 - my_r2_score: 0.8984 - val_loss: 0.7160 - val_root_mean_squared_error: 0.8461 - val_my_r2_score: 0.7991\n",
      "Epoch 9/15\n",
      "4233/4243 [============================>.] - ETA: 0s - loss: 0.5383 - root_mean_squared_error: 0.7337 - my_r2_score: 0.8985\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7157 - root_mean_squared_error: 0.8460 - my_r2_score: 0.7991\n",
      "my_val_loss [0.7157140374183655, 0.8459988236427307, 0.7990699410438538]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5382 - root_mean_squared_error: 0.7336 - my_r2_score: 0.8985 - val_loss: 0.7157 - val_root_mean_squared_error: 0.8460 - val_my_r2_score: 0.7991\n",
      "Epoch 10/15\n",
      "4236/4243 [============================>.] - ETA: 0s - loss: 0.5378 - root_mean_squared_error: 0.7334 - my_r2_score: 0.8987\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7223 - root_mean_squared_error: 0.8499 - my_r2_score: 0.7971\n",
      "my_val_loss [0.7223249673843384, 0.849897027015686, 0.797084629535675]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5379 - root_mean_squared_error: 0.7334 - my_r2_score: 0.8986 - val_loss: 0.7223 - val_root_mean_squared_error: 0.8499 - val_my_r2_score: 0.7971\n",
      "Epoch 11/15\n",
      "4236/4243 [============================>.] - ETA: 0s - loss: 0.5378 - root_mean_squared_error: 0.7334 - my_r2_score: 0.8986\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.560999509019894e-07.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7183 - root_mean_squared_error: 0.8475 - my_r2_score: 0.7983\n",
      "my_val_loss [0.7183032035827637, 0.8475276827812195, 0.798301637172699]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5378 - root_mean_squared_error: 0.7333 - my_r2_score: 0.8986 - val_loss: 0.7183 - val_root_mean_squared_error: 0.8475 - val_my_r2_score: 0.7983\n",
      "Epoch 00011: early stopping\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.6880 - root_mean_squared_error: 0.8294 - my_r2_score: 0.5881\n",
      "Epoch 1/15\n",
      "   2/4950 [..............................] - ETA: 36:24 - loss: 3349.7966 - root_mean_squared_error: 57.8774 - my_r2_score: -749.9609WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.8791s). Check your callbacks.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7238 - root_mean_squared_error: 0.8508 - my_r2_score: 0.7816\n",
      "my_val_loss [0.7237879037857056, 0.8507572412490845, 0.7816153168678284]\n",
      "4950/4950 [==============================] - 15s 3ms/step - loss: 13.0689 - root_mean_squared_error: 3.6151 - my_r2_score: -1.6618 - val_loss: 0.7238 - val_root_mean_squared_error: 0.8508 - val_my_r2_score: 0.7816\n",
      "Epoch 2/15\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7078 - root_mean_squared_error: 0.8413 - my_r2_score: 0.7883\n",
      "my_val_loss [0.7078443169593811, 0.8413348197937012, 0.788343071937561]\n",
      "4950/4950 [==============================] - 14s 3ms/step - loss: 0.6476 - root_mean_squared_error: 0.8047 - my_r2_score: 0.8774 - val_loss: 0.7078 - val_root_mean_squared_error: 0.8413 - val_my_r2_score: 0.7883\n",
      "Epoch 3/15\n",
      "4943/4950 [============================>.] - ETA: 0s - loss: 0.6323 - root_mean_squared_error: 0.7952 - my_r2_score: 0.8801\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7427 - root_mean_squared_error: 0.8618 - my_r2_score: 0.7814\n",
      "my_val_loss [0.7426734566688538, 0.8617850542068481, 0.7814311385154724]\n",
      "4950/4950 [==============================] - 14s 3ms/step - loss: 0.6324 - root_mean_squared_error: 0.7952 - my_r2_score: 0.8801 - val_loss: 0.7427 - val_root_mean_squared_error: 0.8618 - val_my_r2_score: 0.7814\n",
      "Epoch 4/15\n",
      "4938/4950 [============================>.] - ETA: 0s - loss: 0.5860 - root_mean_squared_error: 0.7655 - my_r2_score: 0.8892\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "  1/200 [..............................] - ETA: 0s - loss: 1.4789 - root_mean_squared_error: 1.2161 - my_r2_score: 0.6207WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7728 - root_mean_squared_error: 0.8791 - my_r2_score: 0.7686\n",
      "my_val_loss [0.7727735638618469, 0.8790754079818726, 0.7685787081718445]\n",
      "4950/4950 [==============================] - 14s 3ms/step - loss: 0.5859 - root_mean_squared_error: 0.7654 - my_r2_score: 0.8892 - val_loss: 0.7728 - val_root_mean_squared_error: 0.8791 - val_my_r2_score: 0.7686\n",
      "Epoch 5/15\n",
      "4950/4950 [==============================] - ETA: 0s - loss: 0.5625 - root_mean_squared_error: 0.7500 - my_r2_score: 0.8937\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7272 - root_mean_squared_error: 0.8528 - my_r2_score: 0.7822\n",
      "my_val_loss [0.7272337675094604, 0.8527800440788269, 0.782224178314209]\n",
      "4950/4950 [==============================] - 14s 3ms/step - loss: 0.5625 - root_mean_squared_error: 0.7500 - my_r2_score: 0.8937 - val_loss: 0.7272 - val_root_mean_squared_error: 0.8528 - val_my_r2_score: 0.7822\n",
      "Epoch 6/15\n",
      "4942/4950 [============================>.] - ETA: 0s - loss: 0.5535 - root_mean_squared_error: 0.7440 - my_r2_score: 0.8953\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7119 - root_mean_squared_error: 0.8438 - my_r2_score: 0.7881\n",
      "my_val_loss [0.7119484543800354, 0.8437703847885132, 0.7880886793136597]\n",
      "4950/4950 [==============================] - 14s 3ms/step - loss: 0.5536 - root_mean_squared_error: 0.7440 - my_r2_score: 0.8953 - val_loss: 0.7119 - val_root_mean_squared_error: 0.8438 - val_my_r2_score: 0.7881\n",
      "Epoch 7/15\n",
      "4945/4950 [============================>.] - ETA: 0s - loss: 0.5508 - root_mean_squared_error: 0.7421 - my_r2_score: 0.8960\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7284 - root_mean_squared_error: 0.8535 - my_r2_score: 0.7822\n",
      "my_val_loss [0.7283854484558105, 0.8534550070762634, 0.7822086811065674]\n",
      "4950/4950 [==============================] - 13s 3ms/step - loss: 0.5507 - root_mean_squared_error: 0.7421 - my_r2_score: 0.8960 - val_loss: 0.7284 - val_root_mean_squared_error: 0.8535 - val_my_r2_score: 0.7822\n",
      "Epoch 8/15\n",
      "4931/4950 [============================>.] - ETA: 0s - loss: 0.5498 - root_mean_squared_error: 0.7415 - my_r2_score: 0.8960\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7181 - root_mean_squared_error: 0.8474 - my_r2_score: 0.7856\n",
      "my_val_loss [0.7181290984153748, 0.8474249839782715, 0.785630464553833]\n",
      "4950/4950 [==============================] - 13s 3ms/step - loss: 0.5497 - root_mean_squared_error: 0.7414 - my_r2_score: 0.8961 - val_loss: 0.7181 - val_root_mean_squared_error: 0.8474 - val_my_r2_score: 0.7856\n",
      "Epoch 9/15\n",
      "4948/4950 [============================>.] - ETA: 0s - loss: 0.5494 - root_mean_squared_error: 0.7412 - my_r2_score: 0.8962\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7183 - root_mean_squared_error: 0.8475 - my_r2_score: 0.7856\n",
      "my_val_loss [0.7182847261428833, 0.8475167751312256, 0.785618007183075]\n",
      "4950/4950 [==============================] - 13s 3ms/step - loss: 0.5494 - root_mean_squared_error: 0.7412 - my_r2_score: 0.8962 - val_loss: 0.7183 - val_root_mean_squared_error: 0.8475 - val_my_r2_score: 0.7856\n",
      "Epoch 00009: early stopping\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.7078 - root_mean_squared_error: 0.8413 - my_r2_score: 0.5890\n",
      "Epoch 1/15\n",
      "   2/5940 [..............................] - ETA: 51:13 - loss: 3307.3359 - root_mean_squared_error: 57.5094 - my_r2_score: -540.0782WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 1.0311s). Check your callbacks.\n",
      "  1/240 [..............................] - ETA: 0s - loss: 0.4959 - root_mean_squared_error: 0.7042 - my_r2_score: 0.8532617WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 0.8128 - root_mean_squared_error: 0.9016 - my_r2_score: 0.7381\n",
      "my_val_loss [0.8128467202186584, 0.9015800952911377, 0.7380561828613281]\n",
      "5940/5940 [==============================] - 17s 3ms/step - loss: 8.2044 - root_mean_squared_error: 2.8643 - my_r2_score: -0.5571 - val_loss: 0.8128 - val_root_mean_squared_error: 0.9016 - val_my_r2_score: 0.7381\n",
      "Epoch 2/15\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 0.7590 - root_mean_squared_error: 0.8712 - my_r2_score: 0.7580\n",
      "my_val_loss [0.7590498924255371, 0.8712347149848938, 0.7580031752586365]\n",
      "5940/5940 [==============================] - 16s 3ms/step - loss: 0.6539 - root_mean_squared_error: 0.8086 - my_r2_score: 0.8756 - val_loss: 0.7590 - val_root_mean_squared_error: 0.8712 - val_my_r2_score: 0.7580\n",
      "Epoch 3/15\n",
      "5932/5940 [============================>.] - ETA: 0s - loss: 0.6321 - root_mean_squared_error: 0.7951 - my_r2_score: 0.8799\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 0.7812 - root_mean_squared_error: 0.8839 - my_r2_score: 0.7470\n",
      "my_val_loss [0.781218945980072, 0.8838658928871155, 0.7470042705535889]\n",
      "5940/5940 [==============================] - 16s 3ms/step - loss: 0.6320 - root_mean_squared_error: 0.7950 - my_r2_score: 0.8800 - val_loss: 0.7812 - val_root_mean_squared_error: 0.8839 - val_my_r2_score: 0.7470\n",
      "Epoch 4/15\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 0.7289 - root_mean_squared_error: 0.8537 - my_r2_score: 0.7668\n",
      "my_val_loss [0.7288779616355896, 0.8537434935569763, 0.7667943239212036]\n",
      "5940/5940 [==============================] - 16s 3ms/step - loss: 0.5781 - root_mean_squared_error: 0.7603 - my_r2_score: 0.8903 - val_loss: 0.7289 - val_root_mean_squared_error: 0.8537 - val_my_r2_score: 0.7668\n",
      "Epoch 5/15\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 0.7005 - root_mean_squared_error: 0.8370 - my_r2_score: 0.7752\n",
      "my_val_loss [0.7005110383033752, 0.8369653820991516, 0.7752311825752258]\n",
      "5940/5940 [==============================] - 15s 3ms/step - loss: 0.5681 - root_mean_squared_error: 0.7538 - my_r2_score: 0.8922 - val_loss: 0.7005 - val_root_mean_squared_error: 0.8370 - val_my_r2_score: 0.7752\n",
      "Epoch 6/15\n",
      "5928/5940 [============================>.] - ETA: 0s - loss: 0.5614 - root_mean_squared_error: 0.7493 - my_r2_score: 0.8937\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 0.7220 - root_mean_squared_error: 0.8497 - my_r2_score: 0.7666\n",
      "my_val_loss [0.7220300436019897, 0.8497235178947449, 0.7665722370147705]\n",
      "5940/5940 [==============================] - 15s 3ms/step - loss: 0.5614 - root_mean_squared_error: 0.7493 - my_r2_score: 0.8936 - val_loss: 0.7220 - val_root_mean_squared_error: 0.8497 - val_my_r2_score: 0.7666\n",
      "Epoch 7/15\n",
      "5921/5940 [============================>.] - ETA: 0s - loss: 0.5373 - root_mean_squared_error: 0.7330 - my_r2_score: 0.8981\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.7134 - root_mean_squared_error: 0.8446 - my_r2_score: 0.7689\n",
      "my_val_loss [0.7133846282958984, 0.8446210026741028, 0.7688734531402588]\n",
      "5940/5940 [==============================] - 15s 3ms/step - loss: 0.5379 - root_mean_squared_error: 0.7334 - my_r2_score: 0.8981 - val_loss: 0.7134 - val_root_mean_squared_error: 0.8446 - val_my_r2_score: 0.7689\n",
      "Epoch 8/15\n",
      "5936/5940 [============================>.] - ETA: 0s - loss: 0.5287 - root_mean_squared_error: 0.7271 - my_r2_score: 0.8998\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "  1/240 [..............................] - ETA: 0s - loss: 0.5964 - root_mean_squared_error: 0.7723 - my_r2_score: 0.8234WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 0.7420 - root_mean_squared_error: 0.8614 - my_r2_score: 0.7600\n",
      "my_val_loss [0.7420170903205872, 0.8614041209220886, 0.7600077986717224]\n",
      "5940/5940 [==============================] - 16s 3ms/step - loss: 0.5287 - root_mean_squared_error: 0.7271 - my_r2_score: 0.8999 - val_loss: 0.7420 - val_root_mean_squared_error: 0.8614 - val_my_r2_score: 0.7600\n",
      "Epoch 9/15\n",
      "5922/5940 [============================>.] - ETA: 0s - loss: 0.5256 - root_mean_squared_error: 0.7250 - my_r2_score: 0.9004\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.7418 - root_mean_squared_error: 0.8613 - my_r2_score: 0.7600\n",
      "my_val_loss [0.741833508014679, 0.8612975478172302, 0.7599981427192688]\n",
      "5940/5940 [==============================] - 16s 3ms/step - loss: 0.5255 - root_mean_squared_error: 0.7249 - my_r2_score: 0.9004 - val_loss: 0.7418 - val_root_mean_squared_error: 0.8613 - val_my_r2_score: 0.7600\n",
      "Epoch 10/15\n",
      "5939/5940 [============================>.] - ETA: 0s - loss: 0.5244 - root_mean_squared_error: 0.7242 - my_r2_score: 0.9008\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 0.7308 - root_mean_squared_error: 0.8549 - my_r2_score: 0.7635\n",
      "my_val_loss [0.7307889461517334, 0.8548619747161865, 0.7634697556495667]\n",
      "5940/5940 [==============================] - 16s 3ms/step - loss: 0.5244 - root_mean_squared_error: 0.7242 - my_r2_score: 0.9008 - val_loss: 0.7308 - val_root_mean_squared_error: 0.8549 - val_my_r2_score: 0.7635\n",
      "Epoch 11/15\n",
      "5930/5940 [============================>.] - ETA: 0s - loss: 0.5240 - root_mean_squared_error: 0.7239 - my_r2_score: 0.9007\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.7306 - root_mean_squared_error: 0.8547 - my_r2_score: 0.7635\n",
      "my_val_loss [0.7305514812469482, 0.8547230362892151, 0.7635369896888733]\n",
      "5940/5940 [==============================] - 15s 3ms/step - loss: 0.5241 - root_mean_squared_error: 0.7239 - my_r2_score: 0.9007 - val_loss: 0.7306 - val_root_mean_squared_error: 0.8547 - val_my_r2_score: 0.7635\n",
      "Epoch 12/15\n",
      "5927/5940 [============================>.] - ETA: 0s - loss: 0.5240 - root_mean_squared_error: 0.7239 - my_r2_score: 0.9007\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.560999509019894e-07.\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 0.7291 - root_mean_squared_error: 0.8539 - my_r2_score: 0.7640\n",
      "my_val_loss [0.7291043400764465, 0.8538760542869568, 0.7640120983123779]\n",
      "5940/5940 [==============================] - 16s 3ms/step - loss: 0.5240 - root_mean_squared_error: 0.7239 - my_r2_score: 0.9006 - val_loss: 0.7291 - val_root_mean_squared_error: 0.8539 - val_my_r2_score: 0.7640\n",
      "Epoch 00012: early stopping\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.7005 - root_mean_squared_error: 0.8370 - my_r2_score: 0.5797\n",
      "time: 7min 45s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_val_loss = []\n",
    "for num_units, activation, batch_size in zip(list_best_units, list_best_activation, list_best_batch_size):\n",
    "    model = build_and_compile_model(X_train, num_units=num_units, activation=activation)\n",
    "    model = train_model(model, X_train, y_train, X_test, y_test, checkpoint_name, logdir,\n",
    "                        batch_size=batch_size, epochs=15, re_train=True)\n",
    "    val_loss, _, _ = model.evaluate(X_test, y_test)\n",
    "    list_val_loss.append(val_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>activation</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180</td>\n",
       "      <td>112</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.687983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>145</td>\n",
       "      <td>96</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.707844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.700511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  batch_size activation  val_loss\n",
       "0    180         112    sigmoid  0.687983\n",
       "1    145          96    sigmoid  0.707844\n",
       "2    190          80    sigmoid  0.700511"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "improved_result = pd.DataFrame({'units':list_best_units,\n",
    "                            'batch_size': list_best_batch_size,\n",
    "                            'activation': list_best_activation,\n",
    "                            'val_loss': list_val_loss})\n",
    "improved_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "# Save improve_result\n",
    "improved_result.to_csv('improved_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">Ta chọn bộ tham số tốt nhất:\n",
    "    - units: 180\n",
    "    - batch_size: 112\n",
    "    - activation: sigmoid\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "   2/4243 [..............................] - ETA: 29:01 - loss: 3324.7104 - root_mean_squared_error: 57.6603 - my_r2_score: -567.5795WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.8164s). Check your callbacks.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.6919 - root_mean_squared_error: 0.8318 - my_r2_score: 0.8086\n",
      "my_val_loss [0.6919029951095581, 0.8318070769309998, 0.8085955381393433]\n",
      "4243/4243 [==============================] - 14s 3ms/step - loss: 12.0107 - root_mean_squared_error: 3.4656 - my_r2_score: -1.3215 - val_loss: 0.6919 - val_root_mean_squared_error: 0.8318 - val_my_r2_score: 0.8086\n",
      "Epoch 2/15\n",
      "4231/4243 [============================>.] - ETA: 0s - loss: 0.6476 - root_mean_squared_error: 0.8047 - my_r2_score: 0.8776\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.6994 - root_mean_squared_error: 0.8363 - my_r2_score: 0.8070\n",
      "my_val_loss [0.6994108557701111, 0.8363078832626343, 0.8070346713066101]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.6475 - root_mean_squared_error: 0.8047 - my_r2_score: 0.8776 - val_loss: 0.6994 - val_root_mean_squared_error: 0.8363 - val_my_r2_score: 0.8070\n",
      "Epoch 3/15\n",
      "4229/4243 [============================>.] - ETA: 0s - loss: 0.5994 - root_mean_squared_error: 0.7742 - my_r2_score: 0.8869\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7522 - root_mean_squared_error: 0.8673 - my_r2_score: 0.7917\n",
      "my_val_loss [0.7522487044334412, 0.8673227429389954, 0.79169100522995]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5998 - root_mean_squared_error: 0.7745 - my_r2_score: 0.8868 - val_loss: 0.7522 - val_root_mean_squared_error: 0.8673 - val_my_r2_score: 0.7917\n",
      "Epoch 4/15\n",
      "4237/4243 [============================>.] - ETA: 0s - loss: 0.5769 - root_mean_squared_error: 0.7595 - my_r2_score: 0.8914\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.6966 - root_mean_squared_error: 0.8346 - my_r2_score: 0.8080\n",
      "my_val_loss [0.6966375112533569, 0.8346481323242188, 0.8079749345779419]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5768 - root_mean_squared_error: 0.7595 - my_r2_score: 0.8914 - val_loss: 0.6966 - val_root_mean_squared_error: 0.8346 - val_my_r2_score: 0.8080\n",
      "Epoch 5/15\n",
      "4225/4243 [============================>.] - ETA: 0s - loss: 0.5671 - root_mean_squared_error: 0.7530 - my_r2_score: 0.8931\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7046 - root_mean_squared_error: 0.8394 - my_r2_score: 0.8058\n",
      "my_val_loss [0.7046274542808533, 0.8394209146499634, 0.8058498501777649]\n",
      "4243/4243 [==============================] - 11s 3ms/step - loss: 0.5674 - root_mean_squared_error: 0.7532 - my_r2_score: 0.8931 - val_loss: 0.7046 - val_root_mean_squared_error: 0.8394 - val_my_r2_score: 0.8058\n",
      "Epoch 6/15\n",
      "4239/4243 [============================>.] - ETA: 0s - loss: 0.5643 - root_mean_squared_error: 0.7512 - my_r2_score: 0.8937\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7323 - root_mean_squared_error: 0.8557 - my_r2_score: 0.7972\n",
      "my_val_loss [0.7322805523872375, 0.8557339310646057, 0.7972412109375]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5642 - root_mean_squared_error: 0.7512 - my_r2_score: 0.8937 - val_loss: 0.7323 - val_root_mean_squared_error: 0.8557 - val_my_r2_score: 0.7972\n",
      "Epoch 7/15\n",
      "4237/4243 [============================>.] - ETA: 0s - loss: 0.5632 - root_mean_squared_error: 0.7504 - my_r2_score: 0.8937\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7124 - root_mean_squared_error: 0.8440 - my_r2_score: 0.8032\n",
      "my_val_loss [0.7124181389808655, 0.8440486788749695, 0.8032339215278625]\n",
      "4243/4243 [==============================] - 13s 3ms/step - loss: 0.5631 - root_mean_squared_error: 0.7504 - my_r2_score: 0.8937 - val_loss: 0.7124 - val_root_mean_squared_error: 0.8440 - val_my_r2_score: 0.8032\n",
      "Epoch 8/15\n",
      "4243/4243 [==============================] - ETA: 0s - loss: 0.5628 - root_mean_squared_error: 0.7502 - my_r2_score: 0.8941\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7143 - root_mean_squared_error: 0.8451 - my_r2_score: 0.8027\n",
      "my_val_loss [0.7142524719238281, 0.8451346158981323, 0.8026557564735413]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5628 - root_mean_squared_error: 0.7502 - my_r2_score: 0.8941 - val_loss: 0.7143 - val_root_mean_squared_error: 0.8451 - val_my_r2_score: 0.8027\n",
      "Epoch 00008: early stopping\n",
      "time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "model = build_and_compile_model(X_train, num_units=180, activation='sigmoid')\n",
    "model = train_model(model, X_train, y_train, X_test, y_test, checkpoint_name, logdir,\n",
    "                        batch_size=112, epochs=15, re_train=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sai số rmse:                    0.817\n",
      "Hệ số xác định r2-score: 0.875\n",
      "Tỉ lệ True positive:           0.407\n",
      "time: 27.1 s\n"
     ]
    }
   ],
   "source": [
    "# On train\n",
    "train_result_df = evaluate(model, X_train, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sai số rmse:                    0.832\n",
      "Hệ số xác định r2-score: 0.870\n",
      "Tỉ lệ True positive:           0.410\n",
      "time: 422 ms\n"
     ]
    }
   ],
   "source": [
    "# On test\n",
    "test_result_df = evaluate(model, X_test, y_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">Nhận xét:\n",
    "    - Đây là mô hình tốt nhất với kỹ thuật thêm biến mới \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save best model \n",
    "- Lưu mô hình tốt nhất để triển khai API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 47 ms\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "model.save('improved_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x244d556fe20>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 63 ms\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "new_model = load_model('improved_model.hdf5', custom_objects={'my_r2_score': my_r2_score})\n",
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sai số rmse:                    0.832\n",
      "Hệ số xác định r2-score: 0.870\n",
      "Tỉ lệ True positive:           0.410\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id$Year</th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>speed</th>\n",
       "      <th>Time</th>\n",
       "      <th>KakuteiJyuni</th>\n",
       "      <th>top3</th>\n",
       "      <th>pred_speed</th>\n",
       "      <th>rank</th>\n",
       "      <th>top3_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015104603</td>\n",
       "      <td>59.259259</td>\n",
       "      <td>72.90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58.376282</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015104342</td>\n",
       "      <td>58.695652</td>\n",
       "      <td>73.60</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>58.323719</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015101022</td>\n",
       "      <td>58.064516</td>\n",
       "      <td>74.40</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>58.159218</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015105549</td>\n",
       "      <td>59.016393</td>\n",
       "      <td>73.20</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>58.005974</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015103961</td>\n",
       "      <td>58.935880</td>\n",
       "      <td>73.30</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>57.844486</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19140</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2011104098</td>\n",
       "      <td>57.345133</td>\n",
       "      <td>113.00</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>56.821014</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19141</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2014101156</td>\n",
       "      <td>57.882983</td>\n",
       "      <td>111.95</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>56.752712</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19142</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2013105705</td>\n",
       "      <td>55.670103</td>\n",
       "      <td>116.40</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>56.709106</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19143</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2011106130</td>\n",
       "      <td>57.754011</td>\n",
       "      <td>112.20</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>56.639183</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19144</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2012105645</td>\n",
       "      <td>56.842105</td>\n",
       "      <td>114.00</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>56.462006</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19145 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id$Year  race_id    KettoNum      speed    Time  KakuteiJyuni  top3  \\\n",
       "0         2018    34535  2015104603  59.259259   72.90             1     1   \n",
       "1         2018    34535  2015104342  58.695652   73.60             7     0   \n",
       "2         2018    34535  2015101022  58.064516   74.40            10     0   \n",
       "3         2018    34535  2015105549  59.016393   73.20             3     1   \n",
       "4         2018    34535  2015103961  58.935880   73.30             4     0   \n",
       "...        ...      ...         ...        ...     ...           ...   ...   \n",
       "19140     2018    35925  2011104098  57.345133  113.00             9     0   \n",
       "19141     2018    35925  2014101156  57.882983  111.95             4     0   \n",
       "19142     2018    35925  2013105705  55.670103  116.40            15     0   \n",
       "19143     2018    35925  2011106130  57.754011  112.20             6     0   \n",
       "19144     2018    35925  2012105645  56.842105  114.00            14     0   \n",
       "\n",
       "       pred_speed  rank  top3_pred  \n",
       "0       58.376282     1          1  \n",
       "1       58.323719     2          1  \n",
       "2       58.159218     3          1  \n",
       "3       58.005974     4          0  \n",
       "4       57.844486     5          0  \n",
       "...           ...   ...        ...  \n",
       "19140   56.821014    11          0  \n",
       "19141   56.752712    12          0  \n",
       "19142   56.709106    13          0  \n",
       "19143   56.639183    14          0  \n",
       "19144   56.462006    15          0  \n",
       "\n",
       "[19145 rows x 10 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 516 ms\n"
     ]
    }
   ],
   "source": [
    "# On test\n",
    "test = evaluate(new_model, X_test, y_test_df)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
