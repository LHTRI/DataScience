{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cải tiến mô hình với các biến mới\n",
    "- Các biến mới được thêm vào mô hình là:\n",
    "    - converted_ChokyosiCode\n",
    "    - converted_BanusiCode\n",
    "    - converted_UM_BreederCode\n",
    "    - Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import subprocess\n",
    "from glob import glob\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import scipy.stats as ss\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import keras\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.initializers import RandomNormal, Constant\n",
    "from tensorflow.keras.metrics import Metric\n",
    "import tensorflow.keras.optimizers as Optimizer\n",
    "import tensorflow_addons as tfa \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "import os.path as path\n",
    "lib_path =  path.abspath(path.join('' ,\"../../api/common\"))\n",
    "sys.path.insert(1, lib_path)\n",
    "from transform_split_data import transform_split_data\n",
    "from predict import predict, evaluate\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext autotime\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Khởi tạo phương thức giải phóng bộ nhớ gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "time: 750 ms\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "- Load dữ liệu theo các option "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>id$Year</th>\n",
       "      <th>ChokyosiCode</th>\n",
       "      <th>BanusiCode</th>\n",
       "      <th>UM_BreederCode</th>\n",
       "      <th>Odds</th>\n",
       "      <th>Kyori</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_0</th>\n",
       "      <th>...</th>\n",
       "      <th>KS_Syotai_川崎</th>\n",
       "      <th>KS_Syotai_笠松</th>\n",
       "      <th>id$JyoCD_1</th>\n",
       "      <th>KS_ChokyosiCode_365.0</th>\n",
       "      <th>CH_Syotai_川崎</th>\n",
       "      <th>top2_ChokyosiCode</th>\n",
       "      <th>top2_BanusiCode</th>\n",
       "      <th>top2_UM_BreederCode</th>\n",
       "      <th>before_Odds</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12135</td>\n",
       "      <td>2009100729</td>\n",
       "      <td>2011</td>\n",
       "      <td>1010</td>\n",
       "      <td>949030</td>\n",
       "      <td>600016</td>\n",
       "      <td>222</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.352506</td>\n",
       "      <td>-0.921203</td>\n",
       "      <td>0.090456</td>\n",
       "      <td>0.243086</td>\n",
       "      <td>59.833795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26439</td>\n",
       "      <td>2011103176</td>\n",
       "      <td>2015</td>\n",
       "      <td>1095</td>\n",
       "      <td>477030</td>\n",
       "      <td>100046</td>\n",
       "      <td>35</td>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.254953</td>\n",
       "      <td>-0.661138</td>\n",
       "      <td>-0.651838</td>\n",
       "      <td>-0.672843</td>\n",
       "      <td>58.230257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33661</td>\n",
       "      <td>2013100779</td>\n",
       "      <td>2017</td>\n",
       "      <td>1123</td>\n",
       "      <td>78006</td>\n",
       "      <td>703397</td>\n",
       "      <td>148</td>\n",
       "      <td>2.348416</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.206765</td>\n",
       "      <td>-0.442809</td>\n",
       "      <td>-1.241063</td>\n",
       "      <td>-0.613398</td>\n",
       "      <td>57.920792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28482</td>\n",
       "      <td>2011105992</td>\n",
       "      <td>2016</td>\n",
       "      <td>1129</td>\n",
       "      <td>547800</td>\n",
       "      <td>610012</td>\n",
       "      <td>52</td>\n",
       "      <td>1.378788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.453573</td>\n",
       "      <td>-0.510523</td>\n",
       "      <td>-0.974169</td>\n",
       "      <td>-0.578217</td>\n",
       "      <td>60.876249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>291</td>\n",
       "      <td>2004104307</td>\n",
       "      <td>2008</td>\n",
       "      <td>331</td>\n",
       "      <td>142006</td>\n",
       "      <td>701079</td>\n",
       "      <td>933</td>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.477666</td>\n",
       "      <td>-1.938882</td>\n",
       "      <td>-0.988132</td>\n",
       "      <td>0.072652</td>\n",
       "      <td>57.627119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475191</th>\n",
       "      <td>18201</td>\n",
       "      <td>2005106482</td>\n",
       "      <td>2013</td>\n",
       "      <td>1013</td>\n",
       "      <td>789006</td>\n",
       "      <td>400018</td>\n",
       "      <td>911</td>\n",
       "      <td>3.027155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232084</td>\n",
       "      <td>-0.467961</td>\n",
       "      <td>0.392597</td>\n",
       "      <td>3.096416</td>\n",
       "      <td>49.966265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475192</th>\n",
       "      <td>7309</td>\n",
       "      <td>2006102916</td>\n",
       "      <td>2010</td>\n",
       "      <td>1076</td>\n",
       "      <td>310007</td>\n",
       "      <td>393126</td>\n",
       "      <td>369</td>\n",
       "      <td>-1.530097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.380029</td>\n",
       "      <td>0.211269</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.553954</td>\n",
       "      <td>60.301508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475193</th>\n",
       "      <td>16882</td>\n",
       "      <td>2007104657</td>\n",
       "      <td>2012</td>\n",
       "      <td>1102</td>\n",
       "      <td>103006</td>\n",
       "      <td>330314</td>\n",
       "      <td>63</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.541780</td>\n",
       "      <td>-1.376219</td>\n",
       "      <td>-0.335163</td>\n",
       "      <td>-0.299192</td>\n",
       "      <td>62.113587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475194</th>\n",
       "      <td>8963</td>\n",
       "      <td>2007104503</td>\n",
       "      <td>2010</td>\n",
       "      <td>221</td>\n",
       "      <td>100006</td>\n",
       "      <td>310390</td>\n",
       "      <td>109</td>\n",
       "      <td>-1.530097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.098465</td>\n",
       "      <td>-0.945524</td>\n",
       "      <td>-1.037796</td>\n",
       "      <td>-0.678908</td>\n",
       "      <td>60.050042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475195</th>\n",
       "      <td>32570</td>\n",
       "      <td>2014103689</td>\n",
       "      <td>2017</td>\n",
       "      <td>436</td>\n",
       "      <td>129008</td>\n",
       "      <td>510045</td>\n",
       "      <td>717</td>\n",
       "      <td>-0.802876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.745284</td>\n",
       "      <td>0.159340</td>\n",
       "      <td>-0.418325</td>\n",
       "      <td>-0.477525</td>\n",
       "      <td>57.849197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>475196 rows × 212 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        race_id    KettoNum  id$Year  ChokyosiCode  BanusiCode  \\\n",
       "0         12135  2009100729     2011          1010      949030   \n",
       "1         26439  2011103176     2015          1095      477030   \n",
       "2         33661  2013100779     2017          1123       78006   \n",
       "3         28482  2011105992     2016          1129      547800   \n",
       "4           291  2004104307     2008           331      142006   \n",
       "...         ...         ...      ...           ...         ...   \n",
       "475191    18201  2005106482     2013          1013      789006   \n",
       "475192     7309  2006102916     2010          1076      310007   \n",
       "475193    16882  2007104657     2012          1102      103006   \n",
       "475194     8963  2007104503     2010           221      100006   \n",
       "475195    32570  2014103689     2017           436      129008   \n",
       "\n",
       "        UM_BreederCode  Odds     Kyori  TenkoBaba$DirtBabaCD_0  \\\n",
       "0               600016   222 -1.045283                     1.0   \n",
       "1               100046    35  0.166752                     0.0   \n",
       "2               703397   148  2.348416                     1.0   \n",
       "3               610012    52  1.378788                     1.0   \n",
       "4               701079   933  0.166752                     0.0   \n",
       "...                ...   ...       ...                     ...   \n",
       "475191          400018   911  3.027155                     0.0   \n",
       "475192          393126   369 -1.530097                     0.0   \n",
       "475193          330314    63 -1.045283                     1.0   \n",
       "475194          310390   109 -1.530097                     0.0   \n",
       "475195          510045   717 -0.802876                     0.0   \n",
       "\n",
       "        TenkoBaba$SibaBabaCD_0  ...  KS_Syotai_川崎　　　　　　　　  \\\n",
       "0                          0.0  ...                   0.0   \n",
       "1                          1.0  ...                   0.0   \n",
       "2                          0.0  ...                   0.0   \n",
       "3                          0.0  ...                   0.0   \n",
       "4                          1.0  ...                   0.0   \n",
       "...                        ...  ...                   ...   \n",
       "475191                     0.0  ...                   0.0   \n",
       "475192                     1.0  ...                   0.0   \n",
       "475193                     0.0  ...                   0.0   \n",
       "475194                     1.0  ...                   0.0   \n",
       "475195                     1.0  ...                   0.0   \n",
       "\n",
       "        KS_Syotai_笠松　　　　　　　　  id$JyoCD_1  KS_ChokyosiCode_365.0  \\\n",
       "0                        0.0         0.0                    0.0   \n",
       "1                        0.0         0.0                    0.0   \n",
       "2                        0.0         0.0                    0.0   \n",
       "3                        0.0         0.0                    0.0   \n",
       "4                        0.0         0.0                    0.0   \n",
       "...                      ...         ...                    ...   \n",
       "475191                   0.0         0.0                    0.0   \n",
       "475192                   0.0         0.0                    0.0   \n",
       "475193                   0.0         0.0                    0.0   \n",
       "475194                   0.0         0.0                    0.0   \n",
       "475195                   0.0         0.0                    0.0   \n",
       "\n",
       "        CH_Syotai_川崎　　　　　　　　  top2_ChokyosiCode  top2_BanusiCode  \\\n",
       "0                        0.0          -1.352506        -0.921203   \n",
       "1                        0.0           0.254953        -0.661138   \n",
       "2                        0.0          -0.206765        -0.442809   \n",
       "3                        0.0           0.453573        -0.510523   \n",
       "4                        0.0          -1.477666        -1.938882   \n",
       "...                      ...                ...              ...   \n",
       "475191                   0.0           0.232084        -0.467961   \n",
       "475192                   0.0          -0.380029         0.211269   \n",
       "475193                   0.0           0.541780        -1.376219   \n",
       "475194                   0.0          -1.098465        -0.945524   \n",
       "475195                   0.0          -0.745284         0.159340   \n",
       "\n",
       "        top2_UM_BreederCode  before_Odds      speed  \n",
       "0                  0.090456     0.243086  59.833795  \n",
       "1                 -0.651838    -0.672843  58.230257  \n",
       "2                 -1.241063    -0.613398  57.920792  \n",
       "3                 -0.974169    -0.578217  60.876249  \n",
       "4                 -0.988132     0.072652  57.627119  \n",
       "...                     ...          ...        ...  \n",
       "475191             0.392597     3.096416  49.966265  \n",
       "475192             1.335489    -0.553954  60.301508  \n",
       "475193            -0.335163    -0.299192  62.113587  \n",
       "475194            -1.037796    -0.678908  60.050042  \n",
       "475195            -0.418325    -0.477525  57.849197  \n",
       "\n",
       "[475196 rows x 212 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.52 s\n"
     ]
    }
   ],
   "source": [
    "# Load dữ liệu\n",
    "train_data = pd.read_csv('train_data_all.csv')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>id$Year</th>\n",
       "      <th>Kyori</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_1</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_1</th>\n",
       "      <th>id$RaceNum</th>\n",
       "      <th>TrackCD_52</th>\n",
       "      <th>...</th>\n",
       "      <th>KS_Syotai_川崎</th>\n",
       "      <th>KS_Syotai_笠松</th>\n",
       "      <th>id$JyoCD_1</th>\n",
       "      <th>KS_ChokyosiCode_365.0</th>\n",
       "      <th>CH_Syotai_川崎</th>\n",
       "      <th>top2_ChokyosiCode</th>\n",
       "      <th>top2_BanusiCode</th>\n",
       "      <th>top2_UM_BreederCode</th>\n",
       "      <th>before_Odds</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015101022</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.398731</td>\n",
       "      <td>-0.216783</td>\n",
       "      <td>0.174543</td>\n",
       "      <td>-0.638874</td>\n",
       "      <td>58.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015103483</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.415822</td>\n",
       "      <td>-0.318228</td>\n",
       "      <td>-1.717806</td>\n",
       "      <td>4.068149</td>\n",
       "      <td>57.908847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015106010</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.351113</td>\n",
       "      <td>-2.011561</td>\n",
       "      <td>-0.232294</td>\n",
       "      <td>-0.547888</td>\n",
       "      <td>59.178082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015102342</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.812923</td>\n",
       "      <td>-0.653485</td>\n",
       "      <td>-1.837928</td>\n",
       "      <td>0.188494</td>\n",
       "      <td>58.775510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015102323</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.469803</td>\n",
       "      <td>-0.723257</td>\n",
       "      <td>-0.230315</td>\n",
       "      <td>-0.456902</td>\n",
       "      <td>57.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19140</th>\n",
       "      <td>35925</td>\n",
       "      <td>2014105425</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.252109</td>\n",
       "      <td>1.811460</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.671629</td>\n",
       "      <td>58.378378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19141</th>\n",
       "      <td>35925</td>\n",
       "      <td>2014105543</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.897933</td>\n",
       "      <td>2.023608</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.698319</td>\n",
       "      <td>57.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19142</th>\n",
       "      <td>35925</td>\n",
       "      <td>2011106130</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.608380</td>\n",
       "      <td>0.873101</td>\n",
       "      <td>-1.450211</td>\n",
       "      <td>1.291248</td>\n",
       "      <td>57.754011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19143</th>\n",
       "      <td>35925</td>\n",
       "      <td>2012102418</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.897933</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.435675</td>\n",
       "      <td>-0.701958</td>\n",
       "      <td>57.497782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19144</th>\n",
       "      <td>35925</td>\n",
       "      <td>2013104045</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.291467</td>\n",
       "      <td>0.204303</td>\n",
       "      <td>-0.451692</td>\n",
       "      <td>-0.032299</td>\n",
       "      <td>57.243816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19145 rows × 208 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       race_id    KettoNum  id$Year     Kyori  TenkoBaba$DirtBabaCD_0  \\\n",
       "0        34535  2015101022     2018 -1.045283                     0.0   \n",
       "1        34535  2015103483     2018 -1.045283                     0.0   \n",
       "2        34535  2015106010     2018 -1.045283                     0.0   \n",
       "3        34535  2015102342     2018 -1.045283                     0.0   \n",
       "4        34535  2015102323     2018 -1.045283                     0.0   \n",
       "...        ...         ...      ...       ...                     ...   \n",
       "19140    35925  2014105425     2018  0.409159                     0.0   \n",
       "19141    35925  2014105543     2018  0.409159                     0.0   \n",
       "19142    35925  2011106130     2018  0.409159                     0.0   \n",
       "19143    35925  2012102418     2018  0.409159                     0.0   \n",
       "19144    35925  2013104045     2018  0.409159                     0.0   \n",
       "\n",
       "       TenkoBaba$SibaBabaCD_0  TenkoBaba$SibaBabaCD_1  TenkoBaba$DirtBabaCD_1  \\\n",
       "0                         1.0                     0.0                     1.0   \n",
       "1                         1.0                     0.0                     1.0   \n",
       "2                         1.0                     0.0                     1.0   \n",
       "3                         1.0                     0.0                     1.0   \n",
       "4                         1.0                     0.0                     1.0   \n",
       "...                       ...                     ...                     ...   \n",
       "19140                     1.0                     0.0                     1.0   \n",
       "19141                     1.0                     0.0                     1.0   \n",
       "19142                     1.0                     0.0                     1.0   \n",
       "19143                     1.0                     0.0                     1.0   \n",
       "19144                     1.0                     0.0                     1.0   \n",
       "\n",
       "       id$RaceNum  TrackCD_52  ...  KS_Syotai_川崎　　　　　　　　  \\\n",
       "0       -1.550314         0.0  ...                   0.0   \n",
       "1       -1.550314         0.0  ...                   0.0   \n",
       "2       -1.550314         0.0  ...                   0.0   \n",
       "3       -1.550314         0.0  ...                   0.0   \n",
       "4       -1.550314         0.0  ...                   0.0   \n",
       "...           ...         ...  ...                   ...   \n",
       "19140    1.664316         0.0  ...                   0.0   \n",
       "19141    1.664316         0.0  ...                   0.0   \n",
       "19142    1.664316         0.0  ...                   0.0   \n",
       "19143    1.664316         0.0  ...                   0.0   \n",
       "19144    1.664316         0.0  ...                   0.0   \n",
       "\n",
       "       KS_Syotai_笠松　　　　　　　　  id$JyoCD_1  KS_ChokyosiCode_365.0  \\\n",
       "0                       0.0         0.0                    0.0   \n",
       "1                       0.0         0.0                    0.0   \n",
       "2                       0.0         0.0                    0.0   \n",
       "3                       0.0         0.0                    0.0   \n",
       "4                       0.0         0.0                    0.0   \n",
       "...                     ...         ...                    ...   \n",
       "19140                   0.0         0.0                    0.0   \n",
       "19141                   0.0         0.0                    0.0   \n",
       "19142                   0.0         0.0                    0.0   \n",
       "19143                   0.0         0.0                    0.0   \n",
       "19144                   0.0         0.0                    0.0   \n",
       "\n",
       "       CH_Syotai_川崎　　　　　　　　  top2_ChokyosiCode  top2_BanusiCode  \\\n",
       "0                       0.0           0.398731        -0.216783   \n",
       "1                       0.0          -1.415822        -0.318228   \n",
       "2                       0.0          -1.351113        -2.011561   \n",
       "3                       0.0          -0.812923        -0.653485   \n",
       "4                       0.0          -1.469803        -0.723257   \n",
       "...                     ...                ...              ...   \n",
       "19140                   0.0           0.252109         1.811460   \n",
       "19141                   0.0           1.897933         2.023608   \n",
       "19142                   0.0          -0.608380         0.873101   \n",
       "19143                   0.0           1.897933         0.945055   \n",
       "19144                   0.0           0.291467         0.204303   \n",
       "\n",
       "       top2_UM_BreederCode  before_Odds      speed  \n",
       "0                 0.174543    -0.638874  58.064516  \n",
       "1                -1.717806     4.068149  57.908847  \n",
       "2                -0.232294    -0.547888  59.178082  \n",
       "3                -1.837928     0.188494  58.775510  \n",
       "4                -0.230315    -0.456902  57.142857  \n",
       "...                    ...          ...        ...  \n",
       "19140             1.335489    -0.671629  58.378378  \n",
       "19141             1.335489    -0.698319  57.857143  \n",
       "19142            -1.450211     1.291248  57.754011  \n",
       "19143             0.435675    -0.701958  57.497782  \n",
       "19144            -0.451692    -0.032299  57.243816  \n",
       "\n",
       "[19145 rows x 208 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 406 ms\n"
     ]
    }
   ],
   "source": [
    "# Load dữ liệu\n",
    "test_data = pd.read_csv('test_data_all.csv')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id$Year</th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>speed</th>\n",
       "      <th>Time</th>\n",
       "      <th>KakuteiJyuni</th>\n",
       "      <th>top3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011</td>\n",
       "      <td>12135</td>\n",
       "      <td>2009100729</td>\n",
       "      <td>59.833795</td>\n",
       "      <td>72.20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>26439</td>\n",
       "      <td>2011103176</td>\n",
       "      <td>58.230257</td>\n",
       "      <td>105.10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>33661</td>\n",
       "      <td>2013100779</td>\n",
       "      <td>57.920792</td>\n",
       "      <td>161.60</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>28482</td>\n",
       "      <td>2011105992</td>\n",
       "      <td>60.876249</td>\n",
       "      <td>130.10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008</td>\n",
       "      <td>291</td>\n",
       "      <td>2004104307</td>\n",
       "      <td>57.627119</td>\n",
       "      <td>106.20</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475191</th>\n",
       "      <td>2013</td>\n",
       "      <td>18201</td>\n",
       "      <td>2005106482</td>\n",
       "      <td>49.966265</td>\n",
       "      <td>207.50</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475192</th>\n",
       "      <td>2010</td>\n",
       "      <td>7309</td>\n",
       "      <td>2006102916</td>\n",
       "      <td>60.301508</td>\n",
       "      <td>59.70</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475193</th>\n",
       "      <td>2012</td>\n",
       "      <td>16882</td>\n",
       "      <td>2007104657</td>\n",
       "      <td>62.113587</td>\n",
       "      <td>69.55</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475194</th>\n",
       "      <td>2010</td>\n",
       "      <td>8963</td>\n",
       "      <td>2007104503</td>\n",
       "      <td>60.050042</td>\n",
       "      <td>59.95</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475195</th>\n",
       "      <td>2017</td>\n",
       "      <td>32570</td>\n",
       "      <td>2014103689</td>\n",
       "      <td>57.849197</td>\n",
       "      <td>80.90</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>475196 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id$Year  race_id    KettoNum      speed    Time  KakuteiJyuni  top3\n",
       "0          2011    12135  2009100729  59.833795   72.20             1     1\n",
       "1          2015    26439  2011103176  58.230257  105.10             1     1\n",
       "2          2017    33661  2013100779  57.920792  161.60            11     0\n",
       "3          2016    28482  2011105992  60.876249  130.10             2     1\n",
       "4          2008      291  2004104307  57.627119  106.20             5     0\n",
       "...         ...      ...         ...        ...     ...           ...   ...\n",
       "475191     2013    18201  2005106482  49.966265  207.50            13     0\n",
       "475192     2010     7309  2006102916  60.301508   59.70             8     0\n",
       "475193     2012    16882  2007104657  62.113587   69.55             6     0\n",
       "475194     2010     8963  2007104503  60.050042   59.95             5     0\n",
       "475195     2017    32570  2014103689  57.849197   80.90             8     0\n",
       "\n",
       "[475196 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 297 ms\n"
     ]
    }
   ],
   "source": [
    "y_train_df = pd.read_csv('y_train_df_all.csv')\n",
    "y_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id$Year</th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>speed</th>\n",
       "      <th>Time</th>\n",
       "      <th>KakuteiJyuni</th>\n",
       "      <th>top3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015101022</td>\n",
       "      <td>58.064516</td>\n",
       "      <td>74.4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015103483</td>\n",
       "      <td>57.908847</td>\n",
       "      <td>74.6</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015106010</td>\n",
       "      <td>59.178082</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015102342</td>\n",
       "      <td>58.775510</td>\n",
       "      <td>73.5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015102323</td>\n",
       "      <td>57.142857</td>\n",
       "      <td>75.6</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19140</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2014105425</td>\n",
       "      <td>58.378378</td>\n",
       "      <td>111.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19141</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2014105543</td>\n",
       "      <td>57.857143</td>\n",
       "      <td>112.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19142</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2011106130</td>\n",
       "      <td>57.754011</td>\n",
       "      <td>112.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19143</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2012102418</td>\n",
       "      <td>57.497782</td>\n",
       "      <td>112.7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19144</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2013104045</td>\n",
       "      <td>57.243816</td>\n",
       "      <td>113.2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19145 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id$Year  race_id    KettoNum      speed   Time  KakuteiJyuni  top3\n",
       "0         2018    34535  2015101022  58.064516   74.4            10     0\n",
       "1         2018    34535  2015103483  57.908847   74.6            11     0\n",
       "2         2018    34535  2015106010  59.178082   73.0             2     1\n",
       "3         2018    34535  2015102342  58.775510   73.5             6     0\n",
       "4         2018    34535  2015102323  57.142857   75.6            16     0\n",
       "...        ...      ...         ...        ...    ...           ...   ...\n",
       "19140     2018    35925  2014105425  58.378378  111.0             1     1\n",
       "19141     2018    35925  2014105543  57.857143  112.0             5     0\n",
       "19142     2018    35925  2011106130  57.754011  112.2             6     0\n",
       "19143     2018    35925  2012102418  57.497782  112.7             8     0\n",
       "19144     2018    35925  2013104045  57.243816  113.2            12     0\n",
       "\n",
       "[19145 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 94 ms\n"
     ]
    }
   ],
   "source": [
    "y_test_df = pd.read_csv('y_test_df_all.csv')\n",
    "y_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create X, y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kyori</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_1</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_1</th>\n",
       "      <th>id$RaceNum</th>\n",
       "      <th>TrackCD_52</th>\n",
       "      <th>TrackCD_17</th>\n",
       "      <th>JyokenInfo$SyubetuCD_18</th>\n",
       "      <th>GradeCD_</th>\n",
       "      <th>...</th>\n",
       "      <th>KS_Syotai_フランス</th>\n",
       "      <th>KS_Syotai_川崎</th>\n",
       "      <th>KS_Syotai_笠松</th>\n",
       "      <th>id$JyoCD_1</th>\n",
       "      <th>KS_ChokyosiCode_365.0</th>\n",
       "      <th>CH_Syotai_川崎</th>\n",
       "      <th>top2_ChokyosiCode</th>\n",
       "      <th>top2_BanusiCode</th>\n",
       "      <th>top2_UM_BreederCode</th>\n",
       "      <th>before_Odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.352506</td>\n",
       "      <td>-0.921203</td>\n",
       "      <td>0.090456</td>\n",
       "      <td>0.243086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.254953</td>\n",
       "      <td>-0.661138</td>\n",
       "      <td>-0.651838</td>\n",
       "      <td>-0.672843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.348416</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.203121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.206765</td>\n",
       "      <td>-0.442809</td>\n",
       "      <td>-1.241063</td>\n",
       "      <td>-0.613398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.378788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.079838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.453573</td>\n",
       "      <td>-0.510523</td>\n",
       "      <td>-0.974169</td>\n",
       "      <td>-0.578217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.477666</td>\n",
       "      <td>-1.938882</td>\n",
       "      <td>-0.988132</td>\n",
       "      <td>0.072652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475191</th>\n",
       "      <td>3.027155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.673597</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232084</td>\n",
       "      <td>-0.467961</td>\n",
       "      <td>0.392597</td>\n",
       "      <td>3.096416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475192</th>\n",
       "      <td>-1.530097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.965836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.380029</td>\n",
       "      <td>0.211269</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.553954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475193</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.541780</td>\n",
       "      <td>-1.376219</td>\n",
       "      <td>-0.335163</td>\n",
       "      <td>-0.299192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475194</th>\n",
       "      <td>-1.530097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.258075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.098465</td>\n",
       "      <td>-0.945524</td>\n",
       "      <td>-1.037796</td>\n",
       "      <td>-0.678908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475195</th>\n",
       "      <td>-0.802876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.258075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.745284</td>\n",
       "      <td>0.159340</td>\n",
       "      <td>-0.418325</td>\n",
       "      <td>-0.477525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>475196 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Kyori  TenkoBaba$DirtBabaCD_0  TenkoBaba$SibaBabaCD_0  \\\n",
       "0      -1.045283                     1.0                     0.0   \n",
       "1       0.166752                     0.0                     1.0   \n",
       "2       2.348416                     1.0                     0.0   \n",
       "3       1.378788                     1.0                     0.0   \n",
       "4       0.166752                     0.0                     1.0   \n",
       "...          ...                     ...                     ...   \n",
       "475191  3.027155                     0.0                     0.0   \n",
       "475192 -1.530097                     0.0                     1.0   \n",
       "475193 -1.045283                     1.0                     0.0   \n",
       "475194 -1.530097                     0.0                     1.0   \n",
       "475195 -0.802876                     0.0                     1.0   \n",
       "\n",
       "        TenkoBaba$SibaBabaCD_1  TenkoBaba$DirtBabaCD_1  id$RaceNum  \\\n",
       "0                          0.0                     0.0   -1.550314   \n",
       "1                          0.0                     0.0    0.495360   \n",
       "2                          1.0                     0.0    0.203121   \n",
       "3                          1.0                     0.0    1.079838   \n",
       "4                          0.0                     0.0    0.495360   \n",
       "...                        ...                     ...         ...   \n",
       "475191                     0.0                     0.0   -0.673597   \n",
       "475192                     0.0                     0.0   -0.965836   \n",
       "475193                     1.0                     0.0    0.495360   \n",
       "475194                     0.0                     1.0   -1.258075   \n",
       "475195                     0.0                     1.0   -1.258075   \n",
       "\n",
       "        TrackCD_52  TrackCD_17  JyokenInfo$SyubetuCD_18  GradeCD_   ...  \\\n",
       "0              0.0         1.0                      0.0        1.0  ...   \n",
       "1              0.0         0.0                      0.0        1.0  ...   \n",
       "2              0.0         0.0                      0.0        1.0  ...   \n",
       "3              0.0         0.0                      0.0        0.0  ...   \n",
       "4              0.0         0.0                      0.0        1.0  ...   \n",
       "...            ...         ...                      ...        ...  ...   \n",
       "475191         1.0         0.0                      0.0        1.0  ...   \n",
       "475192         0.0         0.0                      0.0        1.0  ...   \n",
       "475193         0.0         1.0                      0.0        1.0  ...   \n",
       "475194         0.0         0.0                      0.0        1.0  ...   \n",
       "475195         0.0         0.0                      0.0        1.0  ...   \n",
       "\n",
       "        KS_Syotai_フランス　　　　　　  KS_Syotai_川崎　　　　　　　　  KS_Syotai_笠松　　　　　　　　  \\\n",
       "0                        0.0                   0.0                   0.0   \n",
       "1                        0.0                   0.0                   0.0   \n",
       "2                        0.0                   0.0                   0.0   \n",
       "3                        0.0                   0.0                   0.0   \n",
       "4                        0.0                   0.0                   0.0   \n",
       "...                      ...                   ...                   ...   \n",
       "475191                   0.0                   0.0                   0.0   \n",
       "475192                   0.0                   0.0                   0.0   \n",
       "475193                   0.0                   0.0                   0.0   \n",
       "475194                   0.0                   0.0                   0.0   \n",
       "475195                   0.0                   0.0                   0.0   \n",
       "\n",
       "        id$JyoCD_1  KS_ChokyosiCode_365.0  CH_Syotai_川崎　　　　　　　　  \\\n",
       "0              0.0                    0.0                   0.0   \n",
       "1              0.0                    0.0                   0.0   \n",
       "2              0.0                    0.0                   0.0   \n",
       "3              0.0                    0.0                   0.0   \n",
       "4              0.0                    0.0                   0.0   \n",
       "...            ...                    ...                   ...   \n",
       "475191         0.0                    0.0                   0.0   \n",
       "475192         0.0                    0.0                   0.0   \n",
       "475193         0.0                    0.0                   0.0   \n",
       "475194         0.0                    0.0                   0.0   \n",
       "475195         0.0                    0.0                   0.0   \n",
       "\n",
       "        top2_ChokyosiCode  top2_BanusiCode  top2_UM_BreederCode  before_Odds  \n",
       "0               -1.352506        -0.921203             0.090456     0.243086  \n",
       "1                0.254953        -0.661138            -0.651838    -0.672843  \n",
       "2               -0.206765        -0.442809            -1.241063    -0.613398  \n",
       "3                0.453573        -0.510523            -0.974169    -0.578217  \n",
       "4               -1.477666        -1.938882            -0.988132     0.072652  \n",
       "...                   ...              ...                  ...          ...  \n",
       "475191           0.232084        -0.467961             0.392597     3.096416  \n",
       "475192          -0.380029         0.211269             1.335489    -0.553954  \n",
       "475193           0.541780        -1.376219            -0.335163    -0.299192  \n",
       "475194          -1.098465        -0.945524            -1.037796    -0.678908  \n",
       "475195          -0.745284         0.159340            -0.418325    -0.477525  \n",
       "\n",
       "[475196 rows x 204 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 235 ms\n"
     ]
    }
   ],
   "source": [
    "ref_col = ['ChokyosiCode', 'BanusiCode', 'UM_BreederCode', 'Odds']\n",
    "drop_columns = ['race_id', 'KettoNum', 'id$Year', 'speed'] + ref_col\n",
    "X_train = train_data.drop(drop_columns, axis=1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    475196.000000\n",
       "mean         58.393319\n",
       "std           2.308153\n",
       "min          21.973550\n",
       "25%          56.942004\n",
       "50%          58.536585\n",
       "75%          59.916782\n",
       "max          66.666667\n",
       "Name: speed, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 32 ms\n"
     ]
    }
   ],
   "source": [
    "y_train = train_data['speed']\n",
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 375 ms\n"
     ]
    }
   ],
   "source": [
    "y_train.hist(bins=100);\n",
    "plt.xlabel('speed');\n",
    "plt.ylabel('frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kyori</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_1</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_1</th>\n",
       "      <th>id$RaceNum</th>\n",
       "      <th>TrackCD_52</th>\n",
       "      <th>TrackCD_17</th>\n",
       "      <th>JyokenInfo$SyubetuCD_18</th>\n",
       "      <th>GradeCD_</th>\n",
       "      <th>...</th>\n",
       "      <th>KS_Syotai_フランス</th>\n",
       "      <th>KS_Syotai_川崎</th>\n",
       "      <th>KS_Syotai_笠松</th>\n",
       "      <th>id$JyoCD_1</th>\n",
       "      <th>KS_ChokyosiCode_365.0</th>\n",
       "      <th>CH_Syotai_川崎</th>\n",
       "      <th>top2_ChokyosiCode</th>\n",
       "      <th>top2_BanusiCode</th>\n",
       "      <th>top2_UM_BreederCode</th>\n",
       "      <th>before_Odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.398731</td>\n",
       "      <td>-0.216783</td>\n",
       "      <td>0.174543</td>\n",
       "      <td>-0.638874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.415822</td>\n",
       "      <td>-0.318228</td>\n",
       "      <td>-1.717806</td>\n",
       "      <td>4.068149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.351113</td>\n",
       "      <td>-2.011561</td>\n",
       "      <td>-0.232294</td>\n",
       "      <td>-0.547888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.812923</td>\n",
       "      <td>-0.653485</td>\n",
       "      <td>-1.837928</td>\n",
       "      <td>0.188494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.469803</td>\n",
       "      <td>-0.723257</td>\n",
       "      <td>-0.230315</td>\n",
       "      <td>-0.456902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19140</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.252109</td>\n",
       "      <td>1.811460</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.671629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19141</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.897933</td>\n",
       "      <td>2.023608</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.698319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19142</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.608380</td>\n",
       "      <td>0.873101</td>\n",
       "      <td>-1.450211</td>\n",
       "      <td>1.291248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19143</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.897933</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.435675</td>\n",
       "      <td>-0.701958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19144</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.291467</td>\n",
       "      <td>0.204303</td>\n",
       "      <td>-0.451692</td>\n",
       "      <td>-0.032299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19145 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Kyori  TenkoBaba$DirtBabaCD_0  TenkoBaba$SibaBabaCD_0  \\\n",
       "0     -1.045283                     0.0                     1.0   \n",
       "1     -1.045283                     0.0                     1.0   \n",
       "2     -1.045283                     0.0                     1.0   \n",
       "3     -1.045283                     0.0                     1.0   \n",
       "4     -1.045283                     0.0                     1.0   \n",
       "...         ...                     ...                     ...   \n",
       "19140  0.409159                     0.0                     1.0   \n",
       "19141  0.409159                     0.0                     1.0   \n",
       "19142  0.409159                     0.0                     1.0   \n",
       "19143  0.409159                     0.0                     1.0   \n",
       "19144  0.409159                     0.0                     1.0   \n",
       "\n",
       "       TenkoBaba$SibaBabaCD_1  TenkoBaba$DirtBabaCD_1  id$RaceNum  TrackCD_52  \\\n",
       "0                         0.0                     1.0   -1.550314         0.0   \n",
       "1                         0.0                     1.0   -1.550314         0.0   \n",
       "2                         0.0                     1.0   -1.550314         0.0   \n",
       "3                         0.0                     1.0   -1.550314         0.0   \n",
       "4                         0.0                     1.0   -1.550314         0.0   \n",
       "...                       ...                     ...         ...         ...   \n",
       "19140                     0.0                     1.0    1.664316         0.0   \n",
       "19141                     0.0                     1.0    1.664316         0.0   \n",
       "19142                     0.0                     1.0    1.664316         0.0   \n",
       "19143                     0.0                     1.0    1.664316         0.0   \n",
       "19144                     0.0                     1.0    1.664316         0.0   \n",
       "\n",
       "       TrackCD_17  JyokenInfo$SyubetuCD_18  GradeCD_   ...  \\\n",
       "0             0.0                      0.0        1.0  ...   \n",
       "1             0.0                      0.0        1.0  ...   \n",
       "2             0.0                      0.0        1.0  ...   \n",
       "3             0.0                      0.0        1.0  ...   \n",
       "4             0.0                      0.0        1.0  ...   \n",
       "...           ...                      ...        ...  ...   \n",
       "19140         0.0                      0.0        1.0  ...   \n",
       "19141         0.0                      0.0        1.0  ...   \n",
       "19142         0.0                      0.0        1.0  ...   \n",
       "19143         0.0                      0.0        1.0  ...   \n",
       "19144         0.0                      0.0        1.0  ...   \n",
       "\n",
       "       KS_Syotai_フランス　　　　　　  KS_Syotai_川崎　　　　　　　　  KS_Syotai_笠松　　　　　　　　  \\\n",
       "0                       0.0                   0.0                   0.0   \n",
       "1                       0.0                   0.0                   0.0   \n",
       "2                       0.0                   0.0                   0.0   \n",
       "3                       0.0                   0.0                   0.0   \n",
       "4                       0.0                   0.0                   0.0   \n",
       "...                     ...                   ...                   ...   \n",
       "19140                   0.0                   0.0                   0.0   \n",
       "19141                   0.0                   0.0                   0.0   \n",
       "19142                   0.0                   0.0                   0.0   \n",
       "19143                   0.0                   0.0                   0.0   \n",
       "19144                   0.0                   0.0                   0.0   \n",
       "\n",
       "       id$JyoCD_1  KS_ChokyosiCode_365.0  CH_Syotai_川崎　　　　　　　　  \\\n",
       "0             0.0                    0.0                   0.0   \n",
       "1             0.0                    0.0                   0.0   \n",
       "2             0.0                    0.0                   0.0   \n",
       "3             0.0                    0.0                   0.0   \n",
       "4             0.0                    0.0                   0.0   \n",
       "...           ...                    ...                   ...   \n",
       "19140         0.0                    0.0                   0.0   \n",
       "19141         0.0                    0.0                   0.0   \n",
       "19142         0.0                    0.0                   0.0   \n",
       "19143         0.0                    0.0                   0.0   \n",
       "19144         0.0                    0.0                   0.0   \n",
       "\n",
       "       top2_ChokyosiCode  top2_BanusiCode  top2_UM_BreederCode  before_Odds  \n",
       "0               0.398731        -0.216783             0.174543    -0.638874  \n",
       "1              -1.415822        -0.318228            -1.717806     4.068149  \n",
       "2              -1.351113        -2.011561            -0.232294    -0.547888  \n",
       "3              -0.812923        -0.653485            -1.837928     0.188494  \n",
       "4              -1.469803        -0.723257            -0.230315    -0.456902  \n",
       "...                  ...              ...                  ...          ...  \n",
       "19140           0.252109         1.811460             1.335489    -0.671629  \n",
       "19141           1.897933         2.023608             1.335489    -0.698319  \n",
       "19142          -0.608380         0.873101            -1.450211     1.291248  \n",
       "19143           1.897933         0.945055             0.435675    -0.701958  \n",
       "19144           0.291467         0.204303            -0.451692    -0.032299  \n",
       "\n",
       "[19145 rows x 204 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 32 ms\n"
     ]
    }
   ],
   "source": [
    "drop_columns = ['race_id', 'KettoNum', 'id$Year', 'speed']\n",
    "X_test = test_data.drop(drop_columns, axis=1)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    19145.000000\n",
       "mean        58.013408\n",
       "std          2.305421\n",
       "min         38.876890\n",
       "25%         56.509695\n",
       "50%         58.142665\n",
       "75%         59.558824\n",
       "max         65.573770\n",
       "Name: speed, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "y_test = test_data['speed']\n",
    "y_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 281 ms\n"
     ]
    }
   ],
   "source": [
    "y_test.hist(bins=100);\n",
    "plt.xlabel('speed');\n",
    "plt.ylabel('frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train with ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create and compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7_eizWaqi_7",
    "outputId": "4361457c-e661-4e75-8c78-3c1a7789b705"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def my_r2_score(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true) ) ) \n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "def BatchNorm():\n",
    "    return BatchNormalization(\n",
    "                momentum=0.95, \n",
    "                epsilon=0.005,\n",
    "                beta_initializer=RandomNormal(mean=0.0, stddev=0.05), \n",
    "                gamma_initializer=Constant(value=0.9)\n",
    "                )\n",
    "\n",
    "#model.add(BatchNorm())\n",
    "#model.add(Dropout(0.07))\n",
    "#model.add(Dense(units=num_2, activation='relu'))\n",
    "#model.add(BatchNorm())     \n",
    "\n",
    "def build_and_compile_model(X_train, num_units=100, activation='sigmoid'):\n",
    "    input_shape = X_train.shape[1] \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=num_units, activation=activation, kernel_initializer='he_normal',\n",
    "                    input_shape=(input_shape,)))\n",
    "    model.add(Dense(1))\n",
    "      \n",
    "    model.compile(loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError(), my_r2_score], optimizer=Optimizer.Adam(0.01)) #my_r2_score\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setup callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6x6dteutin0",
    "outputId": "0b4eab09-31e5-4ef0-d9cd-105656a99abe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "class My_checkoint(Callback):\n",
    "        \n",
    "    def __init__(self, model, X_test, y_test, checkpoint_name):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.mode = model\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        cpn = checkpoint_name + format(epoch, '02d') + '-.hdf5'\n",
    "        #cpn = os.path.join(checkpoint_dir, 'model'+format(epoch, '02d') + '-.hdf5')\n",
    "        val_loss = self.mode.evaluate(self.X_test, self.y_test)\n",
    "        print('my_val_loss', val_loss)\n",
    "        self.mode.save(cpn)\n",
    "        \n",
    "def callback_model(model, checkpoint_name, logdir, X_test, y_test):\n",
    "  \n",
    "    _logdir = os.path.join(logdir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_callback = TensorBoard(_logdir, histogram_freq=1)\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                  factor=0.3,\n",
    "                                  patience=1,\n",
    "                                  mode='min',\n",
    "                                  verbose=1)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=7,\n",
    "                                   monitor='val_loss',\n",
    "                                   mode='min',\n",
    "                                   verbose=1)\n",
    "    \n",
    "    csv_logger = CSVLogger('log.log', separator=',', append=False)\n",
    "    \n",
    "    callbacks_list = [tensorboard_callback, reduce_lr, early_stopping, csv_logger, My_checkoint(model, X_test, y_test, checkpoint_name)]\n",
    "    \n",
    "    return callbacks_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xttwiHh4u0ES",
    "outputId": "31e678b8-9b64-4fb6-9154-525972b48aca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, checkpoint_name, logdir, batch_size=32, epochs=10, re_train=True):\n",
    "\n",
    "    if re_train:\n",
    "        # Clear old folder\n",
    "        %rmdir /q/s {logdir}\n",
    "        # Clear old file\n",
    "        path = checkpoint_name + '**'\n",
    "        all_path_files = glob(path)\n",
    "        for file in all_path_files:\n",
    "            os.remove(file)\n",
    "        # fit model\n",
    "        callbacks_list = callback_model(model, checkpoint_name, logdir, X_test, y_test)\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        #del model\n",
    "        \n",
    "    # Get best file by reading log file\n",
    "    df = pd.read_csv('log.log')\n",
    "    best_epoch = df.loc[df['val_loss']==df['val_loss'].min(), 'epoch'].values[0]\n",
    "    best_file = 'model-' + format(best_epoch, '02d') + '-.hdf5'\n",
    "    #best_file = os.path.join(checkpoint_dir, 'model'+format(best_epoch, '02d') + '-.hdf5')\n",
    "    model = load_model(best_file, custom_objects={'my_r2_score': my_r2_score})\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">Train với bộ thông số tốt nhất trước khi cải tiến:\n",
    "    - units: 275\n",
    "    - batch_size: 128\n",
    "    - activation: sigmoid\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "   1/3713 [..............................] - ETA: 0s - loss: 3407.7861 - root_mean_squared_error: 58.3762 - my_r2_score: -869.6402WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "   2/3713 [..............................] - ETA: 11:05 - loss: 3268.1196 - root_mean_squared_error: 57.1675 - my_r2_score: -772.7656WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.3568s). Check your callbacks.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.8465 - root_mean_squared_error: 0.9201 - my_r2_score: 0.7816\n",
      "my_val_loss [0.846534013748169, 0.9200728535652161, 0.7816009521484375]\n",
      "3713/3713 [==============================] - 12s 3ms/step - loss: 9.1707 - root_mean_squared_error: 3.0283 - my_r2_score: -0.8609 - val_loss: 0.8465 - val_root_mean_squared_error: 0.9201 - val_my_r2_score: 0.7816\n",
      "Epoch 2/15\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7373 - root_mean_squared_error: 0.8586 - my_r2_score: 0.8088\n",
      "my_val_loss [0.7372522354125977, 0.8586339354515076, 0.8087766170501709]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.6447 - root_mean_squared_error: 0.8029 - my_r2_score: 0.8783 - val_loss: 0.7373 - val_root_mean_squared_error: 0.8586 - val_my_r2_score: 0.8088\n",
      "Epoch 3/15\n",
      "3708/3713 [============================>.] - ETA: 0s - loss: 0.6251 - root_mean_squared_error: 0.7906 - my_r2_score: 0.8823\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7418 - root_mean_squared_error: 0.8613 - my_r2_score: 0.8053\n",
      "my_val_loss [0.7417726516723633, 0.8612622618675232, 0.8053088188171387]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.6253 - root_mean_squared_error: 0.7908 - my_r2_score: 0.8822 - val_loss: 0.7418 - val_root_mean_squared_error: 0.8613 - val_my_r2_score: 0.8053\n",
      "Epoch 4/15\n",
      "  1/150 [..............................] - ETA: 0s - loss: 1.1424 - root_mean_squared_error: 1.0688 - my_r2_score: 0.716524WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.6774 - root_mean_squared_error: 0.8230 - my_r2_score: 0.8245\n",
      "my_val_loss [0.677399218082428, 0.823042631149292, 0.8244916796684265]\n",
      "3713/3713 [==============================] - 11s 3ms/step - loss: 0.5720 - root_mean_squared_error: 0.7563 - my_r2_score: 0.8924 - val_loss: 0.6774 - val_root_mean_squared_error: 0.8230 - val_my_r2_score: 0.8245\n",
      "Epoch 5/15\n",
      "3697/3713 [============================>.] - ETA: 0s - loss: 0.5630 - root_mean_squared_error: 0.7503 - my_r2_score: 0.8941\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "  1/150 [..............................] - ETA: 0s - loss: 1.1209 - root_mean_squared_error: 1.0587 - my_r2_score: 0.7218WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7390 - root_mean_squared_error: 0.8597 - my_r2_score: 0.8068\n",
      "my_val_loss [0.7389982342720032, 0.8596500754356384, 0.8067997097969055]\n",
      "3713/3713 [==============================] - 10s 3ms/step - loss: 0.5628 - root_mean_squared_error: 0.7502 - my_r2_score: 0.8941 - val_loss: 0.7390 - val_root_mean_squared_error: 0.8597 - val_my_r2_score: 0.8068\n",
      "Epoch 6/15\n",
      "3702/3713 [============================>.] - ETA: 0s - loss: 0.5391 - root_mean_squared_error: 0.7343 - my_r2_score: 0.8986\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7437 - root_mean_squared_error: 0.8624 - my_r2_score: 0.8050\n",
      "my_val_loss [0.743665337562561, 0.8623603582382202, 0.8050245046615601]\n",
      "3713/3713 [==============================] - 10s 3ms/step - loss: 0.5390 - root_mean_squared_error: 0.7342 - my_r2_score: 0.8985 - val_loss: 0.7437 - val_root_mean_squared_error: 0.8624 - val_my_r2_score: 0.8050\n",
      "Epoch 7/15\n",
      "3706/3713 [============================>.] - ETA: 0s - loss: 0.5294 - root_mean_squared_error: 0.7276 - my_r2_score: 0.9004\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7234 - root_mean_squared_error: 0.8505 - my_r2_score: 0.8104\n",
      "my_val_loss [0.7233819961547852, 0.8505186438560486, 0.8104389309883118]\n",
      "3713/3713 [==============================] - 10s 3ms/step - loss: 0.5294 - root_mean_squared_error: 0.7276 - my_r2_score: 0.9004 - val_loss: 0.7234 - val_root_mean_squared_error: 0.8505 - val_my_r2_score: 0.8104\n",
      "Epoch 8/15\n",
      "3694/3713 [============================>.] - ETA: 0s - loss: 0.5259 - root_mean_squared_error: 0.7252 - my_r2_score: 0.9011\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7048 - root_mean_squared_error: 0.8395 - my_r2_score: 0.8158\n",
      "my_val_loss [0.7047942876815796, 0.839520275592804, 0.8157834410667419]\n",
      "3713/3713 [==============================] - 10s 3ms/step - loss: 0.5259 - root_mean_squared_error: 0.7252 - my_r2_score: 0.9011 - val_loss: 0.7048 - val_root_mean_squared_error: 0.8395 - val_my_r2_score: 0.8158\n",
      "Epoch 9/15\n",
      "3702/3713 [============================>.] - ETA: 0s - loss: 0.5251 - root_mean_squared_error: 0.7247 - my_r2_score: 0.9012\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7052 - root_mean_squared_error: 0.8398 - my_r2_score: 0.8156\n",
      "my_val_loss [0.705213189125061, 0.839769721031189, 0.8155540227890015]\n",
      "3713/3713 [==============================] - 10s 3ms/step - loss: 0.5248 - root_mean_squared_error: 0.7244 - my_r2_score: 0.9012 - val_loss: 0.7052 - val_root_mean_squared_error: 0.8398 - val_my_r2_score: 0.8156\n",
      "Epoch 10/15\n",
      "3694/3713 [============================>.] - ETA: 0s - loss: 0.5247 - root_mean_squared_error: 0.7244 - my_r2_score: 0.9012\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7109 - root_mean_squared_error: 0.8431 - my_r2_score: 0.8140\n",
      "my_val_loss [0.710860550403595, 0.8431254625320435, 0.8139658570289612]\n",
      "3713/3713 [==============================] - 10s 3ms/step - loss: 0.5245 - root_mean_squared_error: 0.7242 - my_r2_score: 0.9012 - val_loss: 0.7109 - val_root_mean_squared_error: 0.8431 - val_my_r2_score: 0.8140\n",
      "Epoch 11/15\n",
      "3709/3713 [============================>.] - ETA: 0s - loss: 0.5244 - root_mean_squared_error: 0.7241 - my_r2_score: 0.9012\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.560999509019894e-07.\n",
      "  1/150 [..............................] - ETA: 0s - loss: 1.1216 - root_mean_squared_error: 1.0591 - my_r2_score: 0.7216WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7097 - root_mean_squared_error: 0.8424 - my_r2_score: 0.8143\n",
      "my_val_loss [0.709717869758606, 0.8424475193023682, 0.8142989873886108]\n",
      "3713/3713 [==============================] - 10s 3ms/step - loss: 0.5244 - root_mean_squared_error: 0.7241 - my_r2_score: 0.9012 - val_loss: 0.7097 - val_root_mean_squared_error: 0.8424 - val_my_r2_score: 0.8143\n",
      "Epoch 00011: early stopping\n",
      "time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "checkpoint_name = 'model-'\n",
    "baseDir = os.path.abspath(os.getcwd())\n",
    "logs_name = 'training_logs'\n",
    "logdir = os.path.join(baseDir, logs_name)\n",
    "#checkpoint_dir = os.path.join(baseDir, checkpoint_name)\n",
    "model = build_and_compile_model(X_train, num_units=275, activation='sigmoid')\n",
    "\n",
    "model = train_model(model, X_train, y_train, X_test, y_test, checkpoint_name, logdir,\n",
    "                    batch_size=128, epochs=15, re_train=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sai số rmse:                    0.747\n",
      "Hệ số xác định r2-score: 0.895\n",
      "Tỉ lệ True positive:           0.424\n",
      "time: 28.5 s\n"
     ]
    }
   ],
   "source": [
    "# On train\n",
    "train_result_df = evaluate(model, X_train, y_train_df)\n",
    "#train_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sai số rmse:                    0.823\n",
      "Hệ số xác định r2-score: 0.873\n",
      "Tỉ lệ True positive:           0.412\n",
      "time: 406 ms\n"
     ]
    }
   ],
   "source": [
    "# On test\n",
    "test_result_df = evaluate(model, X_test, y_test_df)\n",
    "#test_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">Nhận xét:\n",
    "    Độ chính xác được cải thiện đáng kể   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>lr</th>\n",
       "      <th>my_r2_score</th>\n",
       "      <th>root_mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_my_r2_score</th>\n",
       "      <th>val_root_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9.170742</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>-0.860877</td>\n",
       "      <td>3.028323</td>\n",
       "      <td>0.846534</td>\n",
       "      <td>0.781601</td>\n",
       "      <td>0.920073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.644679</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.878321</td>\n",
       "      <td>0.802919</td>\n",
       "      <td>0.737252</td>\n",
       "      <td>0.808777</td>\n",
       "      <td>0.858634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.625326</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.882236</td>\n",
       "      <td>0.790776</td>\n",
       "      <td>0.741773</td>\n",
       "      <td>0.805309</td>\n",
       "      <td>0.861262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.571976</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.892418</td>\n",
       "      <td>0.756291</td>\n",
       "      <td>0.677399</td>\n",
       "      <td>0.824492</td>\n",
       "      <td>0.823043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.562790</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.894116</td>\n",
       "      <td>0.750193</td>\n",
       "      <td>0.738998</td>\n",
       "      <td>0.806800</td>\n",
       "      <td>0.859650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.539047</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.898544</td>\n",
       "      <td>0.734199</td>\n",
       "      <td>0.743665</td>\n",
       "      <td>0.805025</td>\n",
       "      <td>0.862360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.529364</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.900405</td>\n",
       "      <td>0.727574</td>\n",
       "      <td>0.723382</td>\n",
       "      <td>0.810439</td>\n",
       "      <td>0.850519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.525945</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.901080</td>\n",
       "      <td>0.725221</td>\n",
       "      <td>0.704794</td>\n",
       "      <td>0.815783</td>\n",
       "      <td>0.839520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.524813</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.901248</td>\n",
       "      <td>0.724440</td>\n",
       "      <td>0.705213</td>\n",
       "      <td>0.815554</td>\n",
       "      <td>0.839770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.524467</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.901234</td>\n",
       "      <td>0.724201</td>\n",
       "      <td>0.710861</td>\n",
       "      <td>0.813966</td>\n",
       "      <td>0.843125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.524366</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.901248</td>\n",
       "      <td>0.724131</td>\n",
       "      <td>0.709718</td>\n",
       "      <td>0.814299</td>\n",
       "      <td>0.842448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch      loss        lr  my_r2_score  root_mean_squared_error  val_loss  \\\n",
       "0       1  9.170742  0.010000    -0.860877                 3.028323  0.846534   \n",
       "1       2  0.644679  0.010000     0.878321                 0.802919  0.737252   \n",
       "2       3  0.625326  0.010000     0.882236                 0.790776  0.741773   \n",
       "3       4  0.571976  0.003000     0.892418                 0.756291  0.677399   \n",
       "4       5  0.562790  0.003000     0.894116                 0.750193  0.738998   \n",
       "5       6  0.539047  0.000900     0.898544                 0.734199  0.743665   \n",
       "6       7  0.529364  0.000270     0.900405                 0.727574  0.723382   \n",
       "7       8  0.525945  0.000081     0.901080                 0.725221  0.704794   \n",
       "8       9  0.524813  0.000024     0.901248                 0.724440  0.705213   \n",
       "9      10  0.524467  0.000007     0.901234                 0.724201  0.710861   \n",
       "10     11  0.524366  0.000002     0.901248                 0.724131  0.709718   \n",
       "\n",
       "    val_my_r2_score  val_root_mean_squared_error  \n",
       "0          0.781601                     0.920073  \n",
       "1          0.808777                     0.858634  \n",
       "2          0.805309                     0.861262  \n",
       "3          0.824492                     0.823043  \n",
       "4          0.806800                     0.859650  \n",
       "5          0.805025                     0.862360  \n",
       "6          0.810439                     0.850519  \n",
       "7          0.815783                     0.839520  \n",
       "8          0.815554                     0.839770  \n",
       "9          0.813966                     0.843125  \n",
       "10         0.814299                     0.842448  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "log_data = pd.read_csv('log.log')\n",
    "log_data['epoch'] = log_data['epoch'] + 1\n",
    "log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 250 ms\n"
     ]
    }
   ],
   "source": [
    "def plot_loss(log_data):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    #fig.add_subplot(1, 2, 1)\n",
    "    sns.lineplot(x='epoch', y='root_mean_squared_error', data=log_data, marker='s', label='train_loss');\n",
    "    sns.lineplot(x='epoch', y='val_root_mean_squared_error', data=log_data, marker='s', label='val_loss');\n",
    "    plt.xlabel('Epoch', size=13)\n",
    "    plt.xticks(np.arange(1, log_data['epoch'].max()+1, step=1))\n",
    "    plt.ylabel('Error [speed]', size=13)\n",
    "    plt.legend()\n",
    "    plt.title('Learning-curve', size=15, y=1.02)\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_loss(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 234 ms\n"
     ]
    }
   ],
   "source": [
    "def plot_r2(log_data):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    #fig.add_subplot(1, 2, 1)\n",
    "    sns.lineplot(x='epoch', y='my_r2_score', data=log_data, marker='s', label='train_score');\n",
    "    sns.lineplot(x='epoch', y='val_my_r2_score', data=log_data, marker='s', label='val_score');\n",
    "    plt.xlabel('Epoch', size=13)\n",
    "    plt.xticks(np.arange(1, log_data['epoch'].max()+1, step=1))\n",
    "    plt.ylabel('rate', size=13)\n",
    "    plt.legend()\n",
    "    plt.title('Score-curve', size=15, y=1.02)\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_r2(log_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tune parameter\n",
    "- Điều chỉnh các siêu tham số bằng phương pháp BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 94 ms\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners.bayesian import BayesianOptimization\n",
    "from kerastuner import HyperModel\n",
    "\n",
    "class RegressionHyperModel(HyperModel):\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            Dense(\n",
    "                units=hp.Int('units', 100, 300, 5, default=100),\n",
    "                activation=hp.Choice(\n",
    "                    'activation',\n",
    "                    values=['sigmoid', 'tanh'],\n",
    "                    default='sigmoid'),\n",
    "                kernel_initializer='he_normal',\n",
    "                input_shape=self.input_shape\n",
    "            )\n",
    "        )\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "                                           optimizer=Optimizer.Adam(0.01)) \n",
    "        return model\n",
    "    \n",
    "class MyTuner(BayesianOptimization):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 16, 128, step=16)\n",
    "        #kwargs['epochs'] = trial.hyperparameters.Int('epochs', 10, 20, 5)\n",
    "        super(MyTuner, self).run_trial(trial, *args, **kwargs)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 45 Complete [00h 04m 53s]\n",
      "val_loss: 0.7114317417144775\n",
      "\n",
      "Best val_loss So Far: 0.685166209936142\n",
      "Total elapsed time: 04h 22m 41s\n",
      "\n",
      "Search: Running Trial #46\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "units             |285               |255               \n",
      "activation        |sigmoid           |sigmoid           \n",
      "batch_size        |48                |48                \n",
      "\n",
      "Epoch 1/15\n",
      "9900/9900 [==============================] - 25s 3ms/step - loss: 3.8479 - root_mean_squared_error: 1.9616 - val_loss: 1.3555 - val_root_mean_squared_error: 1.1643\n",
      "Epoch 2/15\n",
      "9900/9900 [==============================] - 24s 2ms/step - loss: 0.6722 - root_mean_squared_error: 0.8199 - val_loss: 0.7421 - val_root_mean_squared_error: 0.8615\n",
      "Epoch 3/15\n",
      "9900/9900 [==============================] - 27s 3ms/step - loss: 0.6532 - root_mean_squared_error: 0.8082 - val_loss: 0.7392 - val_root_mean_squared_error: 0.8597\n",
      "Epoch 4/15\n",
      "9900/9900 [==============================] - 29s 3ms/step - loss: 0.6394 - root_mean_squared_error: 0.7996 - val_loss: 0.7585 - val_root_mean_squared_error: 0.8709\n",
      "Epoch 5/15\n",
      "9900/9900 [==============================] - 28s 3ms/step - loss: 0.6364 - root_mean_squared_error: 0.7977 - val_loss: 0.8837 - val_root_mean_squared_error: 0.9401\n",
      "Epoch 6/15\n",
      "9900/9900 [==============================] - 24s 2ms/step - loss: 0.6277 - root_mean_squared_error: 0.7923 - val_loss: 0.7766 - val_root_mean_squared_error: 0.8812\n",
      "Epoch 1/15\n",
      "9900/9900 [==============================] - 25s 2ms/step - loss: 3.7911 - root_mean_squared_error: 1.9471 - val_loss: 0.7514 - val_root_mean_squared_error: 0.8668\n",
      "Epoch 2/15\n",
      "9900/9900 [==============================] - 24s 2ms/step - loss: 0.6695 - root_mean_squared_error: 0.8182 - val_loss: 0.7284 - val_root_mean_squared_error: 0.8534\n",
      "Epoch 3/15\n",
      "9900/9900 [==============================] - 24s 2ms/step - loss: 0.6553 - root_mean_squared_error: 0.8095 - val_loss: 0.6949 - val_root_mean_squared_error: 0.8336\n",
      "Epoch 4/15\n",
      "9900/9900 [==============================] - 24s 2ms/step - loss: 0.6425 - root_mean_squared_error: 0.8015 - val_loss: 0.7423 - val_root_mean_squared_error: 0.8616\n",
      "Epoch 5/15\n",
      "9900/9900 [==============================] - 25s 2ms/step - loss: 0.6354 - root_mean_squared_error: 0.7971 - val_loss: 0.9378 - val_root_mean_squared_error: 0.9684\n",
      "Epoch 6/15\n",
      "8950/9900 [==========================>...] - ETA: 2s - loss: 0.6223 - root_mean_squared_error: 0.7889"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-3ea0b4320b99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             )\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\kerastuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-92f5007237cb>\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m#kwargs['epochs'] = trial.hyperparameters.Int('epochs', 10, 20, 5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMyTuner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\kerastuner\\engine\\multi_execution_tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'callbacks'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'min'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\kerastuner\\engine\\tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[1;34m(self, trial, fit_args, fit_kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \"\"\"\n\u001b[0;32m    140\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4h 27min 47s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_shape = (X_train.shape[1], )\n",
    "hypermodel = RegressionHyperModel(input_shape)\n",
    "project_name = 'improve_search'\n",
    "\n",
    "tuner = MyTuner(\n",
    "            hypermodel,\n",
    "            objective='val_loss',\n",
    "            max_trials=50,\n",
    "            executions_per_trial=2,\n",
    "            seed=42,\n",
    "            project_name=project_name,\n",
    "            overwrite=False\n",
    "            )\n",
    "tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=15, callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=3)], verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các bộ tham số tốt nhất\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>activation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180</td>\n",
       "      <td>112</td>\n",
       "      <td>sigmoid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>145</td>\n",
       "      <td>96</td>\n",
       "      <td>sigmoid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>sigmoid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  batch_size activation\n",
       "0    180         112    sigmoid\n",
       "1    145          96    sigmoid\n",
       "2    190          80    sigmoid"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "list_best_hp = tuner.get_best_hyperparameters(num_trials=3)\n",
    "list_best_units = []\n",
    "list_best_batch_size = []\n",
    "list_best_activation = []\n",
    "\n",
    "for best_hp in list_best_hp:\n",
    "    best_units = best_hp.get('units')\n",
    "    list_best_units.append(best_units)\n",
    "    best_batch_size = best_hp.get('batch_size')\n",
    "    list_best_batch_size.append(best_batch_size)\n",
    "    best_activation = best_hp.get('activation')\n",
    "    list_best_activation.append(best_activation)\n",
    "\n",
    "hp_result = pd.DataFrame({'units':list_best_units,\n",
    "                            'batch_size': list_best_batch_size,\n",
    "                            'activation': list_best_activation\n",
    "                            })\n",
    "print('Các bộ tham số tốt nhất')\n",
    "hp_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">Phương pháp:\n",
    "    - Ta train lại 3 mô hình tốt nhất để đánh giá\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "   2/4243 [..............................] - ETA: 4:30:43 - loss: 3340.3420 - root_mean_squared_error: 57.7957 - my_r2_score: -615.6491WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 7.6564s). Check your callbacks.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7374 - root_mean_squared_error: 0.8587 - my_r2_score: 0.7954\n",
      "my_val_loss [0.7374114394187927, 0.8587266206741333, 0.7953792214393616]\n",
      "4243/4243 [==============================] - 20s 5ms/step - loss: 11.9850 - root_mean_squared_error: 3.4619 - my_r2_score: -1.2797 - val_loss: 0.7374 - val_root_mean_squared_error: 0.8587 - val_my_r2_score: 0.7954\n",
      "Epoch 2/15\n",
      "4234/4243 [============================>.] - ETA: 0s - loss: 0.6467 - root_mean_squared_error: 0.8042 - my_r2_score: 0.8778\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7409 - root_mean_squared_error: 0.8608 - my_r2_score: 0.7967\n",
      "my_val_loss [0.7409430742263794, 0.8607804775238037, 0.7966784238815308]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.6466 - root_mean_squared_error: 0.8041 - my_r2_score: 0.8778 - val_loss: 0.7409 - val_root_mean_squared_error: 0.8608 - val_my_r2_score: 0.7967\n",
      "Epoch 3/15\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7346 - root_mean_squared_error: 0.8571 - my_r2_score: 0.7957\n",
      "my_val_loss [0.7346359491348267, 0.8571090698242188, 0.7956712245941162]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5961 - root_mean_squared_error: 0.7721 - my_r2_score: 0.8874 - val_loss: 0.7346 - val_root_mean_squared_error: 0.8571 - val_my_r2_score: 0.7957\n",
      "Epoch 4/15\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.6880 - root_mean_squared_error: 0.8294 - my_r2_score: 0.8089\n",
      "my_val_loss [0.6879829168319702, 0.829447329044342, 0.8089497685432434]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5845 - root_mean_squared_error: 0.7645 - my_r2_score: 0.8897 - val_loss: 0.6880 - val_root_mean_squared_error: 0.8294 - val_my_r2_score: 0.8089\n",
      "Epoch 5/15\n",
      "4241/4243 [============================>.] - ETA: 0s - loss: 0.5761 - root_mean_squared_error: 0.7590 - my_r2_score: 0.8914\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7277 - root_mean_squared_error: 0.8530 - my_r2_score: 0.7957\n",
      "my_val_loss [0.7276540398597717, 0.8530263900756836, 0.7956914305686951]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5761 - root_mean_squared_error: 0.7590 - my_r2_score: 0.8914 - val_loss: 0.7277 - val_root_mean_squared_error: 0.8530 - val_my_r2_score: 0.7957\n",
      "Epoch 6/15\n",
      "4238/4243 [============================>.] - ETA: 0s - loss: 0.5523 - root_mean_squared_error: 0.7432 - my_r2_score: 0.8959\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7119 - root_mean_squared_error: 0.8437 - my_r2_score: 0.7998\n",
      "my_val_loss [0.7118668556213379, 0.8437220454216003, 0.7998181581497192]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5524 - root_mean_squared_error: 0.7432 - my_r2_score: 0.8959 - val_loss: 0.7119 - val_root_mean_squared_error: 0.8437 - val_my_r2_score: 0.7998\n",
      "Epoch 7/15\n",
      "4237/4243 [============================>.] - ETA: 0s - loss: 0.5428 - root_mean_squared_error: 0.7368 - my_r2_score: 0.8976\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7073 - root_mean_squared_error: 0.8410 - my_r2_score: 0.8017\n",
      "my_val_loss [0.7072784900665283, 0.8409985303878784, 0.8016982078552246]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5428 - root_mean_squared_error: 0.7367 - my_r2_score: 0.8976 - val_loss: 0.7073 - val_root_mean_squared_error: 0.8410 - val_my_r2_score: 0.8017\n",
      "Epoch 8/15\n",
      "4243/4243 [==============================] - ETA: 0s - loss: 0.5393 - root_mean_squared_error: 0.7344 - my_r2_score: 0.8984\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "  1/171 [..............................] - ETA: 0s - loss: 1.1603 - root_mean_squared_error: 1.0772 - my_r2_score: 0.7370WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7160 - root_mean_squared_error: 0.8461 - my_r2_score: 0.7991\n",
      "my_val_loss [0.7159593105316162, 0.8461437821388245, 0.7990587949752808]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5393 - root_mean_squared_error: 0.7344 - my_r2_score: 0.8984 - val_loss: 0.7160 - val_root_mean_squared_error: 0.8461 - val_my_r2_score: 0.7991\n",
      "Epoch 9/15\n",
      "4233/4243 [============================>.] - ETA: 0s - loss: 0.5383 - root_mean_squared_error: 0.7337 - my_r2_score: 0.8985\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7157 - root_mean_squared_error: 0.8460 - my_r2_score: 0.7991\n",
      "my_val_loss [0.7157140374183655, 0.8459988236427307, 0.7990699410438538]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5382 - root_mean_squared_error: 0.7336 - my_r2_score: 0.8985 - val_loss: 0.7157 - val_root_mean_squared_error: 0.8460 - val_my_r2_score: 0.7991\n",
      "Epoch 10/15\n",
      "4236/4243 [============================>.] - ETA: 0s - loss: 0.5378 - root_mean_squared_error: 0.7334 - my_r2_score: 0.8987\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7223 - root_mean_squared_error: 0.8499 - my_r2_score: 0.7971\n",
      "my_val_loss [0.7223249673843384, 0.849897027015686, 0.797084629535675]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5379 - root_mean_squared_error: 0.7334 - my_r2_score: 0.8986 - val_loss: 0.7223 - val_root_mean_squared_error: 0.8499 - val_my_r2_score: 0.7971\n",
      "Epoch 11/15\n",
      "4236/4243 [============================>.] - ETA: 0s - loss: 0.5378 - root_mean_squared_error: 0.7334 - my_r2_score: 0.8986\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.560999509019894e-07.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7183 - root_mean_squared_error: 0.8475 - my_r2_score: 0.7983\n",
      "my_val_loss [0.7183032035827637, 0.8475276827812195, 0.798301637172699]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5378 - root_mean_squared_error: 0.7333 - my_r2_score: 0.8986 - val_loss: 0.7183 - val_root_mean_squared_error: 0.8475 - val_my_r2_score: 0.7983\n",
      "Epoch 00011: early stopping\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.6880 - root_mean_squared_error: 0.8294 - my_r2_score: 0.5881\n",
      "Epoch 1/15\n",
      "   2/4950 [..............................] - ETA: 36:24 - loss: 3349.7966 - root_mean_squared_error: 57.8774 - my_r2_score: -749.9609WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.8791s). Check your callbacks.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7238 - root_mean_squared_error: 0.8508 - my_r2_score: 0.7816\n",
      "my_val_loss [0.7237879037857056, 0.8507572412490845, 0.7816153168678284]\n",
      "4950/4950 [==============================] - 15s 3ms/step - loss: 13.0689 - root_mean_squared_error: 3.6151 - my_r2_score: -1.6618 - val_loss: 0.7238 - val_root_mean_squared_error: 0.8508 - val_my_r2_score: 0.7816\n",
      "Epoch 2/15\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7078 - root_mean_squared_error: 0.8413 - my_r2_score: 0.7883\n",
      "my_val_loss [0.7078443169593811, 0.8413348197937012, 0.788343071937561]\n",
      "4950/4950 [==============================] - 14s 3ms/step - loss: 0.6476 - root_mean_squared_error: 0.8047 - my_r2_score: 0.8774 - val_loss: 0.7078 - val_root_mean_squared_error: 0.8413 - val_my_r2_score: 0.7883\n",
      "Epoch 3/15\n",
      "4943/4950 [============================>.] - ETA: 0s - loss: 0.6323 - root_mean_squared_error: 0.7952 - my_r2_score: 0.8801\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7427 - root_mean_squared_error: 0.8618 - my_r2_score: 0.7814\n",
      "my_val_loss [0.7426734566688538, 0.8617850542068481, 0.7814311385154724]\n",
      "4950/4950 [==============================] - 14s 3ms/step - loss: 0.6324 - root_mean_squared_error: 0.7952 - my_r2_score: 0.8801 - val_loss: 0.7427 - val_root_mean_squared_error: 0.8618 - val_my_r2_score: 0.7814\n",
      "Epoch 4/15\n",
      "4938/4950 [============================>.] - ETA: 0s - loss: 0.5860 - root_mean_squared_error: 0.7655 - my_r2_score: 0.8892\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "  1/200 [..............................] - ETA: 0s - loss: 1.4789 - root_mean_squared_error: 1.2161 - my_r2_score: 0.6207WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7728 - root_mean_squared_error: 0.8791 - my_r2_score: 0.7686\n",
      "my_val_loss [0.7727735638618469, 0.8790754079818726, 0.7685787081718445]\n",
      "4950/4950 [==============================] - 14s 3ms/step - loss: 0.5859 - root_mean_squared_error: 0.7654 - my_r2_score: 0.8892 - val_loss: 0.7728 - val_root_mean_squared_error: 0.8791 - val_my_r2_score: 0.7686\n",
      "Epoch 5/15\n",
      "4950/4950 [==============================] - ETA: 0s - loss: 0.5625 - root_mean_squared_error: 0.7500 - my_r2_score: 0.8937\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7272 - root_mean_squared_error: 0.8528 - my_r2_score: 0.7822\n",
      "my_val_loss [0.7272337675094604, 0.8527800440788269, 0.782224178314209]\n",
      "4950/4950 [==============================] - 14s 3ms/step - loss: 0.5625 - root_mean_squared_error: 0.7500 - my_r2_score: 0.8937 - val_loss: 0.7272 - val_root_mean_squared_error: 0.8528 - val_my_r2_score: 0.7822\n",
      "Epoch 6/15\n",
      "4942/4950 [============================>.] - ETA: 0s - loss: 0.5535 - root_mean_squared_error: 0.7440 - my_r2_score: 0.8953\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7119 - root_mean_squared_error: 0.8438 - my_r2_score: 0.7881\n",
      "my_val_loss [0.7119484543800354, 0.8437703847885132, 0.7880886793136597]\n",
      "4950/4950 [==============================] - 14s 3ms/step - loss: 0.5536 - root_mean_squared_error: 0.7440 - my_r2_score: 0.8953 - val_loss: 0.7119 - val_root_mean_squared_error: 0.8438 - val_my_r2_score: 0.7881\n",
      "Epoch 7/15\n",
      "4945/4950 [============================>.] - ETA: 0s - loss: 0.5508 - root_mean_squared_error: 0.7421 - my_r2_score: 0.8960\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7284 - root_mean_squared_error: 0.8535 - my_r2_score: 0.7822\n",
      "my_val_loss [0.7283854484558105, 0.8534550070762634, 0.7822086811065674]\n",
      "4950/4950 [==============================] - 13s 3ms/step - loss: 0.5507 - root_mean_squared_error: 0.7421 - my_r2_score: 0.8960 - val_loss: 0.7284 - val_root_mean_squared_error: 0.8535 - val_my_r2_score: 0.7822\n",
      "Epoch 8/15\n",
      "4931/4950 [============================>.] - ETA: 0s - loss: 0.5498 - root_mean_squared_error: 0.7415 - my_r2_score: 0.8960\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7181 - root_mean_squared_error: 0.8474 - my_r2_score: 0.7856\n",
      "my_val_loss [0.7181290984153748, 0.8474249839782715, 0.785630464553833]\n",
      "4950/4950 [==============================] - 13s 3ms/step - loss: 0.5497 - root_mean_squared_error: 0.7414 - my_r2_score: 0.8961 - val_loss: 0.7181 - val_root_mean_squared_error: 0.8474 - val_my_r2_score: 0.7856\n",
      "Epoch 9/15\n",
      "4948/4950 [============================>.] - ETA: 0s - loss: 0.5494 - root_mean_squared_error: 0.7412 - my_r2_score: 0.8962\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7183 - root_mean_squared_error: 0.8475 - my_r2_score: 0.7856\n",
      "my_val_loss [0.7182847261428833, 0.8475167751312256, 0.785618007183075]\n",
      "4950/4950 [==============================] - 13s 3ms/step - loss: 0.5494 - root_mean_squared_error: 0.7412 - my_r2_score: 0.8962 - val_loss: 0.7183 - val_root_mean_squared_error: 0.8475 - val_my_r2_score: 0.7856\n",
      "Epoch 00009: early stopping\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.7078 - root_mean_squared_error: 0.8413 - my_r2_score: 0.5890\n",
      "Epoch 1/15\n",
      "   2/5940 [..............................] - ETA: 51:13 - loss: 3307.3359 - root_mean_squared_error: 57.5094 - my_r2_score: -540.0782WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 1.0311s). Check your callbacks.\n",
      "  1/240 [..............................] - ETA: 0s - loss: 0.4959 - root_mean_squared_error: 0.7042 - my_r2_score: 0.8532617WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 0.8128 - root_mean_squared_error: 0.9016 - my_r2_score: 0.7381\n",
      "my_val_loss [0.8128467202186584, 0.9015800952911377, 0.7380561828613281]\n",
      "5940/5940 [==============================] - 17s 3ms/step - loss: 8.2044 - root_mean_squared_error: 2.8643 - my_r2_score: -0.5571 - val_loss: 0.8128 - val_root_mean_squared_error: 0.9016 - val_my_r2_score: 0.7381\n",
      "Epoch 2/15\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 0.7590 - root_mean_squared_error: 0.8712 - my_r2_score: 0.7580\n",
      "my_val_loss [0.7590498924255371, 0.8712347149848938, 0.7580031752586365]\n",
      "5940/5940 [==============================] - 16s 3ms/step - loss: 0.6539 - root_mean_squared_error: 0.8086 - my_r2_score: 0.8756 - val_loss: 0.7590 - val_root_mean_squared_error: 0.8712 - val_my_r2_score: 0.7580\n",
      "Epoch 3/15\n",
      "5932/5940 [============================>.] - ETA: 0s - loss: 0.6321 - root_mean_squared_error: 0.7951 - my_r2_score: 0.8799\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 0.7812 - root_mean_squared_error: 0.8839 - my_r2_score: 0.7470\n",
      "my_val_loss [0.781218945980072, 0.8838658928871155, 0.7470042705535889]\n",
      "5940/5940 [==============================] - 16s 3ms/step - loss: 0.6320 - root_mean_squared_error: 0.7950 - my_r2_score: 0.8800 - val_loss: 0.7812 - val_root_mean_squared_error: 0.8839 - val_my_r2_score: 0.7470\n",
      "Epoch 4/15\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 0.7289 - root_mean_squared_error: 0.8537 - my_r2_score: 0.7668\n",
      "my_val_loss [0.7288779616355896, 0.8537434935569763, 0.7667943239212036]\n",
      "5940/5940 [==============================] - 16s 3ms/step - loss: 0.5781 - root_mean_squared_error: 0.7603 - my_r2_score: 0.8903 - val_loss: 0.7289 - val_root_mean_squared_error: 0.8537 - val_my_r2_score: 0.7668\n",
      "Epoch 5/15\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 0.7005 - root_mean_squared_error: 0.8370 - my_r2_score: 0.7752\n",
      "my_val_loss [0.7005110383033752, 0.8369653820991516, 0.7752311825752258]\n",
      "5940/5940 [==============================] - 15s 3ms/step - loss: 0.5681 - root_mean_squared_error: 0.7538 - my_r2_score: 0.8922 - val_loss: 0.7005 - val_root_mean_squared_error: 0.8370 - val_my_r2_score: 0.7752\n",
      "Epoch 6/15\n",
      "5928/5940 [============================>.] - ETA: 0s - loss: 0.5614 - root_mean_squared_error: 0.7493 - my_r2_score: 0.8937\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 0.7220 - root_mean_squared_error: 0.8497 - my_r2_score: 0.7666\n",
      "my_val_loss [0.7220300436019897, 0.8497235178947449, 0.7665722370147705]\n",
      "5940/5940 [==============================] - 15s 3ms/step - loss: 0.5614 - root_mean_squared_error: 0.7493 - my_r2_score: 0.8936 - val_loss: 0.7220 - val_root_mean_squared_error: 0.8497 - val_my_r2_score: 0.7666\n",
      "Epoch 7/15\n",
      "5921/5940 [============================>.] - ETA: 0s - loss: 0.5373 - root_mean_squared_error: 0.7330 - my_r2_score: 0.8981\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.7134 - root_mean_squared_error: 0.8446 - my_r2_score: 0.7689\n",
      "my_val_loss [0.7133846282958984, 0.8446210026741028, 0.7688734531402588]\n",
      "5940/5940 [==============================] - 15s 3ms/step - loss: 0.5379 - root_mean_squared_error: 0.7334 - my_r2_score: 0.8981 - val_loss: 0.7134 - val_root_mean_squared_error: 0.8446 - val_my_r2_score: 0.7689\n",
      "Epoch 8/15\n",
      "5936/5940 [============================>.] - ETA: 0s - loss: 0.5287 - root_mean_squared_error: 0.7271 - my_r2_score: 0.8998\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "  1/240 [..............................] - ETA: 0s - loss: 0.5964 - root_mean_squared_error: 0.7723 - my_r2_score: 0.8234WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 0.7420 - root_mean_squared_error: 0.8614 - my_r2_score: 0.7600\n",
      "my_val_loss [0.7420170903205872, 0.8614041209220886, 0.7600077986717224]\n",
      "5940/5940 [==============================] - 16s 3ms/step - loss: 0.5287 - root_mean_squared_error: 0.7271 - my_r2_score: 0.8999 - val_loss: 0.7420 - val_root_mean_squared_error: 0.8614 - val_my_r2_score: 0.7600\n",
      "Epoch 9/15\n",
      "5922/5940 [============================>.] - ETA: 0s - loss: 0.5256 - root_mean_squared_error: 0.7250 - my_r2_score: 0.9004\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.7418 - root_mean_squared_error: 0.8613 - my_r2_score: 0.7600\n",
      "my_val_loss [0.741833508014679, 0.8612975478172302, 0.7599981427192688]\n",
      "5940/5940 [==============================] - 16s 3ms/step - loss: 0.5255 - root_mean_squared_error: 0.7249 - my_r2_score: 0.9004 - val_loss: 0.7418 - val_root_mean_squared_error: 0.8613 - val_my_r2_score: 0.7600\n",
      "Epoch 10/15\n",
      "5939/5940 [============================>.] - ETA: 0s - loss: 0.5244 - root_mean_squared_error: 0.7242 - my_r2_score: 0.9008\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 0.7308 - root_mean_squared_error: 0.8549 - my_r2_score: 0.7635\n",
      "my_val_loss [0.7307889461517334, 0.8548619747161865, 0.7634697556495667]\n",
      "5940/5940 [==============================] - 16s 3ms/step - loss: 0.5244 - root_mean_squared_error: 0.7242 - my_r2_score: 0.9008 - val_loss: 0.7308 - val_root_mean_squared_error: 0.8549 - val_my_r2_score: 0.7635\n",
      "Epoch 11/15\n",
      "5930/5940 [============================>.] - ETA: 0s - loss: 0.5240 - root_mean_squared_error: 0.7239 - my_r2_score: 0.9007\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.7306 - root_mean_squared_error: 0.8547 - my_r2_score: 0.7635\n",
      "my_val_loss [0.7305514812469482, 0.8547230362892151, 0.7635369896888733]\n",
      "5940/5940 [==============================] - 15s 3ms/step - loss: 0.5241 - root_mean_squared_error: 0.7239 - my_r2_score: 0.9007 - val_loss: 0.7306 - val_root_mean_squared_error: 0.8547 - val_my_r2_score: 0.7635\n",
      "Epoch 12/15\n",
      "5927/5940 [============================>.] - ETA: 0s - loss: 0.5240 - root_mean_squared_error: 0.7239 - my_r2_score: 0.9007\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.560999509019894e-07.\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 0.7291 - root_mean_squared_error: 0.8539 - my_r2_score: 0.7640\n",
      "my_val_loss [0.7291043400764465, 0.8538760542869568, 0.7640120983123779]\n",
      "5940/5940 [==============================] - 16s 3ms/step - loss: 0.5240 - root_mean_squared_error: 0.7239 - my_r2_score: 0.9006 - val_loss: 0.7291 - val_root_mean_squared_error: 0.8539 - val_my_r2_score: 0.7640\n",
      "Epoch 00012: early stopping\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.7005 - root_mean_squared_error: 0.8370 - my_r2_score: 0.5797\n",
      "time: 7min 45s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_val_loss = []\n",
    "for num_units, activation, batch_size in zip(list_best_units, list_best_activation, list_best_batch_size):\n",
    "    model = build_and_compile_model(X_train, num_units=num_units, activation=activation)\n",
    "    model = train_model(model, X_train, y_train, X_test, y_test, checkpoint_name, logdir,\n",
    "                        batch_size=batch_size, epochs=15, re_train=True)\n",
    "    val_loss, _, _ = model.evaluate(X_test, y_test)\n",
    "    list_val_loss.append(val_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>activation</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180</td>\n",
       "      <td>112</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.687983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>145</td>\n",
       "      <td>96</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.707844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.700511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  batch_size activation  val_loss\n",
       "0    180         112    sigmoid  0.687983\n",
       "1    145          96    sigmoid  0.707844\n",
       "2    190          80    sigmoid  0.700511"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "improved_result = pd.DataFrame({'units':list_best_units,\n",
    "                            'batch_size': list_best_batch_size,\n",
    "                            'activation': list_best_activation,\n",
    "                            'val_loss': list_val_loss})\n",
    "improved_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "# Save improve_result\n",
    "improved_result.to_csv('improved_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">Ta chọn bộ tham số tốt nhất:\n",
    "    - units: 180\n",
    "    - batch_size: 112\n",
    "    - activation: sigmoid\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "   2/4243 [..............................] - ETA: 29:01 - loss: 3324.7104 - root_mean_squared_error: 57.6603 - my_r2_score: -567.5795WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.8164s). Check your callbacks.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.6919 - root_mean_squared_error: 0.8318 - my_r2_score: 0.8086\n",
      "my_val_loss [0.6919029951095581, 0.8318070769309998, 0.8085955381393433]\n",
      "4243/4243 [==============================] - 14s 3ms/step - loss: 12.0107 - root_mean_squared_error: 3.4656 - my_r2_score: -1.3215 - val_loss: 0.6919 - val_root_mean_squared_error: 0.8318 - val_my_r2_score: 0.8086\n",
      "Epoch 2/15\n",
      "4231/4243 [============================>.] - ETA: 0s - loss: 0.6476 - root_mean_squared_error: 0.8047 - my_r2_score: 0.8776\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.6994 - root_mean_squared_error: 0.8363 - my_r2_score: 0.8070\n",
      "my_val_loss [0.6994108557701111, 0.8363078832626343, 0.8070346713066101]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.6475 - root_mean_squared_error: 0.8047 - my_r2_score: 0.8776 - val_loss: 0.6994 - val_root_mean_squared_error: 0.8363 - val_my_r2_score: 0.8070\n",
      "Epoch 3/15\n",
      "4229/4243 [============================>.] - ETA: 0s - loss: 0.5994 - root_mean_squared_error: 0.7742 - my_r2_score: 0.8869\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7522 - root_mean_squared_error: 0.8673 - my_r2_score: 0.7917\n",
      "my_val_loss [0.7522487044334412, 0.8673227429389954, 0.79169100522995]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5998 - root_mean_squared_error: 0.7745 - my_r2_score: 0.8868 - val_loss: 0.7522 - val_root_mean_squared_error: 0.8673 - val_my_r2_score: 0.7917\n",
      "Epoch 4/15\n",
      "4237/4243 [============================>.] - ETA: 0s - loss: 0.5769 - root_mean_squared_error: 0.7595 - my_r2_score: 0.8914\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.6966 - root_mean_squared_error: 0.8346 - my_r2_score: 0.8080\n",
      "my_val_loss [0.6966375112533569, 0.8346481323242188, 0.8079749345779419]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5768 - root_mean_squared_error: 0.7595 - my_r2_score: 0.8914 - val_loss: 0.6966 - val_root_mean_squared_error: 0.8346 - val_my_r2_score: 0.8080\n",
      "Epoch 5/15\n",
      "4225/4243 [============================>.] - ETA: 0s - loss: 0.5671 - root_mean_squared_error: 0.7530 - my_r2_score: 0.8931\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7046 - root_mean_squared_error: 0.8394 - my_r2_score: 0.8058\n",
      "my_val_loss [0.7046274542808533, 0.8394209146499634, 0.8058498501777649]\n",
      "4243/4243 [==============================] - 11s 3ms/step - loss: 0.5674 - root_mean_squared_error: 0.7532 - my_r2_score: 0.8931 - val_loss: 0.7046 - val_root_mean_squared_error: 0.8394 - val_my_r2_score: 0.8058\n",
      "Epoch 6/15\n",
      "4239/4243 [============================>.] - ETA: 0s - loss: 0.5643 - root_mean_squared_error: 0.7512 - my_r2_score: 0.8937\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7323 - root_mean_squared_error: 0.8557 - my_r2_score: 0.7972\n",
      "my_val_loss [0.7322805523872375, 0.8557339310646057, 0.7972412109375]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5642 - root_mean_squared_error: 0.7512 - my_r2_score: 0.8937 - val_loss: 0.7323 - val_root_mean_squared_error: 0.8557 - val_my_r2_score: 0.7972\n",
      "Epoch 7/15\n",
      "4237/4243 [============================>.] - ETA: 0s - loss: 0.5632 - root_mean_squared_error: 0.7504 - my_r2_score: 0.8937\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7124 - root_mean_squared_error: 0.8440 - my_r2_score: 0.8032\n",
      "my_val_loss [0.7124181389808655, 0.8440486788749695, 0.8032339215278625]\n",
      "4243/4243 [==============================] - 13s 3ms/step - loss: 0.5631 - root_mean_squared_error: 0.7504 - my_r2_score: 0.8937 - val_loss: 0.7124 - val_root_mean_squared_error: 0.8440 - val_my_r2_score: 0.8032\n",
      "Epoch 8/15\n",
      "4243/4243 [==============================] - ETA: 0s - loss: 0.5628 - root_mean_squared_error: 0.7502 - my_r2_score: 0.8941\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7143 - root_mean_squared_error: 0.8451 - my_r2_score: 0.8027\n",
      "my_val_loss [0.7142524719238281, 0.8451346158981323, 0.8026557564735413]\n",
      "4243/4243 [==============================] - 12s 3ms/step - loss: 0.5628 - root_mean_squared_error: 0.7502 - my_r2_score: 0.8941 - val_loss: 0.7143 - val_root_mean_squared_error: 0.8451 - val_my_r2_score: 0.8027\n",
      "Epoch 00008: early stopping\n",
      "time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "checkpoint_name = 'model-'\n",
    "baseDir = os.path.abspath(os.getcwd())\n",
    "logs_name = 'training_logs'\n",
    "logdir = os.path.join(baseDir, logs_name)\n",
    "\n",
    "model = build_and_compile_model(X_train, num_units=180, activation='sigmoid')\n",
    "model = train_model(model, X_train, y_train, X_test, y_test, checkpoint_name, logdir,\n",
    "                        batch_size=112, epochs=15, re_train=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sai số rmse:                    0.817\n",
      "Hệ số xác định r2-score: 0.875\n",
      "Tỉ lệ True positive:           0.407\n",
      "time: 27.1 s\n"
     ]
    }
   ],
   "source": [
    "# On train\n",
    "train_result_df = evaluate(model, X_train, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sai số rmse:                    0.832\n",
      "Hệ số xác định r2-score: 0.870\n",
      "Tỉ lệ True positive:           0.410\n",
      "time: 422 ms\n"
     ]
    }
   ],
   "source": [
    "# On test\n",
    "test_result_df = evaluate(model, X_test, y_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">Nhận xét:\n",
    "    - Đây là mô hình tốt nhất với kỹ thuật thêm biến mới \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save best model \n",
    "- Lưu mô hình tốt nhất để triển khai API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 47 ms\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "model.save('improved_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x244d556fe20>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 63 ms\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "new_model = load_model('improved_model.hdf5', custom_objects={'my_r2_score': my_r2_score})\n",
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sai số rmse:                    0.832\n",
      "Hệ số xác định r2-score: 0.870\n",
      "Tỉ lệ True positive:           0.410\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id$Year</th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>speed</th>\n",
       "      <th>Time</th>\n",
       "      <th>KakuteiJyuni</th>\n",
       "      <th>top3</th>\n",
       "      <th>pred_speed</th>\n",
       "      <th>rank</th>\n",
       "      <th>top3_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015104603</td>\n",
       "      <td>59.259259</td>\n",
       "      <td>72.90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58.376282</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015104342</td>\n",
       "      <td>58.695652</td>\n",
       "      <td>73.60</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>58.323719</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015101022</td>\n",
       "      <td>58.064516</td>\n",
       "      <td>74.40</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>58.159218</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015105549</td>\n",
       "      <td>59.016393</td>\n",
       "      <td>73.20</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>58.005974</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015103961</td>\n",
       "      <td>58.935880</td>\n",
       "      <td>73.30</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>57.844486</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19140</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2011104098</td>\n",
       "      <td>57.345133</td>\n",
       "      <td>113.00</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>56.821014</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19141</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2014101156</td>\n",
       "      <td>57.882983</td>\n",
       "      <td>111.95</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>56.752712</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19142</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2013105705</td>\n",
       "      <td>55.670103</td>\n",
       "      <td>116.40</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>56.709106</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19143</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2011106130</td>\n",
       "      <td>57.754011</td>\n",
       "      <td>112.20</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>56.639183</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19144</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2012105645</td>\n",
       "      <td>56.842105</td>\n",
       "      <td>114.00</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>56.462006</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19145 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id$Year  race_id    KettoNum      speed    Time  KakuteiJyuni  top3  \\\n",
       "0         2018    34535  2015104603  59.259259   72.90             1     1   \n",
       "1         2018    34535  2015104342  58.695652   73.60             7     0   \n",
       "2         2018    34535  2015101022  58.064516   74.40            10     0   \n",
       "3         2018    34535  2015105549  59.016393   73.20             3     1   \n",
       "4         2018    34535  2015103961  58.935880   73.30             4     0   \n",
       "...        ...      ...         ...        ...     ...           ...   ...   \n",
       "19140     2018    35925  2011104098  57.345133  113.00             9     0   \n",
       "19141     2018    35925  2014101156  57.882983  111.95             4     0   \n",
       "19142     2018    35925  2013105705  55.670103  116.40            15     0   \n",
       "19143     2018    35925  2011106130  57.754011  112.20             6     0   \n",
       "19144     2018    35925  2012105645  56.842105  114.00            14     0   \n",
       "\n",
       "       pred_speed  rank  top3_pred  \n",
       "0       58.376282     1          1  \n",
       "1       58.323719     2          1  \n",
       "2       58.159218     3          1  \n",
       "3       58.005974     4          0  \n",
       "4       57.844486     5          0  \n",
       "...           ...   ...        ...  \n",
       "19140   56.821014    11          0  \n",
       "19141   56.752712    12          0  \n",
       "19142   56.709106    13          0  \n",
       "19143   56.639183    14          0  \n",
       "19144   56.462006    15          0  \n",
       "\n",
       "[19145 rows x 10 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 516 ms\n"
     ]
    }
   ],
   "source": [
    "# On test\n",
    "test = evaluate(new_model, X_test, y_test_df)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
