{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cải tiến mô hình với dữ liệu được lọc theo cự ly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import subprocess\n",
    "from glob import glob\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import scipy.stats as ss\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import keras\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.initializers import RandomNormal, Constant\n",
    "from tensorflow.keras.metrics import Metric\n",
    "import tensorflow.keras.optimizers as Optimizer\n",
    "import tensorflow_addons as tfa \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "import os.path as path\n",
    "lib_path =  path.abspath(path.join('' ,\"../../api/common\"))\n",
    "sys.path.insert(1, lib_path)\n",
    "from transform_split_data import transform_split_data\n",
    "from predict import predict, evaluate\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext autotime\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Khởi tạo phương thức giải phóng bộ nhớ gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "time: 734 ms\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>id$Year</th>\n",
       "      <th>ChokyosiCode</th>\n",
       "      <th>BanusiCode</th>\n",
       "      <th>UM_BreederCode</th>\n",
       "      <th>Odds</th>\n",
       "      <th>Kyori</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_0</th>\n",
       "      <th>...</th>\n",
       "      <th>KS_Syotai_川崎</th>\n",
       "      <th>KS_Syotai_笠松</th>\n",
       "      <th>id$JyoCD_1</th>\n",
       "      <th>KS_ChokyosiCode_365.0</th>\n",
       "      <th>CH_Syotai_川崎</th>\n",
       "      <th>top2_ChokyosiCode</th>\n",
       "      <th>top2_BanusiCode</th>\n",
       "      <th>top2_UM_BreederCode</th>\n",
       "      <th>before_Odds</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12135</td>\n",
       "      <td>2009100729</td>\n",
       "      <td>2011</td>\n",
       "      <td>1010</td>\n",
       "      <td>949030</td>\n",
       "      <td>600016</td>\n",
       "      <td>222</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.352506</td>\n",
       "      <td>-0.921203</td>\n",
       "      <td>0.090456</td>\n",
       "      <td>0.243086</td>\n",
       "      <td>59.833795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26439</td>\n",
       "      <td>2011103176</td>\n",
       "      <td>2015</td>\n",
       "      <td>1095</td>\n",
       "      <td>477030</td>\n",
       "      <td>100046</td>\n",
       "      <td>35</td>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.254953</td>\n",
       "      <td>-0.661138</td>\n",
       "      <td>-0.651838</td>\n",
       "      <td>-0.672843</td>\n",
       "      <td>58.230257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33661</td>\n",
       "      <td>2013100779</td>\n",
       "      <td>2017</td>\n",
       "      <td>1123</td>\n",
       "      <td>78006</td>\n",
       "      <td>703397</td>\n",
       "      <td>148</td>\n",
       "      <td>2.348416</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.206765</td>\n",
       "      <td>-0.442809</td>\n",
       "      <td>-1.241063</td>\n",
       "      <td>-0.613398</td>\n",
       "      <td>57.920792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28482</td>\n",
       "      <td>2011105992</td>\n",
       "      <td>2016</td>\n",
       "      <td>1129</td>\n",
       "      <td>547800</td>\n",
       "      <td>610012</td>\n",
       "      <td>52</td>\n",
       "      <td>1.378788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.453573</td>\n",
       "      <td>-0.510523</td>\n",
       "      <td>-0.974169</td>\n",
       "      <td>-0.578217</td>\n",
       "      <td>60.876249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>291</td>\n",
       "      <td>2004104307</td>\n",
       "      <td>2008</td>\n",
       "      <td>331</td>\n",
       "      <td>142006</td>\n",
       "      <td>701079</td>\n",
       "      <td>933</td>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.477666</td>\n",
       "      <td>-1.938882</td>\n",
       "      <td>-0.988132</td>\n",
       "      <td>0.072652</td>\n",
       "      <td>57.627119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475191</th>\n",
       "      <td>18201</td>\n",
       "      <td>2005106482</td>\n",
       "      <td>2013</td>\n",
       "      <td>1013</td>\n",
       "      <td>789006</td>\n",
       "      <td>400018</td>\n",
       "      <td>911</td>\n",
       "      <td>3.027155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232084</td>\n",
       "      <td>-0.467961</td>\n",
       "      <td>0.392597</td>\n",
       "      <td>3.096416</td>\n",
       "      <td>49.966265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475192</th>\n",
       "      <td>7309</td>\n",
       "      <td>2006102916</td>\n",
       "      <td>2010</td>\n",
       "      <td>1076</td>\n",
       "      <td>310007</td>\n",
       "      <td>393126</td>\n",
       "      <td>369</td>\n",
       "      <td>-1.530097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.380029</td>\n",
       "      <td>0.211269</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.553954</td>\n",
       "      <td>60.301508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475193</th>\n",
       "      <td>16882</td>\n",
       "      <td>2007104657</td>\n",
       "      <td>2012</td>\n",
       "      <td>1102</td>\n",
       "      <td>103006</td>\n",
       "      <td>330314</td>\n",
       "      <td>63</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.541780</td>\n",
       "      <td>-1.376219</td>\n",
       "      <td>-0.335163</td>\n",
       "      <td>-0.299192</td>\n",
       "      <td>62.113587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475194</th>\n",
       "      <td>8963</td>\n",
       "      <td>2007104503</td>\n",
       "      <td>2010</td>\n",
       "      <td>221</td>\n",
       "      <td>100006</td>\n",
       "      <td>310390</td>\n",
       "      <td>109</td>\n",
       "      <td>-1.530097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.098465</td>\n",
       "      <td>-0.945524</td>\n",
       "      <td>-1.037796</td>\n",
       "      <td>-0.678908</td>\n",
       "      <td>60.050042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475195</th>\n",
       "      <td>32570</td>\n",
       "      <td>2014103689</td>\n",
       "      <td>2017</td>\n",
       "      <td>436</td>\n",
       "      <td>129008</td>\n",
       "      <td>510045</td>\n",
       "      <td>717</td>\n",
       "      <td>-0.802876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.745284</td>\n",
       "      <td>0.159340</td>\n",
       "      <td>-0.418325</td>\n",
       "      <td>-0.477525</td>\n",
       "      <td>57.849197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>475196 rows × 212 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        race_id    KettoNum  id$Year  ChokyosiCode  BanusiCode  \\\n",
       "0         12135  2009100729     2011          1010      949030   \n",
       "1         26439  2011103176     2015          1095      477030   \n",
       "2         33661  2013100779     2017          1123       78006   \n",
       "3         28482  2011105992     2016          1129      547800   \n",
       "4           291  2004104307     2008           331      142006   \n",
       "...         ...         ...      ...           ...         ...   \n",
       "475191    18201  2005106482     2013          1013      789006   \n",
       "475192     7309  2006102916     2010          1076      310007   \n",
       "475193    16882  2007104657     2012          1102      103006   \n",
       "475194     8963  2007104503     2010           221      100006   \n",
       "475195    32570  2014103689     2017           436      129008   \n",
       "\n",
       "        UM_BreederCode  Odds     Kyori  TenkoBaba$DirtBabaCD_0  \\\n",
       "0               600016   222 -1.045283                     1.0   \n",
       "1               100046    35  0.166752                     0.0   \n",
       "2               703397   148  2.348416                     1.0   \n",
       "3               610012    52  1.378788                     1.0   \n",
       "4               701079   933  0.166752                     0.0   \n",
       "...                ...   ...       ...                     ...   \n",
       "475191          400018   911  3.027155                     0.0   \n",
       "475192          393126   369 -1.530097                     0.0   \n",
       "475193          330314    63 -1.045283                     1.0   \n",
       "475194          310390   109 -1.530097                     0.0   \n",
       "475195          510045   717 -0.802876                     0.0   \n",
       "\n",
       "        TenkoBaba$SibaBabaCD_0  ...  KS_Syotai_川崎　　　　　　　　  \\\n",
       "0                          0.0  ...                   0.0   \n",
       "1                          1.0  ...                   0.0   \n",
       "2                          0.0  ...                   0.0   \n",
       "3                          0.0  ...                   0.0   \n",
       "4                          1.0  ...                   0.0   \n",
       "...                        ...  ...                   ...   \n",
       "475191                     0.0  ...                   0.0   \n",
       "475192                     1.0  ...                   0.0   \n",
       "475193                     0.0  ...                   0.0   \n",
       "475194                     1.0  ...                   0.0   \n",
       "475195                     1.0  ...                   0.0   \n",
       "\n",
       "        KS_Syotai_笠松　　　　　　　　  id$JyoCD_1  KS_ChokyosiCode_365.0  \\\n",
       "0                        0.0         0.0                    0.0   \n",
       "1                        0.0         0.0                    0.0   \n",
       "2                        0.0         0.0                    0.0   \n",
       "3                        0.0         0.0                    0.0   \n",
       "4                        0.0         0.0                    0.0   \n",
       "...                      ...         ...                    ...   \n",
       "475191                   0.0         0.0                    0.0   \n",
       "475192                   0.0         0.0                    0.0   \n",
       "475193                   0.0         0.0                    0.0   \n",
       "475194                   0.0         0.0                    0.0   \n",
       "475195                   0.0         0.0                    0.0   \n",
       "\n",
       "        CH_Syotai_川崎　　　　　　　　  top2_ChokyosiCode  top2_BanusiCode  \\\n",
       "0                        0.0          -1.352506        -0.921203   \n",
       "1                        0.0           0.254953        -0.661138   \n",
       "2                        0.0          -0.206765        -0.442809   \n",
       "3                        0.0           0.453573        -0.510523   \n",
       "4                        0.0          -1.477666        -1.938882   \n",
       "...                      ...                ...              ...   \n",
       "475191                   0.0           0.232084        -0.467961   \n",
       "475192                   0.0          -0.380029         0.211269   \n",
       "475193                   0.0           0.541780        -1.376219   \n",
       "475194                   0.0          -1.098465        -0.945524   \n",
       "475195                   0.0          -0.745284         0.159340   \n",
       "\n",
       "        top2_UM_BreederCode  before_Odds      speed  \n",
       "0                  0.090456     0.243086  59.833795  \n",
       "1                 -0.651838    -0.672843  58.230257  \n",
       "2                 -1.241063    -0.613398  57.920792  \n",
       "3                 -0.974169    -0.578217  60.876249  \n",
       "4                 -0.988132     0.072652  57.627119  \n",
       "...                     ...          ...        ...  \n",
       "475191             0.392597     3.096416  49.966265  \n",
       "475192             1.335489    -0.553954  60.301508  \n",
       "475193            -0.335163    -0.299192  62.113587  \n",
       "475194            -1.037796    -0.678908  60.050042  \n",
       "475195            -0.418325    -0.477525  57.849197  \n",
       "\n",
       "[475196 rows x 212 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.95 s\n"
     ]
    }
   ],
   "source": [
    "# Load dữ liệu\n",
    "train_data = pd.read_csv('train_data_all.csv')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lọc dữ liệu train theo cự ly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2005107051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2005102028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2005104156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2005105319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2005101044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452048</th>\n",
       "      <td>34534</td>\n",
       "      <td>2012101892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452049</th>\n",
       "      <td>34534</td>\n",
       "      <td>2013104774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452050</th>\n",
       "      <td>34534</td>\n",
       "      <td>2012101704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452051</th>\n",
       "      <td>34534</td>\n",
       "      <td>2012102404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452052</th>\n",
       "      <td>34534</td>\n",
       "      <td>2011103929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>452053 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        race_id    KettoNum\n",
       "0             1  2005107051\n",
       "1             1  2005102028\n",
       "2             1  2005104156\n",
       "3             1  2005105319\n",
       "4             1  2005101044\n",
       "...         ...         ...\n",
       "452048    34534  2012101892\n",
       "452049    34534  2013104774\n",
       "452050    34534  2012101704\n",
       "452051    34534  2012102404\n",
       "452052    34534  2011103929\n",
       "\n",
       "[452053 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 110 ms\n"
     ]
    }
   ],
   "source": [
    "filter_Kyori_id = pd.read_csv('filter_Kyori_id.csv')\n",
    "filter_Kyori_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>id$Year</th>\n",
       "      <th>ChokyosiCode</th>\n",
       "      <th>BanusiCode</th>\n",
       "      <th>UM_BreederCode</th>\n",
       "      <th>Odds</th>\n",
       "      <th>Kyori</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_0</th>\n",
       "      <th>...</th>\n",
       "      <th>KS_Syotai_笠松</th>\n",
       "      <th>id$JyoCD_1</th>\n",
       "      <th>KS_ChokyosiCode_365.0</th>\n",
       "      <th>CH_Syotai_川崎</th>\n",
       "      <th>top2_ChokyosiCode</th>\n",
       "      <th>top2_BanusiCode</th>\n",
       "      <th>top2_UM_BreederCode</th>\n",
       "      <th>before_Odds</th>\n",
       "      <th>speed</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12135</td>\n",
       "      <td>2009100729</td>\n",
       "      <td>2011</td>\n",
       "      <td>1010</td>\n",
       "      <td>949030</td>\n",
       "      <td>600016</td>\n",
       "      <td>222</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.352506</td>\n",
       "      <td>-0.921203</td>\n",
       "      <td>0.090456</td>\n",
       "      <td>0.243086</td>\n",
       "      <td>59.833795</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26439</td>\n",
       "      <td>2011103176</td>\n",
       "      <td>2015</td>\n",
       "      <td>1095</td>\n",
       "      <td>477030</td>\n",
       "      <td>100046</td>\n",
       "      <td>35</td>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.254953</td>\n",
       "      <td>-0.661138</td>\n",
       "      <td>-0.651838</td>\n",
       "      <td>-0.672843</td>\n",
       "      <td>58.230257</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33661</td>\n",
       "      <td>2013100779</td>\n",
       "      <td>2017</td>\n",
       "      <td>1123</td>\n",
       "      <td>78006</td>\n",
       "      <td>703397</td>\n",
       "      <td>148</td>\n",
       "      <td>2.348416</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.206765</td>\n",
       "      <td>-0.442809</td>\n",
       "      <td>-1.241063</td>\n",
       "      <td>-0.613398</td>\n",
       "      <td>57.920792</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28482</td>\n",
       "      <td>2011105992</td>\n",
       "      <td>2016</td>\n",
       "      <td>1129</td>\n",
       "      <td>547800</td>\n",
       "      <td>610012</td>\n",
       "      <td>52</td>\n",
       "      <td>1.378788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.453573</td>\n",
       "      <td>-0.510523</td>\n",
       "      <td>-0.974169</td>\n",
       "      <td>-0.578217</td>\n",
       "      <td>60.876249</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>291</td>\n",
       "      <td>2004104307</td>\n",
       "      <td>2008</td>\n",
       "      <td>331</td>\n",
       "      <td>142006</td>\n",
       "      <td>701079</td>\n",
       "      <td>933</td>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.477666</td>\n",
       "      <td>-1.938882</td>\n",
       "      <td>-0.988132</td>\n",
       "      <td>0.072652</td>\n",
       "      <td>57.627119</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475191</th>\n",
       "      <td>18201</td>\n",
       "      <td>2005106482</td>\n",
       "      <td>2013</td>\n",
       "      <td>1013</td>\n",
       "      <td>789006</td>\n",
       "      <td>400018</td>\n",
       "      <td>911</td>\n",
       "      <td>3.027155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232084</td>\n",
       "      <td>-0.467961</td>\n",
       "      <td>0.392597</td>\n",
       "      <td>3.096416</td>\n",
       "      <td>49.966265</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475192</th>\n",
       "      <td>7309</td>\n",
       "      <td>2006102916</td>\n",
       "      <td>2010</td>\n",
       "      <td>1076</td>\n",
       "      <td>310007</td>\n",
       "      <td>393126</td>\n",
       "      <td>369</td>\n",
       "      <td>-1.530097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.380029</td>\n",
       "      <td>0.211269</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.553954</td>\n",
       "      <td>60.301508</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475193</th>\n",
       "      <td>16882</td>\n",
       "      <td>2007104657</td>\n",
       "      <td>2012</td>\n",
       "      <td>1102</td>\n",
       "      <td>103006</td>\n",
       "      <td>330314</td>\n",
       "      <td>63</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.541780</td>\n",
       "      <td>-1.376219</td>\n",
       "      <td>-0.335163</td>\n",
       "      <td>-0.299192</td>\n",
       "      <td>62.113587</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475194</th>\n",
       "      <td>8963</td>\n",
       "      <td>2007104503</td>\n",
       "      <td>2010</td>\n",
       "      <td>221</td>\n",
       "      <td>100006</td>\n",
       "      <td>310390</td>\n",
       "      <td>109</td>\n",
       "      <td>-1.530097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.098465</td>\n",
       "      <td>-0.945524</td>\n",
       "      <td>-1.037796</td>\n",
       "      <td>-0.678908</td>\n",
       "      <td>60.050042</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475195</th>\n",
       "      <td>32570</td>\n",
       "      <td>2014103689</td>\n",
       "      <td>2017</td>\n",
       "      <td>436</td>\n",
       "      <td>129008</td>\n",
       "      <td>510045</td>\n",
       "      <td>717</td>\n",
       "      <td>-0.802876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.745284</td>\n",
       "      <td>0.159340</td>\n",
       "      <td>-0.418325</td>\n",
       "      <td>-0.477525</td>\n",
       "      <td>57.849197</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>475196 rows × 213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        race_id    KettoNum  id$Year  ChokyosiCode  BanusiCode  \\\n",
       "0         12135  2009100729     2011          1010      949030   \n",
       "1         26439  2011103176     2015          1095      477030   \n",
       "2         33661  2013100779     2017          1123       78006   \n",
       "3         28482  2011105992     2016          1129      547800   \n",
       "4           291  2004104307     2008           331      142006   \n",
       "...         ...         ...      ...           ...         ...   \n",
       "475191    18201  2005106482     2013          1013      789006   \n",
       "475192     7309  2006102916     2010          1076      310007   \n",
       "475193    16882  2007104657     2012          1102      103006   \n",
       "475194     8963  2007104503     2010           221      100006   \n",
       "475195    32570  2014103689     2017           436      129008   \n",
       "\n",
       "        UM_BreederCode  Odds     Kyori  TenkoBaba$DirtBabaCD_0  \\\n",
       "0               600016   222 -1.045283                     1.0   \n",
       "1               100046    35  0.166752                     0.0   \n",
       "2               703397   148  2.348416                     1.0   \n",
       "3               610012    52  1.378788                     1.0   \n",
       "4               701079   933  0.166752                     0.0   \n",
       "...                ...   ...       ...                     ...   \n",
       "475191          400018   911  3.027155                     0.0   \n",
       "475192          393126   369 -1.530097                     0.0   \n",
       "475193          330314    63 -1.045283                     1.0   \n",
       "475194          310390   109 -1.530097                     0.0   \n",
       "475195          510045   717 -0.802876                     0.0   \n",
       "\n",
       "        TenkoBaba$SibaBabaCD_0  ...  KS_Syotai_笠松　　　　　　　　  id$JyoCD_1  \\\n",
       "0                          0.0  ...                   0.0         0.0   \n",
       "1                          1.0  ...                   0.0         0.0   \n",
       "2                          0.0  ...                   0.0         0.0   \n",
       "3                          0.0  ...                   0.0         0.0   \n",
       "4                          1.0  ...                   0.0         0.0   \n",
       "...                        ...  ...                   ...         ...   \n",
       "475191                     0.0  ...                   0.0         0.0   \n",
       "475192                     1.0  ...                   0.0         0.0   \n",
       "475193                     0.0  ...                   0.0         0.0   \n",
       "475194                     1.0  ...                   0.0         0.0   \n",
       "475195                     1.0  ...                   0.0         0.0   \n",
       "\n",
       "        KS_ChokyosiCode_365.0  CH_Syotai_川崎　　　　　　　　  top2_ChokyosiCode  \\\n",
       "0                         0.0                   0.0          -1.352506   \n",
       "1                         0.0                   0.0           0.254953   \n",
       "2                         0.0                   0.0          -0.206765   \n",
       "3                         0.0                   0.0           0.453573   \n",
       "4                         0.0                   0.0          -1.477666   \n",
       "...                       ...                   ...                ...   \n",
       "475191                    0.0                   0.0           0.232084   \n",
       "475192                    0.0                   0.0          -0.380029   \n",
       "475193                    0.0                   0.0           0.541780   \n",
       "475194                    0.0                   0.0          -1.098465   \n",
       "475195                    0.0                   0.0          -0.745284   \n",
       "\n",
       "        top2_BanusiCode  top2_UM_BreederCode  before_Odds      speed  \\\n",
       "0             -0.921203             0.090456     0.243086  59.833795   \n",
       "1             -0.661138            -0.651838    -0.672843  58.230257   \n",
       "2             -0.442809            -1.241063    -0.613398  57.920792   \n",
       "3             -0.510523            -0.974169    -0.578217  60.876249   \n",
       "4             -1.938882            -0.988132     0.072652  57.627119   \n",
       "...                 ...                  ...          ...        ...   \n",
       "475191        -0.467961             0.392597     3.096416  49.966265   \n",
       "475192         0.211269             1.335489    -0.553954  60.301508   \n",
       "475193        -1.376219            -0.335163    -0.299192  62.113587   \n",
       "475194        -0.945524            -1.037796    -0.678908  60.050042   \n",
       "475195         0.159340            -0.418325    -0.477525  57.849197   \n",
       "\n",
       "            _merge  \n",
       "0             both  \n",
       "1             both  \n",
       "2             both  \n",
       "3             both  \n",
       "4             both  \n",
       "...            ...  \n",
       "475191  right_only  \n",
       "475192        both  \n",
       "475193        both  \n",
       "475194        both  \n",
       "475195        both  \n",
       "\n",
       "[475196 rows x 213 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.84 s\n"
     ]
    }
   ],
   "source": [
    "new_train_data = pd.merge(filter_Kyori_id, train_data, on=['race_id', 'KettoNum'], how='right', indicator=True)\n",
    "new_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>id$Year</th>\n",
       "      <th>ChokyosiCode</th>\n",
       "      <th>BanusiCode</th>\n",
       "      <th>UM_BreederCode</th>\n",
       "      <th>Odds</th>\n",
       "      <th>Kyori</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_0</th>\n",
       "      <th>...</th>\n",
       "      <th>KS_Syotai_笠松</th>\n",
       "      <th>id$JyoCD_1</th>\n",
       "      <th>KS_ChokyosiCode_365.0</th>\n",
       "      <th>CH_Syotai_川崎</th>\n",
       "      <th>top2_ChokyosiCode</th>\n",
       "      <th>top2_BanusiCode</th>\n",
       "      <th>top2_UM_BreederCode</th>\n",
       "      <th>before_Odds</th>\n",
       "      <th>speed</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12135</td>\n",
       "      <td>2009100729</td>\n",
       "      <td>2011</td>\n",
       "      <td>1010</td>\n",
       "      <td>949030</td>\n",
       "      <td>600016</td>\n",
       "      <td>222</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.352506</td>\n",
       "      <td>-0.921203</td>\n",
       "      <td>0.090456</td>\n",
       "      <td>0.243086</td>\n",
       "      <td>59.833795</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26439</td>\n",
       "      <td>2011103176</td>\n",
       "      <td>2015</td>\n",
       "      <td>1095</td>\n",
       "      <td>477030</td>\n",
       "      <td>100046</td>\n",
       "      <td>35</td>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.254953</td>\n",
       "      <td>-0.661138</td>\n",
       "      <td>-0.651838</td>\n",
       "      <td>-0.672843</td>\n",
       "      <td>58.230257</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33661</td>\n",
       "      <td>2013100779</td>\n",
       "      <td>2017</td>\n",
       "      <td>1123</td>\n",
       "      <td>78006</td>\n",
       "      <td>703397</td>\n",
       "      <td>148</td>\n",
       "      <td>2.348416</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.206765</td>\n",
       "      <td>-0.442809</td>\n",
       "      <td>-1.241063</td>\n",
       "      <td>-0.613398</td>\n",
       "      <td>57.920792</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28482</td>\n",
       "      <td>2011105992</td>\n",
       "      <td>2016</td>\n",
       "      <td>1129</td>\n",
       "      <td>547800</td>\n",
       "      <td>610012</td>\n",
       "      <td>52</td>\n",
       "      <td>1.378788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.453573</td>\n",
       "      <td>-0.510523</td>\n",
       "      <td>-0.974169</td>\n",
       "      <td>-0.578217</td>\n",
       "      <td>60.876249</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>291</td>\n",
       "      <td>2004104307</td>\n",
       "      <td>2008</td>\n",
       "      <td>331</td>\n",
       "      <td>142006</td>\n",
       "      <td>701079</td>\n",
       "      <td>933</td>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.477666</td>\n",
       "      <td>-1.938882</td>\n",
       "      <td>-0.988132</td>\n",
       "      <td>0.072652</td>\n",
       "      <td>57.627119</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452048</th>\n",
       "      <td>1568</td>\n",
       "      <td>2005109277</td>\n",
       "      <td>2008</td>\n",
       "      <td>1026</td>\n",
       "      <td>75800</td>\n",
       "      <td>230123</td>\n",
       "      <td>2240</td>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.376072</td>\n",
       "      <td>0.566777</td>\n",
       "      <td>-0.009505</td>\n",
       "      <td>0.632507</td>\n",
       "      <td>55.686988</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452049</th>\n",
       "      <td>7309</td>\n",
       "      <td>2006102916</td>\n",
       "      <td>2010</td>\n",
       "      <td>1076</td>\n",
       "      <td>310007</td>\n",
       "      <td>393126</td>\n",
       "      <td>369</td>\n",
       "      <td>-1.530097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.380029</td>\n",
       "      <td>0.211269</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.553954</td>\n",
       "      <td>60.301508</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452050</th>\n",
       "      <td>16882</td>\n",
       "      <td>2007104657</td>\n",
       "      <td>2012</td>\n",
       "      <td>1102</td>\n",
       "      <td>103006</td>\n",
       "      <td>330314</td>\n",
       "      <td>63</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.541780</td>\n",
       "      <td>-1.376219</td>\n",
       "      <td>-0.335163</td>\n",
       "      <td>-0.299192</td>\n",
       "      <td>62.113587</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452051</th>\n",
       "      <td>8963</td>\n",
       "      <td>2007104503</td>\n",
       "      <td>2010</td>\n",
       "      <td>221</td>\n",
       "      <td>100006</td>\n",
       "      <td>310390</td>\n",
       "      <td>109</td>\n",
       "      <td>-1.530097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.098465</td>\n",
       "      <td>-0.945524</td>\n",
       "      <td>-1.037796</td>\n",
       "      <td>-0.678908</td>\n",
       "      <td>60.050042</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452052</th>\n",
       "      <td>32570</td>\n",
       "      <td>2014103689</td>\n",
       "      <td>2017</td>\n",
       "      <td>436</td>\n",
       "      <td>129008</td>\n",
       "      <td>510045</td>\n",
       "      <td>717</td>\n",
       "      <td>-0.802876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.745284</td>\n",
       "      <td>0.159340</td>\n",
       "      <td>-0.418325</td>\n",
       "      <td>-0.477525</td>\n",
       "      <td>57.849197</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>452053 rows × 213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        race_id    KettoNum  id$Year  ChokyosiCode  BanusiCode  \\\n",
       "0         12135  2009100729     2011          1010      949030   \n",
       "1         26439  2011103176     2015          1095      477030   \n",
       "2         33661  2013100779     2017          1123       78006   \n",
       "3         28482  2011105992     2016          1129      547800   \n",
       "4           291  2004104307     2008           331      142006   \n",
       "...         ...         ...      ...           ...         ...   \n",
       "452048     1568  2005109277     2008          1026       75800   \n",
       "452049     7309  2006102916     2010          1076      310007   \n",
       "452050    16882  2007104657     2012          1102      103006   \n",
       "452051     8963  2007104503     2010           221      100006   \n",
       "452052    32570  2014103689     2017           436      129008   \n",
       "\n",
       "        UM_BreederCode  Odds     Kyori  TenkoBaba$DirtBabaCD_0  \\\n",
       "0               600016   222 -1.045283                     1.0   \n",
       "1               100046    35  0.166752                     0.0   \n",
       "2               703397   148  2.348416                     1.0   \n",
       "3               610012    52  1.378788                     1.0   \n",
       "4               701079   933  0.166752                     0.0   \n",
       "...                ...   ...       ...                     ...   \n",
       "452048          230123  2240  0.166752                     0.0   \n",
       "452049          393126   369 -1.530097                     0.0   \n",
       "452050          330314    63 -1.045283                     1.0   \n",
       "452051          310390   109 -1.530097                     0.0   \n",
       "452052          510045   717 -0.802876                     0.0   \n",
       "\n",
       "        TenkoBaba$SibaBabaCD_0  ...  KS_Syotai_笠松　　　　　　　　  id$JyoCD_1  \\\n",
       "0                          0.0  ...                   0.0         0.0   \n",
       "1                          1.0  ...                   0.0         0.0   \n",
       "2                          0.0  ...                   0.0         0.0   \n",
       "3                          0.0  ...                   0.0         0.0   \n",
       "4                          1.0  ...                   0.0         0.0   \n",
       "...                        ...  ...                   ...         ...   \n",
       "452048                     1.0  ...                   0.0         0.0   \n",
       "452049                     1.0  ...                   0.0         0.0   \n",
       "452050                     0.0  ...                   0.0         0.0   \n",
       "452051                     1.0  ...                   0.0         0.0   \n",
       "452052                     1.0  ...                   0.0         0.0   \n",
       "\n",
       "        KS_ChokyosiCode_365.0  CH_Syotai_川崎　　　　　　　　  top2_ChokyosiCode  \\\n",
       "0                         0.0                   0.0          -1.352506   \n",
       "1                         0.0                   0.0           0.254953   \n",
       "2                         0.0                   0.0          -0.206765   \n",
       "3                         0.0                   0.0           0.453573   \n",
       "4                         0.0                   0.0          -1.477666   \n",
       "...                       ...                   ...                ...   \n",
       "452048                    0.0                   0.0          -0.376072   \n",
       "452049                    0.0                   0.0          -0.380029   \n",
       "452050                    0.0                   0.0           0.541780   \n",
       "452051                    0.0                   0.0          -1.098465   \n",
       "452052                    0.0                   0.0          -0.745284   \n",
       "\n",
       "        top2_BanusiCode  top2_UM_BreederCode  before_Odds      speed  _merge  \n",
       "0             -0.921203             0.090456     0.243086  59.833795    both  \n",
       "1             -0.661138            -0.651838    -0.672843  58.230257    both  \n",
       "2             -0.442809            -1.241063    -0.613398  57.920792    both  \n",
       "3             -0.510523            -0.974169    -0.578217  60.876249    both  \n",
       "4             -1.938882            -0.988132     0.072652  57.627119    both  \n",
       "...                 ...                  ...          ...        ...     ...  \n",
       "452048         0.566777            -0.009505     0.632507  55.686988    both  \n",
       "452049         0.211269             1.335489    -0.553954  60.301508    both  \n",
       "452050        -1.376219            -0.335163    -0.299192  62.113587    both  \n",
       "452051        -0.945524            -1.037796    -0.678908  60.050042    both  \n",
       "452052         0.159340            -0.418325    -0.477525  57.849197    both  \n",
       "\n",
       "[452053 rows x 213 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 438 ms\n"
     ]
    }
   ],
   "source": [
    "new_train_data = new_train_data[new_train_data['_merge']=='both']\n",
    "new_train_data.reset_index(drop=True, inplace=True)\n",
    "new_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>id$Year</th>\n",
       "      <th>Kyori</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_1</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_1</th>\n",
       "      <th>id$RaceNum</th>\n",
       "      <th>TrackCD_52</th>\n",
       "      <th>...</th>\n",
       "      <th>KS_Syotai_川崎</th>\n",
       "      <th>KS_Syotai_笠松</th>\n",
       "      <th>id$JyoCD_1</th>\n",
       "      <th>KS_ChokyosiCode_365.0</th>\n",
       "      <th>CH_Syotai_川崎</th>\n",
       "      <th>top2_ChokyosiCode</th>\n",
       "      <th>top2_BanusiCode</th>\n",
       "      <th>top2_UM_BreederCode</th>\n",
       "      <th>before_Odds</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015101022</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.398731</td>\n",
       "      <td>-0.216783</td>\n",
       "      <td>0.174543</td>\n",
       "      <td>-0.638874</td>\n",
       "      <td>58.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015103483</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.415822</td>\n",
       "      <td>-0.318228</td>\n",
       "      <td>-1.717806</td>\n",
       "      <td>4.068149</td>\n",
       "      <td>57.908847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015106010</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.351113</td>\n",
       "      <td>-2.011561</td>\n",
       "      <td>-0.232294</td>\n",
       "      <td>-0.547888</td>\n",
       "      <td>59.178082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015102342</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.812923</td>\n",
       "      <td>-0.653485</td>\n",
       "      <td>-1.837928</td>\n",
       "      <td>0.188494</td>\n",
       "      <td>58.775510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34535</td>\n",
       "      <td>2015102323</td>\n",
       "      <td>2018</td>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.469803</td>\n",
       "      <td>-0.723257</td>\n",
       "      <td>-0.230315</td>\n",
       "      <td>-0.456902</td>\n",
       "      <td>57.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19140</th>\n",
       "      <td>35925</td>\n",
       "      <td>2014105425</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.252109</td>\n",
       "      <td>1.811460</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.671629</td>\n",
       "      <td>58.378378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19141</th>\n",
       "      <td>35925</td>\n",
       "      <td>2014105543</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.897933</td>\n",
       "      <td>2.023608</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.698319</td>\n",
       "      <td>57.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19142</th>\n",
       "      <td>35925</td>\n",
       "      <td>2011106130</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.608380</td>\n",
       "      <td>0.873101</td>\n",
       "      <td>-1.450211</td>\n",
       "      <td>1.291248</td>\n",
       "      <td>57.754011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19143</th>\n",
       "      <td>35925</td>\n",
       "      <td>2012102418</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.897933</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.435675</td>\n",
       "      <td>-0.701958</td>\n",
       "      <td>57.497782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19144</th>\n",
       "      <td>35925</td>\n",
       "      <td>2013104045</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.291467</td>\n",
       "      <td>0.204303</td>\n",
       "      <td>-0.451692</td>\n",
       "      <td>-0.032299</td>\n",
       "      <td>57.243816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19145 rows × 208 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       race_id    KettoNum  id$Year     Kyori  TenkoBaba$DirtBabaCD_0  \\\n",
       "0        34535  2015101022     2018 -1.045283                     0.0   \n",
       "1        34535  2015103483     2018 -1.045283                     0.0   \n",
       "2        34535  2015106010     2018 -1.045283                     0.0   \n",
       "3        34535  2015102342     2018 -1.045283                     0.0   \n",
       "4        34535  2015102323     2018 -1.045283                     0.0   \n",
       "...        ...         ...      ...       ...                     ...   \n",
       "19140    35925  2014105425     2018  0.409159                     0.0   \n",
       "19141    35925  2014105543     2018  0.409159                     0.0   \n",
       "19142    35925  2011106130     2018  0.409159                     0.0   \n",
       "19143    35925  2012102418     2018  0.409159                     0.0   \n",
       "19144    35925  2013104045     2018  0.409159                     0.0   \n",
       "\n",
       "       TenkoBaba$SibaBabaCD_0  TenkoBaba$SibaBabaCD_1  TenkoBaba$DirtBabaCD_1  \\\n",
       "0                         1.0                     0.0                     1.0   \n",
       "1                         1.0                     0.0                     1.0   \n",
       "2                         1.0                     0.0                     1.0   \n",
       "3                         1.0                     0.0                     1.0   \n",
       "4                         1.0                     0.0                     1.0   \n",
       "...                       ...                     ...                     ...   \n",
       "19140                     1.0                     0.0                     1.0   \n",
       "19141                     1.0                     0.0                     1.0   \n",
       "19142                     1.0                     0.0                     1.0   \n",
       "19143                     1.0                     0.0                     1.0   \n",
       "19144                     1.0                     0.0                     1.0   \n",
       "\n",
       "       id$RaceNum  TrackCD_52  ...  KS_Syotai_川崎　　　　　　　　  \\\n",
       "0       -1.550314         0.0  ...                   0.0   \n",
       "1       -1.550314         0.0  ...                   0.0   \n",
       "2       -1.550314         0.0  ...                   0.0   \n",
       "3       -1.550314         0.0  ...                   0.0   \n",
       "4       -1.550314         0.0  ...                   0.0   \n",
       "...           ...         ...  ...                   ...   \n",
       "19140    1.664316         0.0  ...                   0.0   \n",
       "19141    1.664316         0.0  ...                   0.0   \n",
       "19142    1.664316         0.0  ...                   0.0   \n",
       "19143    1.664316         0.0  ...                   0.0   \n",
       "19144    1.664316         0.0  ...                   0.0   \n",
       "\n",
       "       KS_Syotai_笠松　　　　　　　　  id$JyoCD_1  KS_ChokyosiCode_365.0  \\\n",
       "0                       0.0         0.0                    0.0   \n",
       "1                       0.0         0.0                    0.0   \n",
       "2                       0.0         0.0                    0.0   \n",
       "3                       0.0         0.0                    0.0   \n",
       "4                       0.0         0.0                    0.0   \n",
       "...                     ...         ...                    ...   \n",
       "19140                   0.0         0.0                    0.0   \n",
       "19141                   0.0         0.0                    0.0   \n",
       "19142                   0.0         0.0                    0.0   \n",
       "19143                   0.0         0.0                    0.0   \n",
       "19144                   0.0         0.0                    0.0   \n",
       "\n",
       "       CH_Syotai_川崎　　　　　　　　  top2_ChokyosiCode  top2_BanusiCode  \\\n",
       "0                       0.0           0.398731        -0.216783   \n",
       "1                       0.0          -1.415822        -0.318228   \n",
       "2                       0.0          -1.351113        -2.011561   \n",
       "3                       0.0          -0.812923        -0.653485   \n",
       "4                       0.0          -1.469803        -0.723257   \n",
       "...                     ...                ...              ...   \n",
       "19140                   0.0           0.252109         1.811460   \n",
       "19141                   0.0           1.897933         2.023608   \n",
       "19142                   0.0          -0.608380         0.873101   \n",
       "19143                   0.0           1.897933         0.945055   \n",
       "19144                   0.0           0.291467         0.204303   \n",
       "\n",
       "       top2_UM_BreederCode  before_Odds      speed  \n",
       "0                 0.174543    -0.638874  58.064516  \n",
       "1                -1.717806     4.068149  57.908847  \n",
       "2                -0.232294    -0.547888  59.178082  \n",
       "3                -1.837928     0.188494  58.775510  \n",
       "4                -0.230315    -0.456902  57.142857  \n",
       "...                    ...          ...        ...  \n",
       "19140             1.335489    -0.671629  58.378378  \n",
       "19141             1.335489    -0.698319  57.857143  \n",
       "19142            -1.450211     1.291248  57.754011  \n",
       "19143             0.435675    -0.701958  57.497782  \n",
       "19144            -0.451692    -0.032299  57.243816  \n",
       "\n",
       "[19145 rows x 208 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 406 ms\n"
     ]
    }
   ],
   "source": [
    "# Load dữ liệu\n",
    "test_data = pd.read_csv('test_data_all.csv')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id$Year</th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>speed</th>\n",
       "      <th>Time</th>\n",
       "      <th>KakuteiJyuni</th>\n",
       "      <th>top3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011</td>\n",
       "      <td>12135</td>\n",
       "      <td>2009100729</td>\n",
       "      <td>59.833795</td>\n",
       "      <td>72.20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>26439</td>\n",
       "      <td>2011103176</td>\n",
       "      <td>58.230257</td>\n",
       "      <td>105.10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>33661</td>\n",
       "      <td>2013100779</td>\n",
       "      <td>57.920792</td>\n",
       "      <td>161.60</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>28482</td>\n",
       "      <td>2011105992</td>\n",
       "      <td>60.876249</td>\n",
       "      <td>130.10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008</td>\n",
       "      <td>291</td>\n",
       "      <td>2004104307</td>\n",
       "      <td>57.627119</td>\n",
       "      <td>106.20</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475191</th>\n",
       "      <td>2013</td>\n",
       "      <td>18201</td>\n",
       "      <td>2005106482</td>\n",
       "      <td>49.966265</td>\n",
       "      <td>207.50</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475192</th>\n",
       "      <td>2010</td>\n",
       "      <td>7309</td>\n",
       "      <td>2006102916</td>\n",
       "      <td>60.301508</td>\n",
       "      <td>59.70</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475193</th>\n",
       "      <td>2012</td>\n",
       "      <td>16882</td>\n",
       "      <td>2007104657</td>\n",
       "      <td>62.113587</td>\n",
       "      <td>69.55</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475194</th>\n",
       "      <td>2010</td>\n",
       "      <td>8963</td>\n",
       "      <td>2007104503</td>\n",
       "      <td>60.050042</td>\n",
       "      <td>59.95</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475195</th>\n",
       "      <td>2017</td>\n",
       "      <td>32570</td>\n",
       "      <td>2014103689</td>\n",
       "      <td>57.849197</td>\n",
       "      <td>80.90</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>475196 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id$Year  race_id    KettoNum      speed    Time  KakuteiJyuni  top3\n",
       "0          2011    12135  2009100729  59.833795   72.20             1     1\n",
       "1          2015    26439  2011103176  58.230257  105.10             1     1\n",
       "2          2017    33661  2013100779  57.920792  161.60            11     0\n",
       "3          2016    28482  2011105992  60.876249  130.10             2     1\n",
       "4          2008      291  2004104307  57.627119  106.20             5     0\n",
       "...         ...      ...         ...        ...     ...           ...   ...\n",
       "475191     2013    18201  2005106482  49.966265  207.50            13     0\n",
       "475192     2010     7309  2006102916  60.301508   59.70             8     0\n",
       "475193     2012    16882  2007104657  62.113587   69.55             6     0\n",
       "475194     2010     8963  2007104503  60.050042   59.95             5     0\n",
       "475195     2017    32570  2014103689  57.849197   80.90             8     0\n",
       "\n",
       "[475196 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 312 ms\n"
     ]
    }
   ],
   "source": [
    "y_train_df = pd.read_csv('y_train_df_all.csv')\n",
    "y_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>id$Year</th>\n",
       "      <th>speed</th>\n",
       "      <th>Time</th>\n",
       "      <th>KakuteiJyuni</th>\n",
       "      <th>top3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12135</td>\n",
       "      <td>2009100729</td>\n",
       "      <td>2011</td>\n",
       "      <td>59.833795</td>\n",
       "      <td>72.20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26439</td>\n",
       "      <td>2011103176</td>\n",
       "      <td>2015</td>\n",
       "      <td>58.230257</td>\n",
       "      <td>105.10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33661</td>\n",
       "      <td>2013100779</td>\n",
       "      <td>2017</td>\n",
       "      <td>57.920792</td>\n",
       "      <td>161.60</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28482</td>\n",
       "      <td>2011105992</td>\n",
       "      <td>2016</td>\n",
       "      <td>60.876249</td>\n",
       "      <td>130.10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>291</td>\n",
       "      <td>2004104307</td>\n",
       "      <td>2008</td>\n",
       "      <td>57.627119</td>\n",
       "      <td>106.20</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452048</th>\n",
       "      <td>1568</td>\n",
       "      <td>2005109277</td>\n",
       "      <td>2008</td>\n",
       "      <td>55.686988</td>\n",
       "      <td>109.90</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452049</th>\n",
       "      <td>7309</td>\n",
       "      <td>2006102916</td>\n",
       "      <td>2010</td>\n",
       "      <td>60.301508</td>\n",
       "      <td>59.70</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452050</th>\n",
       "      <td>16882</td>\n",
       "      <td>2007104657</td>\n",
       "      <td>2012</td>\n",
       "      <td>62.113587</td>\n",
       "      <td>69.55</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452051</th>\n",
       "      <td>8963</td>\n",
       "      <td>2007104503</td>\n",
       "      <td>2010</td>\n",
       "      <td>60.050042</td>\n",
       "      <td>59.95</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452052</th>\n",
       "      <td>32570</td>\n",
       "      <td>2014103689</td>\n",
       "      <td>2017</td>\n",
       "      <td>57.849197</td>\n",
       "      <td>80.90</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>452053 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        race_id    KettoNum  id$Year      speed    Time  KakuteiJyuni  top3\n",
       "0         12135  2009100729     2011  59.833795   72.20             1     1\n",
       "1         26439  2011103176     2015  58.230257  105.10             1     1\n",
       "2         33661  2013100779     2017  57.920792  161.60            11     0\n",
       "3         28482  2011105992     2016  60.876249  130.10             2     1\n",
       "4           291  2004104307     2008  57.627119  106.20             5     0\n",
       "...         ...         ...      ...        ...     ...           ...   ...\n",
       "452048     1568  2005109277     2008  55.686988  109.90            13     0\n",
       "452049     7309  2006102916     2010  60.301508   59.70             8     0\n",
       "452050    16882  2007104657     2012  62.113587   69.55             6     0\n",
       "452051     8963  2007104503     2010  60.050042   59.95             5     0\n",
       "452052    32570  2014103689     2017  57.849197   80.90             8     0\n",
       "\n",
       "[452053 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 485 ms\n"
     ]
    }
   ],
   "source": [
    "y_train_df = pd.merge(filter_Kyori_id, y_train_df, on=['race_id', 'KettoNum'], how='right', indicator=True)\n",
    "y_train_df = y_train_df[y_train_df['_merge']=='both']\n",
    "y_train_df.drop('_merge', axis=1, inplace=True)\n",
    "y_train_df.reset_index(drop=True, inplace=True)\n",
    "y_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id$Year</th>\n",
       "      <th>race_id</th>\n",
       "      <th>KettoNum</th>\n",
       "      <th>speed</th>\n",
       "      <th>Time</th>\n",
       "      <th>KakuteiJyuni</th>\n",
       "      <th>top3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015101022</td>\n",
       "      <td>58.064516</td>\n",
       "      <td>74.4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015103483</td>\n",
       "      <td>57.908847</td>\n",
       "      <td>74.6</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015106010</td>\n",
       "      <td>59.178082</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015102342</td>\n",
       "      <td>58.775510</td>\n",
       "      <td>73.5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>34535</td>\n",
       "      <td>2015102323</td>\n",
       "      <td>57.142857</td>\n",
       "      <td>75.6</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19140</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2014105425</td>\n",
       "      <td>58.378378</td>\n",
       "      <td>111.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19141</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2014105543</td>\n",
       "      <td>57.857143</td>\n",
       "      <td>112.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19142</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2011106130</td>\n",
       "      <td>57.754011</td>\n",
       "      <td>112.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19143</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2012102418</td>\n",
       "      <td>57.497782</td>\n",
       "      <td>112.7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19144</th>\n",
       "      <td>2018</td>\n",
       "      <td>35925</td>\n",
       "      <td>2013104045</td>\n",
       "      <td>57.243816</td>\n",
       "      <td>113.2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19145 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id$Year  race_id    KettoNum      speed   Time  KakuteiJyuni  top3\n",
       "0         2018    34535  2015101022  58.064516   74.4            10     0\n",
       "1         2018    34535  2015103483  57.908847   74.6            11     0\n",
       "2         2018    34535  2015106010  59.178082   73.0             2     1\n",
       "3         2018    34535  2015102342  58.775510   73.5             6     0\n",
       "4         2018    34535  2015102323  57.142857   75.6            16     0\n",
       "...        ...      ...         ...        ...    ...           ...   ...\n",
       "19140     2018    35925  2014105425  58.378378  111.0             1     1\n",
       "19141     2018    35925  2014105543  57.857143  112.0             5     0\n",
       "19142     2018    35925  2011106130  57.754011  112.2             6     0\n",
       "19143     2018    35925  2012102418  57.497782  112.7             8     0\n",
       "19144     2018    35925  2013104045  57.243816  113.2            12     0\n",
       "\n",
       "[19145 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 46 ms\n"
     ]
    }
   ],
   "source": [
    "y_test_df = pd.read_csv('y_test_df_all.csv')\n",
    "y_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create X, y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kyori</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_1</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_1</th>\n",
       "      <th>id$RaceNum</th>\n",
       "      <th>TrackCD_52</th>\n",
       "      <th>TrackCD_17</th>\n",
       "      <th>JyokenInfo$SyubetuCD_18</th>\n",
       "      <th>GradeCD_</th>\n",
       "      <th>...</th>\n",
       "      <th>KS_Syotai_フランス</th>\n",
       "      <th>KS_Syotai_川崎</th>\n",
       "      <th>KS_Syotai_笠松</th>\n",
       "      <th>id$JyoCD_1</th>\n",
       "      <th>KS_ChokyosiCode_365.0</th>\n",
       "      <th>CH_Syotai_川崎</th>\n",
       "      <th>top2_ChokyosiCode</th>\n",
       "      <th>top2_BanusiCode</th>\n",
       "      <th>top2_UM_BreederCode</th>\n",
       "      <th>before_Odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.352506</td>\n",
       "      <td>-0.921203</td>\n",
       "      <td>0.090456</td>\n",
       "      <td>0.243086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.254953</td>\n",
       "      <td>-0.661138</td>\n",
       "      <td>-0.651838</td>\n",
       "      <td>-0.672843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.348416</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.203121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.206765</td>\n",
       "      <td>-0.442809</td>\n",
       "      <td>-1.241063</td>\n",
       "      <td>-0.613398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.378788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.079838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.453573</td>\n",
       "      <td>-0.510523</td>\n",
       "      <td>-0.974169</td>\n",
       "      <td>-0.578217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.477666</td>\n",
       "      <td>-1.938882</td>\n",
       "      <td>-0.988132</td>\n",
       "      <td>0.072652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452048</th>\n",
       "      <td>0.166752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.376072</td>\n",
       "      <td>0.566777</td>\n",
       "      <td>-0.009505</td>\n",
       "      <td>0.632507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452049</th>\n",
       "      <td>-1.530097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.965836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.380029</td>\n",
       "      <td>0.211269</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.553954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452050</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.541780</td>\n",
       "      <td>-1.376219</td>\n",
       "      <td>-0.335163</td>\n",
       "      <td>-0.299192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452051</th>\n",
       "      <td>-1.530097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.258075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.098465</td>\n",
       "      <td>-0.945524</td>\n",
       "      <td>-1.037796</td>\n",
       "      <td>-0.678908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452052</th>\n",
       "      <td>-0.802876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.258075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.745284</td>\n",
       "      <td>0.159340</td>\n",
       "      <td>-0.418325</td>\n",
       "      <td>-0.477525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>452053 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Kyori  TenkoBaba$DirtBabaCD_0  TenkoBaba$SibaBabaCD_0  \\\n",
       "0      -1.045283                     1.0                     0.0   \n",
       "1       0.166752                     0.0                     1.0   \n",
       "2       2.348416                     1.0                     0.0   \n",
       "3       1.378788                     1.0                     0.0   \n",
       "4       0.166752                     0.0                     1.0   \n",
       "...          ...                     ...                     ...   \n",
       "452048  0.166752                     0.0                     1.0   \n",
       "452049 -1.530097                     0.0                     1.0   \n",
       "452050 -1.045283                     1.0                     0.0   \n",
       "452051 -1.530097                     0.0                     1.0   \n",
       "452052 -0.802876                     0.0                     1.0   \n",
       "\n",
       "        TenkoBaba$SibaBabaCD_1  TenkoBaba$DirtBabaCD_1  id$RaceNum  \\\n",
       "0                          0.0                     0.0   -1.550314   \n",
       "1                          0.0                     0.0    0.495360   \n",
       "2                          1.0                     0.0    0.203121   \n",
       "3                          1.0                     0.0    1.079838   \n",
       "4                          0.0                     0.0    0.495360   \n",
       "...                        ...                     ...         ...   \n",
       "452048                     0.0                     1.0    1.664316   \n",
       "452049                     0.0                     0.0   -0.965836   \n",
       "452050                     1.0                     0.0    0.495360   \n",
       "452051                     0.0                     1.0   -1.258075   \n",
       "452052                     0.0                     1.0   -1.258075   \n",
       "\n",
       "        TrackCD_52  TrackCD_17  JyokenInfo$SyubetuCD_18  GradeCD_   ...  \\\n",
       "0              0.0         1.0                      0.0        1.0  ...   \n",
       "1              0.0         0.0                      0.0        1.0  ...   \n",
       "2              0.0         0.0                      0.0        1.0  ...   \n",
       "3              0.0         0.0                      0.0        0.0  ...   \n",
       "4              0.0         0.0                      0.0        1.0  ...   \n",
       "...            ...         ...                      ...        ...  ...   \n",
       "452048         0.0         0.0                      0.0        0.0  ...   \n",
       "452049         0.0         0.0                      0.0        1.0  ...   \n",
       "452050         0.0         1.0                      0.0        1.0  ...   \n",
       "452051         0.0         0.0                      0.0        1.0  ...   \n",
       "452052         0.0         0.0                      0.0        1.0  ...   \n",
       "\n",
       "        KS_Syotai_フランス　　　　　　  KS_Syotai_川崎　　　　　　　　  KS_Syotai_笠松　　　　　　　　  \\\n",
       "0                        0.0                   0.0                   0.0   \n",
       "1                        0.0                   0.0                   0.0   \n",
       "2                        0.0                   0.0                   0.0   \n",
       "3                        0.0                   0.0                   0.0   \n",
       "4                        0.0                   0.0                   0.0   \n",
       "...                      ...                   ...                   ...   \n",
       "452048                   0.0                   0.0                   0.0   \n",
       "452049                   0.0                   0.0                   0.0   \n",
       "452050                   0.0                   0.0                   0.0   \n",
       "452051                   0.0                   0.0                   0.0   \n",
       "452052                   0.0                   0.0                   0.0   \n",
       "\n",
       "        id$JyoCD_1  KS_ChokyosiCode_365.0  CH_Syotai_川崎　　　　　　　　  \\\n",
       "0              0.0                    0.0                   0.0   \n",
       "1              0.0                    0.0                   0.0   \n",
       "2              0.0                    0.0                   0.0   \n",
       "3              0.0                    0.0                   0.0   \n",
       "4              0.0                    0.0                   0.0   \n",
       "...            ...                    ...                   ...   \n",
       "452048         0.0                    0.0                   0.0   \n",
       "452049         0.0                    0.0                   0.0   \n",
       "452050         0.0                    0.0                   0.0   \n",
       "452051         0.0                    0.0                   0.0   \n",
       "452052         0.0                    0.0                   0.0   \n",
       "\n",
       "        top2_ChokyosiCode  top2_BanusiCode  top2_UM_BreederCode  before_Odds  \n",
       "0               -1.352506        -0.921203             0.090456     0.243086  \n",
       "1                0.254953        -0.661138            -0.651838    -0.672843  \n",
       "2               -0.206765        -0.442809            -1.241063    -0.613398  \n",
       "3                0.453573        -0.510523            -0.974169    -0.578217  \n",
       "4               -1.477666        -1.938882            -0.988132     0.072652  \n",
       "...                   ...              ...                  ...          ...  \n",
       "452048          -0.376072         0.566777            -0.009505     0.632507  \n",
       "452049          -0.380029         0.211269             1.335489    -0.553954  \n",
       "452050           0.541780        -1.376219            -0.335163    -0.299192  \n",
       "452051          -1.098465        -0.945524            -1.037796    -0.678908  \n",
       "452052          -0.745284         0.159340            -0.418325    -0.477525  \n",
       "\n",
       "[452053 rows x 204 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 265 ms\n"
     ]
    }
   ],
   "source": [
    "ref_col = ['ChokyosiCode', 'BanusiCode', 'UM_BreederCode', 'Odds']\n",
    "drop_columns = ['race_id', 'KettoNum', 'id$Year', 'speed'] + ref_col + ['_merge']\n",
    "X_train = new_train_data.drop(drop_columns, axis=1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    452053.000000\n",
       "mean         58.574405\n",
       "std           2.182646\n",
       "min          49.907579\n",
       "25%          57.199603\n",
       "50%          58.655804\n",
       "75%          60.000000\n",
       "max          66.666667\n",
       "Name: speed, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 32 ms\n"
     ]
    }
   ],
   "source": [
    "y_train = new_train_data['speed']\n",
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 422 ms\n"
     ]
    }
   ],
   "source": [
    "y_train.hist(bins=100);\n",
    "plt.xlabel('speed');\n",
    "plt.ylabel('frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kyori</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_0</th>\n",
       "      <th>TenkoBaba$SibaBabaCD_1</th>\n",
       "      <th>TenkoBaba$DirtBabaCD_1</th>\n",
       "      <th>id$RaceNum</th>\n",
       "      <th>TrackCD_52</th>\n",
       "      <th>TrackCD_17</th>\n",
       "      <th>JyokenInfo$SyubetuCD_18</th>\n",
       "      <th>GradeCD_</th>\n",
       "      <th>...</th>\n",
       "      <th>KS_Syotai_フランス</th>\n",
       "      <th>KS_Syotai_川崎</th>\n",
       "      <th>KS_Syotai_笠松</th>\n",
       "      <th>id$JyoCD_1</th>\n",
       "      <th>KS_ChokyosiCode_365.0</th>\n",
       "      <th>CH_Syotai_川崎</th>\n",
       "      <th>top2_ChokyosiCode</th>\n",
       "      <th>top2_BanusiCode</th>\n",
       "      <th>top2_UM_BreederCode</th>\n",
       "      <th>before_Odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.398731</td>\n",
       "      <td>-0.216783</td>\n",
       "      <td>0.174543</td>\n",
       "      <td>-0.638874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.415822</td>\n",
       "      <td>-0.318228</td>\n",
       "      <td>-1.717806</td>\n",
       "      <td>4.068149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.351113</td>\n",
       "      <td>-2.011561</td>\n",
       "      <td>-0.232294</td>\n",
       "      <td>-0.547888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.812923</td>\n",
       "      <td>-0.653485</td>\n",
       "      <td>-1.837928</td>\n",
       "      <td>0.188494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.045283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.550314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.469803</td>\n",
       "      <td>-0.723257</td>\n",
       "      <td>-0.230315</td>\n",
       "      <td>-0.456902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19140</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.252109</td>\n",
       "      <td>1.811460</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.671629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19141</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.897933</td>\n",
       "      <td>2.023608</td>\n",
       "      <td>1.335489</td>\n",
       "      <td>-0.698319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19142</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.608380</td>\n",
       "      <td>0.873101</td>\n",
       "      <td>-1.450211</td>\n",
       "      <td>1.291248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19143</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.897933</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.435675</td>\n",
       "      <td>-0.701958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19144</th>\n",
       "      <td>0.409159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.664316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.291467</td>\n",
       "      <td>0.204303</td>\n",
       "      <td>-0.451692</td>\n",
       "      <td>-0.032299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19145 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Kyori  TenkoBaba$DirtBabaCD_0  TenkoBaba$SibaBabaCD_0  \\\n",
       "0     -1.045283                     0.0                     1.0   \n",
       "1     -1.045283                     0.0                     1.0   \n",
       "2     -1.045283                     0.0                     1.0   \n",
       "3     -1.045283                     0.0                     1.0   \n",
       "4     -1.045283                     0.0                     1.0   \n",
       "...         ...                     ...                     ...   \n",
       "19140  0.409159                     0.0                     1.0   \n",
       "19141  0.409159                     0.0                     1.0   \n",
       "19142  0.409159                     0.0                     1.0   \n",
       "19143  0.409159                     0.0                     1.0   \n",
       "19144  0.409159                     0.0                     1.0   \n",
       "\n",
       "       TenkoBaba$SibaBabaCD_1  TenkoBaba$DirtBabaCD_1  id$RaceNum  TrackCD_52  \\\n",
       "0                         0.0                     1.0   -1.550314         0.0   \n",
       "1                         0.0                     1.0   -1.550314         0.0   \n",
       "2                         0.0                     1.0   -1.550314         0.0   \n",
       "3                         0.0                     1.0   -1.550314         0.0   \n",
       "4                         0.0                     1.0   -1.550314         0.0   \n",
       "...                       ...                     ...         ...         ...   \n",
       "19140                     0.0                     1.0    1.664316         0.0   \n",
       "19141                     0.0                     1.0    1.664316         0.0   \n",
       "19142                     0.0                     1.0    1.664316         0.0   \n",
       "19143                     0.0                     1.0    1.664316         0.0   \n",
       "19144                     0.0                     1.0    1.664316         0.0   \n",
       "\n",
       "       TrackCD_17  JyokenInfo$SyubetuCD_18  GradeCD_   ...  \\\n",
       "0             0.0                      0.0        1.0  ...   \n",
       "1             0.0                      0.0        1.0  ...   \n",
       "2             0.0                      0.0        1.0  ...   \n",
       "3             0.0                      0.0        1.0  ...   \n",
       "4             0.0                      0.0        1.0  ...   \n",
       "...           ...                      ...        ...  ...   \n",
       "19140         0.0                      0.0        1.0  ...   \n",
       "19141         0.0                      0.0        1.0  ...   \n",
       "19142         0.0                      0.0        1.0  ...   \n",
       "19143         0.0                      0.0        1.0  ...   \n",
       "19144         0.0                      0.0        1.0  ...   \n",
       "\n",
       "       KS_Syotai_フランス　　　　　　  KS_Syotai_川崎　　　　　　　　  KS_Syotai_笠松　　　　　　　　  \\\n",
       "0                       0.0                   0.0                   0.0   \n",
       "1                       0.0                   0.0                   0.0   \n",
       "2                       0.0                   0.0                   0.0   \n",
       "3                       0.0                   0.0                   0.0   \n",
       "4                       0.0                   0.0                   0.0   \n",
       "...                     ...                   ...                   ...   \n",
       "19140                   0.0                   0.0                   0.0   \n",
       "19141                   0.0                   0.0                   0.0   \n",
       "19142                   0.0                   0.0                   0.0   \n",
       "19143                   0.0                   0.0                   0.0   \n",
       "19144                   0.0                   0.0                   0.0   \n",
       "\n",
       "       id$JyoCD_1  KS_ChokyosiCode_365.0  CH_Syotai_川崎　　　　　　　　  \\\n",
       "0             0.0                    0.0                   0.0   \n",
       "1             0.0                    0.0                   0.0   \n",
       "2             0.0                    0.0                   0.0   \n",
       "3             0.0                    0.0                   0.0   \n",
       "4             0.0                    0.0                   0.0   \n",
       "...           ...                    ...                   ...   \n",
       "19140         0.0                    0.0                   0.0   \n",
       "19141         0.0                    0.0                   0.0   \n",
       "19142         0.0                    0.0                   0.0   \n",
       "19143         0.0                    0.0                   0.0   \n",
       "19144         0.0                    0.0                   0.0   \n",
       "\n",
       "       top2_ChokyosiCode  top2_BanusiCode  top2_UM_BreederCode  before_Odds  \n",
       "0               0.398731        -0.216783             0.174543    -0.638874  \n",
       "1              -1.415822        -0.318228            -1.717806     4.068149  \n",
       "2              -1.351113        -2.011561            -0.232294    -0.547888  \n",
       "3              -0.812923        -0.653485            -1.837928     0.188494  \n",
       "4              -1.469803        -0.723257            -0.230315    -0.456902  \n",
       "...                  ...              ...                  ...          ...  \n",
       "19140           0.252109         1.811460             1.335489    -0.671629  \n",
       "19141           1.897933         2.023608             1.335489    -0.698319  \n",
       "19142          -0.608380         0.873101            -1.450211     1.291248  \n",
       "19143           1.897933         0.945055             0.435675    -0.701958  \n",
       "19144           0.291467         0.204303            -0.451692    -0.032299  \n",
       "\n",
       "[19145 rows x 204 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 63 ms\n"
     ]
    }
   ],
   "source": [
    "drop_columns = ['race_id', 'KettoNum', 'id$Year', 'speed']\n",
    "X_test = test_data.drop(drop_columns, axis=1)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    19145.000000\n",
       "mean        58.013408\n",
       "std          2.305421\n",
       "min         38.876890\n",
       "25%         56.509695\n",
       "50%         58.142665\n",
       "75%         59.558824\n",
       "max         65.573770\n",
       "Name: speed, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "y_test = test_data['speed']\n",
    "y_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 282 ms\n"
     ]
    }
   ],
   "source": [
    "y_test.hist(bins=100);\n",
    "plt.xlabel('speed');\n",
    "plt.ylabel('frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train with ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create and compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7_eizWaqi_7",
    "outputId": "4361457c-e661-4e75-8c78-3c1a7789b705"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def my_r2_score(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true) ) ) \n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "def BatchNorm():\n",
    "    return BatchNormalization(\n",
    "                momentum=0.95, \n",
    "                epsilon=0.005,\n",
    "                beta_initializer=RandomNormal(mean=0.0, stddev=0.05), \n",
    "                gamma_initializer=Constant(value=0.9)\n",
    "                )\n",
    "\n",
    "def build_and_compile_model(X_train, num_units=100, activation='sigmoid'):\n",
    "    input_shape = X_train.shape[1] \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=num_units, activation=activation, kernel_initializer='he_normal',\n",
    "                    input_shape=(input_shape,)))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(1))\n",
    "      \n",
    "    model.compile(loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError(), my_r2_score], optimizer=Optimizer.Adam(0.01)) #my_r2_score\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setup callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6x6dteutin0",
    "outputId": "0b4eab09-31e5-4ef0-d9cd-105656a99abe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "class My_checkoint(Callback):\n",
    "        \n",
    "    def __init__(self, model, X_test, y_test, checkpoint_name):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.mode = model\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        cpn = checkpoint_name + format(epoch, '02d') + '-.hdf5'\n",
    "        #cpn = os.path.join(checkpoint_dir, 'model'+format(epoch, '02d') + '-.hdf5')\n",
    "        val_loss = self.mode.evaluate(self.X_test, self.y_test)\n",
    "        print('my_val_loss', val_loss)\n",
    "        self.mode.save(cpn)\n",
    "        \n",
    "def callback_model(model, checkpoint_name, logdir, X_test, y_test):\n",
    "  \n",
    "    _logdir = os.path.join(logdir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_callback = TensorBoard(_logdir, histogram_freq=1)\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                  factor=0.3,\n",
    "                                  patience=1,\n",
    "                                  mode='min',\n",
    "                                  verbose=1)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=7,\n",
    "                                   monitor='val_loss',\n",
    "                                   mode='min',\n",
    "                                   verbose=1)\n",
    "    \n",
    "    csv_logger = CSVLogger('log.log', separator=',', append=False)\n",
    "    \n",
    "    callbacks_list = [tensorboard_callback, reduce_lr, early_stopping, csv_logger, My_checkoint(model, X_test, y_test, checkpoint_name)]\n",
    "    \n",
    "    return callbacks_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xttwiHh4u0ES",
    "outputId": "31e678b8-9b64-4fb6-9154-525972b48aca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, checkpoint_name, logdir, batch_size=32, epochs=10, re_train=True):\n",
    "\n",
    "    if re_train:\n",
    "        # Clear old folder\n",
    "        %rmdir /q/s {logdir}\n",
    "        # Clear old file\n",
    "        path = checkpoint_name + '**'\n",
    "        all_path_files = glob(path)\n",
    "        for file in all_path_files:\n",
    "            os.remove(file)\n",
    "        # fit model\n",
    "        callbacks_list = callback_model(model, checkpoint_name, logdir, X_test, y_test)\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        #del model\n",
    "        \n",
    "    # Get best file by reading log file\n",
    "    df = pd.read_csv('log.log')\n",
    "    best_epoch = df.loc[df['val_loss']==df['val_loss'].min(), 'epoch'].values[0]\n",
    "    best_file = 'model-' + format(best_epoch, '02d') + '-.hdf5'\n",
    "    #best_file = os.path.join(checkpoint_dir, 'model'+format(best_epoch, '02d') + '-.hdf5')\n",
    "    model = load_model(best_file, custom_objects={'my_r2_score': my_r2_score})\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">Train với bộ thông số tốt nhất:\n",
    "    - units: 260\n",
    "    - batch_size: 112\n",
    "    - activation: tanh\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "checkpoint_name = 'model-'\n",
    "baseDir = os.path.abspath(os.getcwd())\n",
    "logs_name = 'training_logs'\n",
    "logdir = os.path.join(baseDir, logs_name)\n",
    "#checkpoint_dir = os.path.join(baseDir, checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "   2/4037 [..............................] - ETA: 34:23 - loss: 3168.2971 - root_mean_squared_error: 56.2876 - my_r2_score: -741.2426WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 1.0208s). Check your callbacks.\n",
      "171/171 [==============================] - 0s 3ms/step - loss: 0.8095 - root_mean_squared_error: 0.8997 - my_r2_score: 0.7829\n",
      "my_val_loss [0.8095368146896362, 0.8997426629066467, 0.7829455733299255]\n",
      "4037/4037 [==============================] - 14s 3ms/step - loss: 7.4133 - root_mean_squared_error: 2.7227 - my_r2_score: -0.6465 - val_loss: 0.8095 - val_root_mean_squared_error: 0.8997 - val_my_r2_score: 0.7829\n",
      "Epoch 2/15\n",
      "4028/4037 [============================>.] - ETA: 0s - loss: 2.1679 - root_mean_squared_error: 1.4724 - my_r2_score: 0.5312\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "  1/171 [..............................] - ETA: 0s - loss: 1.3095 - root_mean_squared_error: 1.1443 - my_r2_score: 0.7032WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.8831 - root_mean_squared_error: 0.9397 - my_r2_score: 0.7580\n",
      "my_val_loss [0.8830680251121521, 0.9397169947624207, 0.7579670548439026]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 2.1680 - root_mean_squared_error: 1.4724 - my_r2_score: 0.5311 - val_loss: 0.8831 - val_root_mean_squared_error: 0.9397 - val_my_r2_score: 0.7580\n",
      "Epoch 3/15\n",
      "4035/4037 [============================>.] - ETA: 0s - loss: 1.9676 - root_mean_squared_error: 1.4027 - my_r2_score: 0.5744\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "  1/171 [..............................] - ETA: 0s - loss: 1.3175 - root_mean_squared_error: 1.1478 - my_r2_score: 0.7014WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.9007 - root_mean_squared_error: 0.9491 - my_r2_score: 0.7506\n",
      "my_val_loss [0.9007155299186707, 0.949060320854187, 0.7505550980567932]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 1.9676 - root_mean_squared_error: 1.4027 - my_r2_score: 0.5744 - val_loss: 0.9007 - val_root_mean_squared_error: 0.9491 - val_my_r2_score: 0.7506\n",
      "Epoch 4/15\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7802 - root_mean_squared_error: 0.8833 - my_r2_score: 0.7907\n",
      "my_val_loss [0.7802245020866394, 0.8833032250404358, 0.7907329201698303]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 1.8639 - root_mean_squared_error: 1.3653 - my_r2_score: 0.5968 - val_loss: 0.7802 - val_root_mean_squared_error: 0.8833 - val_my_r2_score: 0.7907\n",
      "Epoch 5/15\n",
      "4028/4037 [============================>.] - ETA: 0s - loss: 1.8134 - root_mean_squared_error: 1.3466 - my_r2_score: 0.6076\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7820 - root_mean_squared_error: 0.8843 - my_r2_score: 0.7877\n",
      "my_val_loss [0.7819671034812927, 0.884289026260376, 0.7876536846160889]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 1.8133 - root_mean_squared_error: 1.3466 - my_r2_score: 0.6077 - val_loss: 0.7820 - val_root_mean_squared_error: 0.8843 - val_my_r2_score: 0.7877\n",
      "Epoch 6/15\n",
      "4031/4037 [============================>.] - ETA: 0s - loss: 1.7678 - root_mean_squared_error: 1.3296 - my_r2_score: 0.6179\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "  1/171 [..............................] - ETA: 0s - loss: 1.2790 - root_mean_squared_error: 1.1309 - my_r2_score: 0.7101WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7982 - root_mean_squared_error: 0.8934 - my_r2_score: 0.7810\n",
      "my_val_loss [0.7982485890388489, 0.8934475779533386, 0.7810449600219727]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 1.7677 - root_mean_squared_error: 1.3295 - my_r2_score: 0.6180 - val_loss: 0.7982 - val_root_mean_squared_error: 0.8934 - val_my_r2_score: 0.7810\n",
      "Epoch 7/15\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7661 - root_mean_squared_error: 0.8753 - my_r2_score: 0.7909\n",
      "my_val_loss [0.766107976436615, 0.8752759695053101, 0.7908934950828552]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 1.7483 - root_mean_squared_error: 1.3222 - my_r2_score: 0.6217 - val_loss: 0.7661 - val_root_mean_squared_error: 0.8753 - val_my_r2_score: 0.7909\n",
      "Epoch 8/15\n",
      "4031/4037 [============================>.] - ETA: 0s - loss: 1.7442 - root_mean_squared_error: 1.3207 - my_r2_score: 0.6225\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "  1/171 [..............................] - ETA: 0s - loss: 1.2483 - root_mean_squared_error: 1.1173 - my_r2_score: 0.7171WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7789 - root_mean_squared_error: 0.8825 - my_r2_score: 0.7875\n",
      "my_val_loss [0.778855562210083, 0.8825279474258423, 0.7874740958213806]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 1.7441 - root_mean_squared_error: 1.3206 - my_r2_score: 0.6224 - val_loss: 0.7789 - val_root_mean_squared_error: 0.8825 - val_my_r2_score: 0.7875\n",
      "Epoch 9/15\n",
      "4022/4037 [============================>.] - ETA: 0s - loss: 1.7328 - root_mean_squared_error: 1.3164 - my_r2_score: 0.6255\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7963 - root_mean_squared_error: 0.8923 - my_r2_score: 0.7818\n",
      "my_val_loss [0.7962550520896912, 0.8923312425613403, 0.7817708253860474]\n",
      "4037/4037 [==============================] - 11s 3ms/step - loss: 1.7328 - root_mean_squared_error: 1.3164 - my_r2_score: 0.6256 - val_loss: 0.7963 - val_root_mean_squared_error: 0.8923 - val_my_r2_score: 0.7818\n",
      "Epoch 10/15\n",
      "4027/4037 [============================>.] - ETA: 0s - loss: 1.7263 - root_mean_squared_error: 1.3139 - my_r2_score: 0.6267\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "  1/171 [..............................] - ETA: 0s - loss: 1.2452 - root_mean_squared_error: 1.1159 - my_r2_score: 0.7178WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7731 - root_mean_squared_error: 0.8793 - my_r2_score: 0.7888\n",
      "my_val_loss [0.7731260061264038, 0.8792758584022522, 0.7888228297233582]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 1.7262 - root_mean_squared_error: 1.3138 - my_r2_score: 0.6267 - val_loss: 0.7731 - val_root_mean_squared_error: 0.8793 - val_my_r2_score: 0.7888\n",
      "Epoch 11/15\n",
      "4032/4037 [============================>.] - ETA: 0s - loss: 1.7316 - root_mean_squared_error: 1.3159 - my_r2_score: 0.6250\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.560999509019894e-07.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7803 - root_mean_squared_error: 0.8834 - my_r2_score: 0.7864\n",
      "my_val_loss [0.7803448438644409, 0.8833712935447693, 0.7864447832107544]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 1.7316 - root_mean_squared_error: 1.3159 - my_r2_score: 0.6250 - val_loss: 0.7803 - val_root_mean_squared_error: 0.8834 - val_my_r2_score: 0.7864\n",
      "Epoch 12/15\n",
      "4025/4037 [============================>.] - ETA: 0s - loss: 1.7349 - root_mean_squared_error: 1.3172 - my_r2_score: 0.6248\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.9682997844938655e-07.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7808 - root_mean_squared_error: 0.8836 - my_r2_score: 0.7863\n",
      "my_val_loss [0.7807795405387878, 0.8836172819137573, 0.7863473892211914]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 1.7350 - root_mean_squared_error: 1.3172 - my_r2_score: 0.6247 - val_loss: 0.7808 - val_root_mean_squared_error: 0.8836 - val_my_r2_score: 0.7863\n",
      "Epoch 13/15\n",
      "4023/4037 [============================>.] - ETA: 0s - loss: 1.7278 - root_mean_squared_error: 1.3144 - my_r2_score: 0.6262\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 5.9048991829513396e-08.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7789 - root_mean_squared_error: 0.8825 - my_r2_score: 0.7869\n",
      "my_val_loss [0.7788772583007812, 0.882540225982666, 0.7869299054145813]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 1.7276 - root_mean_squared_error: 1.3144 - my_r2_score: 0.6261 - val_loss: 0.7789 - val_root_mean_squared_error: 0.8825 - val_my_r2_score: 0.7869\n",
      "Epoch 14/15\n",
      "4034/4037 [============================>.] - ETA: 0s - loss: 1.7294 - root_mean_squared_error: 1.3151 - my_r2_score: 0.6259\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.771469797517966e-08.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7781 - root_mean_squared_error: 0.8821 - my_r2_score: 0.7872\n",
      "my_val_loss [0.7780724167823792, 0.8820841312408447, 0.7871730923652649]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 1.7294 - root_mean_squared_error: 1.3151 - my_r2_score: 0.6259 - val_loss: 0.7781 - val_root_mean_squared_error: 0.8821 - val_my_r2_score: 0.7872\n",
      "Epoch 00014: early stopping\n",
      "time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = build_and_compile_model(X_train, num_units=260, activation='tanh')\n",
    "\n",
    "model = train_model(model, X_train, y_train, X_test, y_test, checkpoint_name, logdir,\n",
    "                    batch_size=112, epochs=15, re_train=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sai số rmse:                    0.664\n",
      "Hệ số xác định r2-score: 0.908\n",
      "Tỉ lệ True positive:           0.400\n",
      "time: 26.6 s\n"
     ]
    }
   ],
   "source": [
    "# On train\n",
    "train_result_df = evaluate(model, X_train, y_train_df)\n",
    "#train_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sai số rmse:                    0.875\n",
      "Hệ số xác định r2-score: 0.856\n",
      "Tỉ lệ True positive:           0.373\n",
      "time: 531 ms\n"
     ]
    }
   ],
   "source": [
    "# On test\n",
    "test_result_df = evaluate(model, X_test, y_test_df)\n",
    "#test_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">Nhận xét:\n",
    "    Độ chính tập train được cải thiện, tuy nhiên tập test ko thay đổi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>lr</th>\n",
       "      <th>my_r2_score</th>\n",
       "      <th>root_mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_my_r2_score</th>\n",
       "      <th>val_root_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7.053571</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>-0.601827</td>\n",
       "      <td>2.655856</td>\n",
       "      <td>1.187730</td>\n",
       "      <td>0.671894</td>\n",
       "      <td>1.089830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.664495</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.640300</td>\n",
       "      <td>1.290153</td>\n",
       "      <td>0.838544</td>\n",
       "      <td>0.766776</td>\n",
       "      <td>0.915721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.586020</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.656849</td>\n",
       "      <td>1.259373</td>\n",
       "      <td>0.833012</td>\n",
       "      <td>0.771860</td>\n",
       "      <td>0.912695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.502647</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.675298</td>\n",
       "      <td>1.225825</td>\n",
       "      <td>0.933898</td>\n",
       "      <td>0.739138</td>\n",
       "      <td>0.966384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.373170</td>\n",
       "      <td>3.000000e-03</td>\n",
       "      <td>0.703197</td>\n",
       "      <td>1.171823</td>\n",
       "      <td>1.489730</td>\n",
       "      <td>0.576785</td>\n",
       "      <td>1.220545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1.318928</td>\n",
       "      <td>9.000000e-04</td>\n",
       "      <td>0.715144</td>\n",
       "      <td>1.148446</td>\n",
       "      <td>0.810347</td>\n",
       "      <td>0.777633</td>\n",
       "      <td>0.900193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1.301577</td>\n",
       "      <td>9.000000e-04</td>\n",
       "      <td>0.718435</td>\n",
       "      <td>1.140867</td>\n",
       "      <td>0.776818</td>\n",
       "      <td>0.787185</td>\n",
       "      <td>0.881373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1.280292</td>\n",
       "      <td>9.000000e-04</td>\n",
       "      <td>0.723322</td>\n",
       "      <td>1.131500</td>\n",
       "      <td>0.772641</td>\n",
       "      <td>0.788442</td>\n",
       "      <td>0.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1.266138</td>\n",
       "      <td>9.000000e-04</td>\n",
       "      <td>0.726285</td>\n",
       "      <td>1.125228</td>\n",
       "      <td>0.821982</td>\n",
       "      <td>0.773669</td>\n",
       "      <td>0.906632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1.246249</td>\n",
       "      <td>2.700000e-04</td>\n",
       "      <td>0.730441</td>\n",
       "      <td>1.116355</td>\n",
       "      <td>0.795432</td>\n",
       "      <td>0.781450</td>\n",
       "      <td>0.891870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1.243558</td>\n",
       "      <td>8.100000e-05</td>\n",
       "      <td>0.731293</td>\n",
       "      <td>1.115149</td>\n",
       "      <td>0.798599</td>\n",
       "      <td>0.780236</td>\n",
       "      <td>0.893643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1.236155</td>\n",
       "      <td>2.430000e-05</td>\n",
       "      <td>0.732637</td>\n",
       "      <td>1.111825</td>\n",
       "      <td>0.807862</td>\n",
       "      <td>0.777520</td>\n",
       "      <td>0.898811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1.239704</td>\n",
       "      <td>7.290000e-06</td>\n",
       "      <td>0.731998</td>\n",
       "      <td>1.113420</td>\n",
       "      <td>0.806955</td>\n",
       "      <td>0.777736</td>\n",
       "      <td>0.898306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1.240183</td>\n",
       "      <td>2.187000e-06</td>\n",
       "      <td>0.731998</td>\n",
       "      <td>1.113635</td>\n",
       "      <td>0.809614</td>\n",
       "      <td>0.776932</td>\n",
       "      <td>0.899786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1.238739</td>\n",
       "      <td>6.560999e-07</td>\n",
       "      <td>0.732227</td>\n",
       "      <td>1.112987</td>\n",
       "      <td>0.805509</td>\n",
       "      <td>0.778147</td>\n",
       "      <td>0.897501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch      loss            lr  my_r2_score  root_mean_squared_error  \\\n",
       "0       1  7.053571  1.000000e-02    -0.601827                 2.655856   \n",
       "1       2  1.664495  1.000000e-02     0.640300                 1.290153   \n",
       "2       3  1.586020  1.000000e-02     0.656849                 1.259373   \n",
       "3       4  1.502647  1.000000e-02     0.675298                 1.225825   \n",
       "4       5  1.373170  3.000000e-03     0.703197                 1.171823   \n",
       "5       6  1.318928  9.000000e-04     0.715144                 1.148446   \n",
       "6       7  1.301577  9.000000e-04     0.718435                 1.140867   \n",
       "7       8  1.280292  9.000000e-04     0.723322                 1.131500   \n",
       "8       9  1.266138  9.000000e-04     0.726285                 1.125228   \n",
       "9      10  1.246249  2.700000e-04     0.730441                 1.116355   \n",
       "10     11  1.243558  8.100000e-05     0.731293                 1.115149   \n",
       "11     12  1.236155  2.430000e-05     0.732637                 1.111825   \n",
       "12     13  1.239704  7.290000e-06     0.731998                 1.113420   \n",
       "13     14  1.240183  2.187000e-06     0.731998                 1.113635   \n",
       "14     15  1.238739  6.560999e-07     0.732227                 1.112987   \n",
       "\n",
       "    val_loss  val_my_r2_score  val_root_mean_squared_error  \n",
       "0   1.187730         0.671894                     1.089830  \n",
       "1   0.838544         0.766776                     0.915721  \n",
       "2   0.833012         0.771860                     0.912695  \n",
       "3   0.933898         0.739138                     0.966384  \n",
       "4   1.489730         0.576785                     1.220545  \n",
       "5   0.810347         0.777633                     0.900193  \n",
       "6   0.776818         0.787185                     0.881373  \n",
       "7   0.772641         0.788442                     0.879000  \n",
       "8   0.821982         0.773669                     0.906632  \n",
       "9   0.795432         0.781450                     0.891870  \n",
       "10  0.798599         0.780236                     0.893643  \n",
       "11  0.807862         0.777520                     0.898811  \n",
       "12  0.806955         0.777736                     0.898306  \n",
       "13  0.809614         0.776932                     0.899786  \n",
       "14  0.805509         0.778147                     0.897501  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "log_data = pd.read_csv('log.log')\n",
    "log_data['epoch'] = log_data['epoch'] + 1\n",
    "log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 313 ms\n"
     ]
    }
   ],
   "source": [
    "def plot_loss(log_data):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    #fig.add_subplot(1, 2, 1)\n",
    "    sns.lineplot(x='epoch', y='root_mean_squared_error', data=log_data, marker='s', label='train_loss');\n",
    "    sns.lineplot(x='epoch', y='val_root_mean_squared_error', data=log_data, marker='s', label='val_loss');\n",
    "    plt.xlabel('Epoch', size=13)\n",
    "    plt.xticks(np.arange(1, log_data['epoch'].max()+1, step=1))\n",
    "    plt.ylabel('Error [speed]', size=13)\n",
    "    plt.legend()\n",
    "    plt.title('Learning-curve', size=15, y=1.02)\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_loss(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 312 ms\n"
     ]
    }
   ],
   "source": [
    "def plot_r2(log_data):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    #fig.add_subplot(1, 2, 1)\n",
    "    sns.lineplot(x='epoch', y='my_r2_score', data=log_data, marker='s', label='train_score');\n",
    "    sns.lineplot(x='epoch', y='val_my_r2_score', data=log_data, marker='s', label='val_score');\n",
    "    plt.xlabel('Epoch', size=13)\n",
    "    plt.xticks(np.arange(1, log_data['epoch'].max()+1, step=1))\n",
    "    plt.ylabel('rate', size=13)\n",
    "    plt.legend()\n",
    "    plt.title('Score-curve', size=15, y=1.02)\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_r2(log_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tune parameter\n",
    "- Điều chỉnh các siêu tham số bằng phương pháp BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners.bayesian import BayesianOptimization\n",
    "from kerastuner import HyperModel\n",
    "\n",
    "class RegressionHyperModel(HyperModel):\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            Dense(\n",
    "                units=hp.Int('units', 100, 300, 5, default=100),\n",
    "                activation=hp.Choice(\n",
    "                    'activation',\n",
    "                    values=['relu', 'sigmoid', 'tanh'],\n",
    "                    default='sigmoid'),\n",
    "                kernel_initializer='he_normal',\n",
    "                input_shape=self.input_shape\n",
    "            )\n",
    "        )\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "                                           optimizer=Optimizer.Adam(0.01)) \n",
    "        return model\n",
    "    \n",
    "class MyTuner(BayesianOptimization):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 16, 128, step=16)\n",
    "        #kwargs['epochs'] = trial.hyperparameters.Int('epochs', 10, 20, 5)\n",
    "        super(MyTuner, self).run_trial(trial, *args, **kwargs)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 01m 58s]\n",
      "val_loss: 0.8350815773010254\n",
      "\n",
      "Best val_loss So Far: 0.7499967515468597\n",
      "Total elapsed time: 00h 46m 54s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "time: 46min 54s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_shape = (X_train.shape[1], )\n",
    "hypermodel = RegressionHyperModel(input_shape)\n",
    "project_name = 'fd_search'\n",
    "\n",
    "tuner = MyTuner(\n",
    "            hypermodel,\n",
    "            objective='val_loss',\n",
    "            max_trials=10,\n",
    "            executions_per_trial=2,\n",
    "            seed=42,\n",
    "            project_name=project_name,\n",
    "            overwrite=True\n",
    "            )\n",
    "tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=15, callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=3)], verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các bộ tham số tốt nhất\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>activation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175</td>\n",
       "      <td>96</td>\n",
       "      <td>tanh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>185</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>260</td>\n",
       "      <td>112</td>\n",
       "      <td>tanh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  batch_size activation\n",
       "0    175          96       tanh\n",
       "1    185         128       tanh\n",
       "2    260         112       tanh"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "list_best_hp = tuner.get_best_hyperparameters(num_trials=3)\n",
    "list_best_units = []\n",
    "list_best_batch_size = []\n",
    "list_best_activation = []\n",
    "\n",
    "for best_hp in list_best_hp:\n",
    "    best_units = best_hp.get('units')\n",
    "    list_best_units.append(best_units)\n",
    "    best_batch_size = best_hp.get('batch_size')\n",
    "    list_best_batch_size.append(best_batch_size)\n",
    "    best_activation = best_hp.get('activation')\n",
    "    list_best_activation.append(best_activation)\n",
    "\n",
    "hp_result = pd.DataFrame({'units':list_best_units,\n",
    "                            'batch_size': list_best_batch_size,\n",
    "                            'activation': list_best_activation\n",
    "                            })\n",
    "print('Các bộ tham số tốt nhất')\n",
    "hp_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">Phương pháp:\n",
    "    - Ta train lại 3 mô hình tốt nhất để đánh giá\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "   1/4709 [..............................] - ETA: 0s - loss: 3466.3311 - root_mean_squared_error: 58.8756 - my_r2_score: -768.6529WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\my_d2l\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "   2/4709 [..............................] - ETA: 15:08 - loss: 3277.2917 - root_mean_squared_error: 57.2476 - my_r2_score: -727.3167WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.3832s). Check your callbacks.\n",
      "4701/4709 [============================>.] - ETA: 0s - loss: 7.3113 - root_mean_squared_error: 2.7039 - my_r2_score: -0.6027WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8713 - root_mean_squared_error: 0.9334 - my_r2_score: 0.7380\n",
      "my_val_loss [0.871292769908905, 0.9334306716918945, 0.7380317449569702]\n",
      "4709/4709 [==============================] - 15s 3ms/step - loss: 7.2999 - root_mean_squared_error: 2.7018 - my_r2_score: -0.6001 - val_loss: 0.8713 - val_root_mean_squared_error: 0.9334 - val_my_r2_score: 0.7380\n",
      "Epoch 2/15\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8277 - root_mean_squared_error: 0.9098 - my_r2_score: 0.7568\n",
      "my_val_loss [0.8277316689491272, 0.9097976088523865, 0.7567923665046692]\n",
      "4709/4709 [==============================] - 14s 3ms/step - loss: 0.5102 - root_mean_squared_error: 0.7143 - my_r2_score: 0.8893 - val_loss: 0.8277 - val_root_mean_squared_error: 0.9098 - val_my_r2_score: 0.7568\n",
      "Epoch 3/15\n",
      "4702/4709 [============================>.] - ETA: 0s - loss: 0.5050 - root_mean_squared_error: 0.7107 - my_r2_score: 0.8903\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8356 - root_mean_squared_error: 0.9141 - my_r2_score: 0.7556\n",
      "my_val_loss [0.8355950117111206, 0.9141088724136353, 0.7556456923484802]\n",
      "4709/4709 [==============================] - 15s 3ms/step - loss: 0.5051 - root_mean_squared_error: 0.7107 - my_r2_score: 0.8903 - val_loss: 0.8356 - val_root_mean_squared_error: 0.9141 - val_my_r2_score: 0.7556\n",
      "Epoch 4/15\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7901 - root_mean_squared_error: 0.8889 - my_r2_score: 0.7662\n",
      "my_val_loss [0.7901192903518677, 0.888886570930481, 0.7661848664283752]\n",
      "4709/4709 [==============================] - 14s 3ms/step - loss: 0.4501 - root_mean_squared_error: 0.6709 - my_r2_score: 0.9022 - val_loss: 0.7901 - val_root_mean_squared_error: 0.8889 - val_my_r2_score: 0.7662\n",
      "Epoch 5/15\n",
      "4700/4709 [============================>.] - ETA: 0s - loss: 0.4402 - root_mean_squared_error: 0.6635 - my_r2_score: 0.9045\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "  1/200 [..............................] - ETA: 0s - loss: 1.5588 - root_mean_squared_error: 1.2485 - my_r2_score: 0.6002WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.8033 - root_mean_squared_error: 0.8963 - my_r2_score: 0.7627\n",
      "my_val_loss [0.8032810091972351, 0.8962594270706177, 0.7626926302909851]\n",
      "4709/4709 [==============================] - 14s 3ms/step - loss: 0.4402 - root_mean_squared_error: 0.6634 - my_r2_score: 0.9045 - val_loss: 0.8033 - val_root_mean_squared_error: 0.8963 - val_my_r2_score: 0.7627\n",
      "Epoch 6/15\n",
      "4705/4709 [============================>.] - ETA: 0s - loss: 0.4149 - root_mean_squared_error: 0.6441 - my_r2_score: 0.9098\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7902 - root_mean_squared_error: 0.8890 - my_r2_score: 0.7663\n",
      "my_val_loss [0.790245771408081, 0.888957679271698, 0.7662957906723022]\n",
      "4709/4709 [==============================] - 15s 3ms/step - loss: 0.4149 - root_mean_squared_error: 0.6441 - my_r2_score: 0.9098 - val_loss: 0.7902 - val_root_mean_squared_error: 0.8890 - val_my_r2_score: 0.7663\n",
      "Epoch 7/15\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7831 - root_mean_squared_error: 0.8849 - my_r2_score: 0.7691\n",
      "my_val_loss [0.7830857038497925, 0.8849213123321533, 0.7691020369529724]\n",
      "4709/4709 [==============================] - 14s 3ms/step - loss: 0.4041 - root_mean_squared_error: 0.6357 - my_r2_score: 0.9123 - val_loss: 0.7831 - val_root_mean_squared_error: 0.8849 - val_my_r2_score: 0.7691\n",
      "Epoch 8/15\n",
      "4709/4709 [==============================] - ETA: 0s - loss: 0.4022 - root_mean_squared_error: 0.6342 - my_r2_score: 0.9126\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7879 - root_mean_squared_error: 0.8876 - my_r2_score: 0.7678\n",
      "my_val_loss [0.7878768444061279, 0.8876242637634277, 0.7677773237228394]\n",
      "4709/4709 [==============================] - 16s 3ms/step - loss: 0.4022 - root_mean_squared_error: 0.6342 - my_r2_score: 0.9126 - val_loss: 0.7879 - val_root_mean_squared_error: 0.8876 - val_my_r2_score: 0.7678\n",
      "Epoch 9/15\n",
      "4693/4709 [============================>.] - ETA: 0s - loss: 0.3988 - root_mean_squared_error: 0.6315 - my_r2_score: 0.9133\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7915 - root_mean_squared_error: 0.8896 - my_r2_score: 0.7668\n",
      "my_val_loss [0.7914621829986572, 0.8896415829658508, 0.7668299674987793]\n",
      "4709/4709 [==============================] - 15s 3ms/step - loss: 0.3988 - root_mean_squared_error: 0.6315 - my_r2_score: 0.9133 - val_loss: 0.7915 - val_root_mean_squared_error: 0.8896 - val_my_r2_score: 0.7668\n",
      "Epoch 10/15\n",
      "4703/4709 [============================>.] - ETA: 0s - loss: 0.3975 - root_mean_squared_error: 0.6304 - my_r2_score: 0.9136\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7862 - root_mean_squared_error: 0.8867 - my_r2_score: 0.7683\n",
      "my_val_loss [0.7862375974655151, 0.8867003917694092, 0.7682636380195618]\n",
      "4709/4709 [==============================] - 15s 3ms/step - loss: 0.3974 - root_mean_squared_error: 0.6304 - my_r2_score: 0.9136 - val_loss: 0.7862 - val_root_mean_squared_error: 0.8867 - val_my_r2_score: 0.7683\n",
      "Epoch 11/15\n",
      "4698/4709 [============================>.] - ETA: 0s - loss: 0.3970 - root_mean_squared_error: 0.6301 - my_r2_score: 0.9137\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7855 - root_mean_squared_error: 0.8863 - my_r2_score: 0.7684\n",
      "my_val_loss [0.7855492234230042, 0.8863121271133423, 0.7684369087219238]\n",
      "4709/4709 [==============================] - 14s 3ms/step - loss: 0.3970 - root_mean_squared_error: 0.6301 - my_r2_score: 0.9137 - val_loss: 0.7855 - val_root_mean_squared_error: 0.8863 - val_my_r2_score: 0.7684\n",
      "Epoch 12/15\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7827 - root_mean_squared_error: 0.8847 - my_r2_score: 0.7693\n",
      "my_val_loss [0.7827378511428833, 0.8847247362136841, 0.7693291306495667]\n",
      "4709/4709 [==============================] - 15s 3ms/step - loss: 0.3969 - root_mean_squared_error: 0.6300 - my_r2_score: 0.9138 - val_loss: 0.7827 - val_root_mean_squared_error: 0.8847 - val_my_r2_score: 0.7693\n",
      "Epoch 13/15\n",
      "4694/4709 [============================>.] - ETA: 0s - loss: 0.3969 - root_mean_squared_error: 0.6300 - my_r2_score: 0.9137\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.560999509019894e-07.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7841 - root_mean_squared_error: 0.8855 - my_r2_score: 0.7689\n",
      "my_val_loss [0.7840571403503418, 0.8854700326919556, 0.7689206004142761]\n",
      "4709/4709 [==============================] - 14s 3ms/step - loss: 0.3969 - root_mean_squared_error: 0.6300 - my_r2_score: 0.9138 - val_loss: 0.7841 - val_root_mean_squared_error: 0.8855 - val_my_r2_score: 0.7689\n",
      "Epoch 14/15\n",
      "4707/4709 [============================>.] - ETA: 0s - loss: 0.3969 - root_mean_squared_error: 0.6300 - my_r2_score: 0.9137\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.9682997844938655e-07.\n",
      "  1/200 [..............................] - ETA: 0s - loss: 1.4464 - root_mean_squared_error: 1.2027 - my_r2_score: 0.6290WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7846 - root_mean_squared_error: 0.8858 - my_r2_score: 0.7687\n",
      "my_val_loss [0.7846294045448303, 0.8857930898666382, 0.7687391042709351]\n",
      "4709/4709 [==============================] - 14s 3ms/step - loss: 0.3969 - root_mean_squared_error: 0.6300 - my_r2_score: 0.9137 - val_loss: 0.7846 - val_root_mean_squared_error: 0.8858 - val_my_r2_score: 0.7687\n",
      "Epoch 15/15\n",
      "4690/4709 [============================>.] - ETA: 0s - loss: 0.3967 - root_mean_squared_error: 0.6299 - my_r2_score: 0.9138\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 5.9048991829513396e-08.\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7846 - root_mean_squared_error: 0.8858 - my_r2_score: 0.7687\n",
      "my_val_loss [0.7846048474311829, 0.8857792019844055, 0.7687463164329529]\n",
      "4709/4709 [==============================] - 14s 3ms/step - loss: 0.3969 - root_mean_squared_error: 0.6300 - my_r2_score: 0.9138 - val_loss: 0.7846 - val_root_mean_squared_error: 0.8858 - val_my_r2_score: 0.7687\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.7827 - root_mean_squared_error: 0.8847 - my_r2_score: 0.5458\n",
      "Epoch 1/15\n",
      "   2/3532 [..............................] - ETA: 34:24 - loss: 3235.4275 - root_mean_squared_error: 56.8808 - my_r2_score: -782.2657WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 1.1678s). Check your callbacks.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.8385 - root_mean_squared_error: 0.9157 - my_r2_score: 0.7867\n",
      "my_val_loss [0.8384547233581543, 0.915671706199646, 0.7866785526275635]\n",
      "3532/3532 [==============================] - 12s 3ms/step - loss: 8.4117 - root_mean_squared_error: 2.9003 - my_r2_score: -0.8683 - val_loss: 0.8385 - val_root_mean_squared_error: 0.9157 - val_my_r2_score: 0.7867\n",
      "Epoch 2/15\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7506 - root_mean_squared_error: 0.8664 - my_r2_score: 0.8063\n",
      "my_val_loss [0.7506258487701416, 0.8663866519927979, 0.8062645196914673]\n",
      "3532/3532 [==============================] - 10s 3ms/step - loss: 0.5029 - root_mean_squared_error: 0.7092 - my_r2_score: 0.8917 - val_loss: 0.7506 - val_root_mean_squared_error: 0.8664 - val_my_r2_score: 0.8063\n",
      "Epoch 3/15\n",
      "3530/3532 [============================>.] - ETA: 0s - loss: 0.4969 - root_mean_squared_error: 0.7049 - my_r2_score: 0.8929\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.8262 - root_mean_squared_error: 0.9089 - my_r2_score: 0.7870\n",
      "my_val_loss [0.8261728286743164, 0.9089404940605164, 0.7870221138000488]\n",
      "3532/3532 [==============================] - 10s 3ms/step - loss: 0.4969 - root_mean_squared_error: 0.7049 - my_r2_score: 0.8929 - val_loss: 0.8262 - val_root_mean_squared_error: 0.9089 - val_my_r2_score: 0.7870\n",
      "Epoch 4/15\n",
      "3529/3532 [============================>.] - ETA: 0s - loss: 0.4424 - root_mean_squared_error: 0.6651 - my_r2_score: 0.9047\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.8001 - root_mean_squared_error: 0.8945 - my_r2_score: 0.7936\n",
      "my_val_loss [0.800108790397644, 0.8944879770278931, 0.7935757040977478]\n",
      "3532/3532 [==============================] - 11s 3ms/step - loss: 0.4424 - root_mean_squared_error: 0.6651 - my_r2_score: 0.9047 - val_loss: 0.8001 - val_root_mean_squared_error: 0.8945 - val_my_r2_score: 0.7936\n",
      "Epoch 5/15\n",
      "3531/3532 [============================>.] - ETA: 0s - loss: 0.4148 - root_mean_squared_error: 0.6440 - my_r2_score: 0.9107\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7739 - root_mean_squared_error: 0.8797 - my_r2_score: 0.8015\n",
      "my_val_loss [0.773928165435791, 0.8797318935394287, 0.801531970500946]\n",
      "3532/3532 [==============================] - 10s 3ms/step - loss: 0.4148 - root_mean_squared_error: 0.6440 - my_r2_score: 0.9107 - val_loss: 0.7739 - val_root_mean_squared_error: 0.8797 - val_my_r2_score: 0.8015\n",
      "Epoch 6/15\n",
      "3524/3532 [============================>.] - ETA: 0s - loss: 0.4019 - root_mean_squared_error: 0.6340 - my_r2_score: 0.9133\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7623 - root_mean_squared_error: 0.8731 - my_r2_score: 0.8035\n",
      "my_val_loss [0.7623321413993835, 0.8731163144111633, 0.8034549951553345]\n",
      "3532/3532 [==============================] - 10s 3ms/step - loss: 0.4019 - root_mean_squared_error: 0.6340 - my_r2_score: 0.9134 - val_loss: 0.7623 - val_root_mean_squared_error: 0.8731 - val_my_r2_score: 0.8035\n",
      "Epoch 7/15\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7471 - root_mean_squared_error: 0.8644 - my_r2_score: 0.8075\n",
      "my_val_loss [0.7471164464950562, 0.8643589615821838, 0.8074701428413391]\n",
      "3532/3532 [==============================] - 10s 3ms/step - loss: 0.3973 - root_mean_squared_error: 0.6304 - my_r2_score: 0.9145 - val_loss: 0.7471 - val_root_mean_squared_error: 0.8644 - val_my_r2_score: 0.8075\n",
      "Epoch 8/15\n",
      "3526/3532 [============================>.] - ETA: 0s - loss: 0.3965 - root_mean_squared_error: 0.6297 - my_r2_score: 0.9146\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7620 - root_mean_squared_error: 0.8729 - my_r2_score: 0.8037\n",
      "my_val_loss [0.7619718313217163, 0.8729099631309509, 0.8036950826644897]\n",
      "3532/3532 [==============================] - 10s 3ms/step - loss: 0.3965 - root_mean_squared_error: 0.6297 - my_r2_score: 0.9146 - val_loss: 0.7620 - val_root_mean_squared_error: 0.8729 - val_my_r2_score: 0.8037\n",
      "Epoch 9/15\n",
      "3526/3532 [============================>.] - ETA: 0s - loss: 0.3951 - root_mean_squared_error: 0.6286 - my_r2_score: 0.9149\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7546 - root_mean_squared_error: 0.8687 - my_r2_score: 0.8057\n",
      "my_val_loss [0.7546037435531616, 0.8686792850494385, 0.805666983127594]\n",
      "3532/3532 [==============================] - 10s 3ms/step - loss: 0.3951 - root_mean_squared_error: 0.6286 - my_r2_score: 0.9149 - val_loss: 0.7546 - val_root_mean_squared_error: 0.8687 - val_my_r2_score: 0.8057\n",
      "Epoch 10/15\n",
      "3528/3532 [============================>.] - ETA: 0s - loss: 0.3946 - root_mean_squared_error: 0.6282 - my_r2_score: 0.9150\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7569 - root_mean_squared_error: 0.8700 - my_r2_score: 0.8051\n",
      "my_val_loss [0.7569151520729065, 0.8700087070465088, 0.8051124215126038]\n",
      "3532/3532 [==============================] - 10s 3ms/step - loss: 0.3946 - root_mean_squared_error: 0.6282 - my_r2_score: 0.9150 - val_loss: 0.7569 - val_root_mean_squared_error: 0.8700 - val_my_r2_score: 0.8051\n",
      "Epoch 11/15\n",
      "3516/3532 [============================>.] - ETA: 0s - loss: 0.3945 - root_mean_squared_error: 0.6281 - my_r2_score: 0.9150\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.560999509019894e-07.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7565 - root_mean_squared_error: 0.8698 - my_r2_score: 0.8052\n",
      "my_val_loss [0.756470263004303, 0.8697530031204224, 0.8052089810371399]\n",
      "3532/3532 [==============================] - 10s 3ms/step - loss: 0.3945 - root_mean_squared_error: 0.6281 - my_r2_score: 0.9150 - val_loss: 0.7565 - val_root_mean_squared_error: 0.8698 - val_my_r2_score: 0.8052\n",
      "Epoch 12/15\n",
      "3530/3532 [============================>.] - ETA: 0s - loss: 0.3945 - root_mean_squared_error: 0.6281 - my_r2_score: 0.9150\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.9682997844938655e-07.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7561 - root_mean_squared_error: 0.8696 - my_r2_score: 0.8053\n",
      "my_val_loss [0.7561244368553162, 0.8695541620254517, 0.8052952289581299]\n",
      "3532/3532 [==============================] - 10s 3ms/step - loss: 0.3945 - root_mean_squared_error: 0.6281 - my_r2_score: 0.9150 - val_loss: 0.7561 - val_root_mean_squared_error: 0.8696 - val_my_r2_score: 0.8053\n",
      "Epoch 13/15\n",
      "3516/3532 [============================>.] - ETA: 0s - loss: 0.3944 - root_mean_squared_error: 0.6280 - my_r2_score: 0.9150\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 5.9048991829513396e-08.\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.7563 - root_mean_squared_error: 0.8697 - my_r2_score: 0.8052\n",
      "my_val_loss [0.7563248872756958, 0.8696694374084473, 0.8052458167076111]\n",
      "3532/3532 [==============================] - 10s 3ms/step - loss: 0.3944 - root_mean_squared_error: 0.6280 - my_r2_score: 0.9150 - val_loss: 0.7563 - val_root_mean_squared_error: 0.8697 - val_my_r2_score: 0.8052\n",
      "Epoch 14/15\n",
      "3530/3532 [============================>.] - ETA: 0s - loss: 0.3944 - root_mean_squared_error: 0.6281 - my_r2_score: 0.9150\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.771469797517966e-08.\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.7563 - root_mean_squared_error: 0.8697 - my_r2_score: 0.8052\n",
      "my_val_loss [0.7563424706459045, 0.8696795105934143, 0.8052417635917664]\n",
      "3532/3532 [==============================] - 11s 3ms/step - loss: 0.3944 - root_mean_squared_error: 0.6280 - my_r2_score: 0.9150 - val_loss: 0.7563 - val_root_mean_squared_error: 0.8697 - val_my_r2_score: 0.8052\n",
      "Epoch 00014: early stopping\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.7471 - root_mean_squared_error: 0.8644 - my_r2_score: 0.5618\n",
      "Epoch 1/15\n",
      "   2/4037 [..............................] - ETA: 28:02 - loss: 3224.7908 - root_mean_squared_error: 56.7872 - my_r2_score: -834.9128WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.8289s). Check your callbacks.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7698 - root_mean_squared_error: 0.8774 - my_r2_score: 0.7898\n",
      "my_val_loss [0.7698479294776917, 0.8774097561836243, 0.7898383140563965]\n",
      "4037/4037 [==============================] - 13s 3ms/step - loss: 6.0494 - root_mean_squared_error: 2.4596 - my_r2_score: -0.3627 - val_loss: 0.7698 - val_root_mean_squared_error: 0.8774 - val_my_r2_score: 0.7898\n",
      "Epoch 2/15\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7680 - root_mean_squared_error: 0.8764 - my_r2_score: 0.7887\n",
      "my_val_loss [0.7680429220199585, 0.8763805627822876, 0.788735568523407]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.4990 - root_mean_squared_error: 0.7064 - my_r2_score: 0.8923 - val_loss: 0.7680 - val_root_mean_squared_error: 0.8764 - val_my_r2_score: 0.7887\n",
      "Epoch 3/15\n",
      "4024/4037 [============================>.] - ETA: 0s - loss: 0.4954 - root_mean_squared_error: 0.7038 - my_r2_score: 0.8930\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.9917 - root_mean_squared_error: 0.9958 - my_r2_score: 0.7206\n",
      "my_val_loss [0.9917095303535461, 0.995846152305603, 0.7206254601478577]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.4955 - root_mean_squared_error: 0.7039 - my_r2_score: 0.8930 - val_loss: 0.9917 - val_root_mean_squared_error: 0.9958 - val_my_r2_score: 0.7206\n",
      "Epoch 4/15\n",
      "4025/4037 [============================>.] - ETA: 0s - loss: 0.4367 - root_mean_squared_error: 0.6608 - my_r2_score: 0.9056\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7698 - root_mean_squared_error: 0.8774 - my_r2_score: 0.7898\n",
      "my_val_loss [0.7698414921760559, 0.877406120300293, 0.7898037433624268]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.4367 - root_mean_squared_error: 0.6608 - my_r2_score: 0.9056 - val_loss: 0.7698 - val_root_mean_squared_error: 0.8774 - val_my_r2_score: 0.7898\n",
      "Epoch 5/15\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7207 - root_mean_squared_error: 0.8490 - my_r2_score: 0.8038\n",
      "my_val_loss [0.7207171320915222, 0.8489506244659424, 0.8037655353546143]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.4073 - root_mean_squared_error: 0.6382 - my_r2_score: 0.9119 - val_loss: 0.7207 - val_root_mean_squared_error: 0.8490 - val_my_r2_score: 0.8038\n",
      "Epoch 6/15\n",
      "4031/4037 [============================>.] - ETA: 0s - loss: 0.4002 - root_mean_squared_error: 0.6326 - my_r2_score: 0.9135\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7305 - root_mean_squared_error: 0.8547 - my_r2_score: 0.8007\n",
      "my_val_loss [0.7305027842521667, 0.8546945452690125, 0.8007142543792725]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.4002 - root_mean_squared_error: 0.6326 - my_r2_score: 0.9135 - val_loss: 0.7305 - val_root_mean_squared_error: 0.8547 - val_my_r2_score: 0.8007\n",
      "Epoch 7/15\n",
      "4023/4037 [============================>.] - ETA: 0s - loss: 0.3882 - root_mean_squared_error: 0.6230 - my_r2_score: 0.9161\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7507 - root_mean_squared_error: 0.8664 - my_r2_score: 0.7956\n",
      "my_val_loss [0.7507234215736389, 0.8664429783821106, 0.7955982685089111]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.3881 - root_mean_squared_error: 0.6230 - my_r2_score: 0.9161 - val_loss: 0.7507 - val_root_mean_squared_error: 0.8664 - val_my_r2_score: 0.7956\n",
      "Epoch 8/15\n",
      "4031/4037 [============================>.] - ETA: 0s - loss: 0.3836 - root_mean_squared_error: 0.6193 - my_r2_score: 0.9170\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "  1/171 [..............................] - ETA: 0s - loss: 1.2059 - root_mean_squared_error: 1.0981 - my_r2_score: 0.7267WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7311 - root_mean_squared_error: 0.8551 - my_r2_score: 0.8009\n",
      "my_val_loss [0.7311333417892456, 0.8550633788108826, 0.8009086847305298]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.3836 - root_mean_squared_error: 0.6193 - my_r2_score: 0.9170 - val_loss: 0.7311 - val_root_mean_squared_error: 0.8551 - val_my_r2_score: 0.8009\n",
      "Epoch 9/15\n",
      "4032/4037 [============================>.] - ETA: 0s - loss: 0.3821 - root_mean_squared_error: 0.6181 - my_r2_score: 0.9174\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7424 - root_mean_squared_error: 0.8616 - my_r2_score: 0.7978\n",
      "my_val_loss [0.7424362897872925, 0.8616474270820618, 0.7977668046951294]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.3820 - root_mean_squared_error: 0.6181 - my_r2_score: 0.9175 - val_loss: 0.7424 - val_root_mean_squared_error: 0.8616 - val_my_r2_score: 0.7978\n",
      "Epoch 10/15\n",
      "4034/4037 [============================>.] - ETA: 0s - loss: 0.3816 - root_mean_squared_error: 0.6177 - my_r2_score: 0.9175\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7387 - root_mean_squared_error: 0.8595 - my_r2_score: 0.7988\n",
      "my_val_loss [0.7387369871139526, 0.8594980835914612, 0.7988137602806091]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.3815 - root_mean_squared_error: 0.6177 - my_r2_score: 0.9175 - val_loss: 0.7387 - val_root_mean_squared_error: 0.8595 - val_my_r2_score: 0.7988\n",
      "Epoch 11/15\n",
      "4028/4037 [============================>.] - ETA: 0s - loss: 0.3815 - root_mean_squared_error: 0.6177 - my_r2_score: 0.9175\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.560999509019894e-07.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7381 - root_mean_squared_error: 0.8591 - my_r2_score: 0.7990\n",
      "my_val_loss [0.7381131052970886, 0.859135091304779, 0.7989819645881653]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.3814 - root_mean_squared_error: 0.6176 - my_r2_score: 0.9176 - val_loss: 0.7381 - val_root_mean_squared_error: 0.8591 - val_my_r2_score: 0.7990\n",
      "Epoch 12/15\n",
      "4035/4037 [============================>.] - ETA: 0s - loss: 0.3813 - root_mean_squared_error: 0.6175 - my_r2_score: 0.9176\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.9682997844938655e-07.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7405 - root_mean_squared_error: 0.8605 - my_r2_score: 0.7983\n",
      "my_val_loss [0.7405150532722473, 0.8605318665504456, 0.7982740998268127]\n",
      "4037/4037 [==============================] - 13s 3ms/step - loss: 0.3813 - root_mean_squared_error: 0.6175 - my_r2_score: 0.9176 - val_loss: 0.7405 - val_root_mean_squared_error: 0.8605 - val_my_r2_score: 0.7983\n",
      "Epoch 00012: early stopping\n",
      "599/599 [==============================] - 1s 2ms/step - loss: 0.7207 - root_mean_squared_error: 0.8490 - my_r2_score: 0.5748\n",
      "time: 8min 38s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_val_loss = []\n",
    "for num_units, activation, batch_size in zip(list_best_units, list_best_activation, list_best_batch_size):\n",
    "    model = build_and_compile_model(X_train, num_units=num_units, activation=activation)\n",
    "    model = train_model(model, X_train, y_train, X_test, y_test, checkpoint_name, logdir,\n",
    "                        batch_size=batch_size, epochs=15, re_train=True)\n",
    "    val_loss, _, _ = model.evaluate(X_test, y_test)\n",
    "    list_val_loss.append(val_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>activation</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175</td>\n",
       "      <td>96</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.782739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>185</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.747116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>260</td>\n",
       "      <td>112</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.720717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   units  batch_size activation  val_loss\n",
       "0    175          96       tanh  0.782739\n",
       "1    185         128       tanh  0.747116\n",
       "2    260         112       tanh  0.720717"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "filter_data_result = pd.DataFrame({'units':list_best_units,\n",
    "                            'batch_size': list_best_batch_size,\n",
    "                            'activation': list_best_activation,\n",
    "                            'val_loss': list_val_loss})\n",
    "filter_data_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "# Save improve_result\n",
    "filter_data_result.to_csv('filter_data_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">Ta chọn bộ tham số tốt nhất:\n",
    "    - units: 260\n",
    "    - batch_size: 112\n",
    "    - activation: tanh\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "   2/4037 [..............................] - ETA: 31:30 - loss: 3128.4944 - root_mean_squared_error: 55.9329 - my_r2_score: -629.3411WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.9320s). Check your callbacks.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7660 - root_mean_squared_error: 0.8752 - my_r2_score: 0.7887\n",
      "my_val_loss [0.7659750580787659, 0.8751999735832214, 0.7887331247329712]\n",
      "4037/4037 [==============================] - 14s 3ms/step - loss: 5.7225 - root_mean_squared_error: 2.3922 - my_r2_score: -0.1578 - val_loss: 0.7660 - val_root_mean_squared_error: 0.8752 - val_my_r2_score: 0.7887\n",
      "Epoch 2/15\n",
      "4027/4037 [============================>.] - ETA: 0s - loss: 0.5017 - root_mean_squared_error: 0.7083 - my_r2_score: 0.8915\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.9387 - root_mean_squared_error: 0.9689 - my_r2_score: 0.7367\n",
      "my_val_loss [0.9387310743331909, 0.968881368637085, 0.7366563081741333]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.5017 - root_mean_squared_error: 0.7083 - my_r2_score: 0.8915 - val_loss: 0.9387 - val_root_mean_squared_error: 0.9689 - val_my_r2_score: 0.7367\n",
      "Epoch 3/15\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7206 - root_mean_squared_error: 0.8489 - my_r2_score: 0.8030\n",
      "my_val_loss [0.7206396460533142, 0.8489049673080444, 0.8029960989952087]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.4448 - root_mean_squared_error: 0.6670 - my_r2_score: 0.9038 - val_loss: 0.7206 - val_root_mean_squared_error: 0.8489 - val_my_r2_score: 0.8030\n",
      "Epoch 4/15\n",
      "  1/171 [..............................] - ETA: 0s - loss: 1.2193 - root_mean_squared_error: 1.1042 - my_r2_score: 0.723661WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7133 - root_mean_squared_error: 0.8446 - my_r2_score: 0.8057\n",
      "my_val_loss [0.7132818698883057, 0.8445601463317871, 0.8056554198265076]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.4342 - root_mean_squared_error: 0.6589 - my_r2_score: 0.9060 - val_loss: 0.7133 - val_root_mean_squared_error: 0.8446 - val_my_r2_score: 0.8057\n",
      "Epoch 5/15\n",
      "4031/4037 [============================>.] - ETA: 0s - loss: 0.4233 - root_mean_squared_error: 0.6506 - my_r2_score: 0.9085\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7421 - root_mean_squared_error: 0.8615 - my_r2_score: 0.7961\n",
      "my_val_loss [0.7421233654022217, 0.8614658117294312, 0.7961270809173584]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.4233 - root_mean_squared_error: 0.6506 - my_r2_score: 0.9085 - val_loss: 0.7421 - val_root_mean_squared_error: 0.8615 - val_my_r2_score: 0.7961\n",
      "Epoch 6/15\n",
      "4029/4037 [============================>.] - ETA: 0s - loss: 0.3926 - root_mean_squared_error: 0.6266 - my_r2_score: 0.9150\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7463 - root_mean_squared_error: 0.8639 - my_r2_score: 0.7958\n",
      "my_val_loss [0.7462936639785767, 0.8638828992843628, 0.795833945274353]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.3927 - root_mean_squared_error: 0.6266 - my_r2_score: 0.9150 - val_loss: 0.7463 - val_root_mean_squared_error: 0.8639 - val_my_r2_score: 0.7958\n",
      "Epoch 7/15\n",
      "4031/4037 [============================>.] - ETA: 0s - loss: 0.3782 - root_mean_squared_error: 0.6150 - my_r2_score: 0.9182\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7467 - root_mean_squared_error: 0.8641 - my_r2_score: 0.7960\n",
      "my_val_loss [0.7467442154884338, 0.8641436100006104, 0.7960271835327148]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.3782 - root_mean_squared_error: 0.6149 - my_r2_score: 0.9182 - val_loss: 0.7467 - val_root_mean_squared_error: 0.8641 - val_my_r2_score: 0.7960\n",
      "Epoch 8/15\n",
      "4033/4037 [============================>.] - ETA: 0s - loss: 0.3732 - root_mean_squared_error: 0.6109 - my_r2_score: 0.9192\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7433 - root_mean_squared_error: 0.8622 - my_r2_score: 0.7967\n",
      "my_val_loss [0.7433152198791504, 0.8621572852134705, 0.7966516613960266]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.3732 - root_mean_squared_error: 0.6109 - my_r2_score: 0.9192 - val_loss: 0.7433 - val_root_mean_squared_error: 0.8622 - val_my_r2_score: 0.7967\n",
      "Epoch 9/15\n",
      "4020/4037 [============================>.] - ETA: 0s - loss: 0.3715 - root_mean_squared_error: 0.6095 - my_r2_score: 0.9196\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7443 - root_mean_squared_error: 0.8627 - my_r2_score: 0.7964\n",
      "my_val_loss [0.7442680597305298, 0.8627097010612488, 0.7964337468147278]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.3715 - root_mean_squared_error: 0.6095 - my_r2_score: 0.9196 - val_loss: 0.7443 - val_root_mean_squared_error: 0.8627 - val_my_r2_score: 0.7964\n",
      "Epoch 10/15\n",
      "4035/4037 [============================>.] - ETA: 0s - loss: 0.3710 - root_mean_squared_error: 0.6091 - my_r2_score: 0.9197\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7432 - root_mean_squared_error: 0.8621 - my_r2_score: 0.7967\n",
      "my_val_loss [0.7431530356407166, 0.8620632290840149, 0.7966877818107605]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.3710 - root_mean_squared_error: 0.6091 - my_r2_score: 0.9197 - val_loss: 0.7432 - val_root_mean_squared_error: 0.8621 - val_my_r2_score: 0.7967\n",
      "Epoch 11/15\n",
      "4037/4037 [==============================] - ETA: 0s - loss: 0.3708 - root_mean_squared_error: 0.6090 - my_r2_score: 0.9198\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.560999509019894e-07.\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7413 - root_mean_squared_error: 0.8610 - my_r2_score: 0.7972\n",
      "my_val_loss [0.7413409352302551, 0.8610115647315979, 0.7972034811973572]\n",
      "4037/4037 [==============================] - 12s 3ms/step - loss: 0.3708 - root_mean_squared_error: 0.6090 - my_r2_score: 0.9198 - val_loss: 0.7413 - val_root_mean_squared_error: 0.8610 - val_my_r2_score: 0.7972\n",
      "Epoch 00011: early stopping\n",
      "time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "model = build_and_compile_model(X_train, num_units=260, activation='tanh')\n",
    "model = train_model(model, X_train, y_train, X_test, y_test, checkpoint_name, logdir,\n",
    "                        batch_size=112, epochs=15, re_train=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sai số rmse:                    0.648\n",
      "Hệ số xác định r2-score: 0.912\n",
      "Tỉ lệ True positive:           0.418\n",
      "time: 25 s\n"
     ]
    }
   ],
   "source": [
    "# On train\n",
    "train_result_df = evaluate(model, X_train, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sai số rmse:                    0.845\n",
      "Hệ số xác định r2-score: 0.866\n",
      "Tỉ lệ True positive:           0.385\n",
      "time: 453 ms\n"
     ]
    }
   ],
   "source": [
    "# On test\n",
    "test_result_df = evaluate(model, X_test, y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>lr</th>\n",
       "      <th>my_r2_score</th>\n",
       "      <th>root_mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_my_r2_score</th>\n",
       "      <th>val_root_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.722518</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>-0.157803</td>\n",
       "      <td>2.392179</td>\n",
       "      <td>0.765975</td>\n",
       "      <td>0.788733</td>\n",
       "      <td>0.875200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.501720</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.891503</td>\n",
       "      <td>0.708322</td>\n",
       "      <td>0.938731</td>\n",
       "      <td>0.736656</td>\n",
       "      <td>0.968881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.444827</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.903845</td>\n",
       "      <td>0.666953</td>\n",
       "      <td>0.720640</td>\n",
       "      <td>0.802996</td>\n",
       "      <td>0.848905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.434201</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.906033</td>\n",
       "      <td>0.658939</td>\n",
       "      <td>0.713282</td>\n",
       "      <td>0.805655</td>\n",
       "      <td>0.844560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.423324</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.908467</td>\n",
       "      <td>0.650634</td>\n",
       "      <td>0.742123</td>\n",
       "      <td>0.796127</td>\n",
       "      <td>0.861466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.392653</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.915026</td>\n",
       "      <td>0.626620</td>\n",
       "      <td>0.746294</td>\n",
       "      <td>0.795834</td>\n",
       "      <td>0.863883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.378159</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.918240</td>\n",
       "      <td>0.614947</td>\n",
       "      <td>0.746744</td>\n",
       "      <td>0.796027</td>\n",
       "      <td>0.864144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.373227</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.919205</td>\n",
       "      <td>0.610923</td>\n",
       "      <td>0.743315</td>\n",
       "      <td>0.796652</td>\n",
       "      <td>0.862157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.371548</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.919621</td>\n",
       "      <td>0.609547</td>\n",
       "      <td>0.744268</td>\n",
       "      <td>0.796434</td>\n",
       "      <td>0.862710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.370996</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.919730</td>\n",
       "      <td>0.609094</td>\n",
       "      <td>0.743153</td>\n",
       "      <td>0.796688</td>\n",
       "      <td>0.862063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.370821</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.919810</td>\n",
       "      <td>0.608951</td>\n",
       "      <td>0.741341</td>\n",
       "      <td>0.797203</td>\n",
       "      <td>0.861012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch      loss        lr  my_r2_score  root_mean_squared_error  val_loss  \\\n",
       "0       1  5.722518  0.010000    -0.157803                 2.392179  0.765975   \n",
       "1       2  0.501720  0.010000     0.891503                 0.708322  0.938731   \n",
       "2       3  0.444827  0.003000     0.903845                 0.666953  0.720640   \n",
       "3       4  0.434201  0.003000     0.906033                 0.658939  0.713282   \n",
       "4       5  0.423324  0.003000     0.908467                 0.650634  0.742123   \n",
       "5       6  0.392653  0.000900     0.915026                 0.626620  0.746294   \n",
       "6       7  0.378159  0.000270     0.918240                 0.614947  0.746744   \n",
       "7       8  0.373227  0.000081     0.919205                 0.610923  0.743315   \n",
       "8       9  0.371548  0.000024     0.919621                 0.609547  0.744268   \n",
       "9      10  0.370996  0.000007     0.919730                 0.609094  0.743153   \n",
       "10     11  0.370821  0.000002     0.919810                 0.608951  0.741341   \n",
       "\n",
       "    val_my_r2_score  val_root_mean_squared_error  \n",
       "0          0.788733                     0.875200  \n",
       "1          0.736656                     0.968881  \n",
       "2          0.802996                     0.848905  \n",
       "3          0.805655                     0.844560  \n",
       "4          0.796127                     0.861466  \n",
       "5          0.795834                     0.863883  \n",
       "6          0.796027                     0.864144  \n",
       "7          0.796652                     0.862157  \n",
       "8          0.796434                     0.862710  \n",
       "9          0.796688                     0.862063  \n",
       "10         0.797203                     0.861012  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "log_data = pd.read_csv('log.log')\n",
    "log_data['epoch'] = log_data['epoch'] + 1\n",
    "log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 344 ms\n"
     ]
    }
   ],
   "source": [
    "def plot_loss(log_data):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    #fig.add_subplot(1, 2, 1)\n",
    "    sns.lineplot(x='epoch', y='root_mean_squared_error', data=log_data, marker='s', label='train_loss');\n",
    "    sns.lineplot(x='epoch', y='val_root_mean_squared_error', data=log_data, marker='s', label='val_loss');\n",
    "    plt.xlabel('Epoch', size=13)\n",
    "    plt.xticks(np.arange(1, log_data['epoch'].max()+1, step=1))\n",
    "    plt.ylabel('Error [speed]', size=13)\n",
    "    plt.legend()\n",
    "    plt.title('Learning-curve', size=15, y=1.02)\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_loss(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 281 ms\n"
     ]
    }
   ],
   "source": [
    "def plot_r2(log_data):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    #fig.add_subplot(1, 2, 1)\n",
    "    sns.lineplot(x='epoch', y='my_r2_score', data=log_data, marker='s', label='train_score');\n",
    "    sns.lineplot(x='epoch', y='val_my_r2_score', data=log_data, marker='s', label='val_score');\n",
    "    plt.xlabel('Epoch', size=13)\n",
    "    plt.xticks(np.arange(1, log_data['epoch'].max()+1, step=1))\n",
    "    plt.ylabel('rate', size=13)\n",
    "    plt.legend()\n",
    "    plt.title('Score-curve', size=15, y=1.02)\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_r2(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
